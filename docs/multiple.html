<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 5 Multiple regression | Demystifying Structural Equation Modeling</title>
<meta name="author" content="Jonathan Amburgey and Sean Raleigh, Westminster College (Salt Lake City, UT)">
<meta name="description" content="Preliminaries We will load the tidyverse package to work with tibbles, the broom package to calculate residuals, and lavaan. library(tidyverse) library(broom) library(lavaan)  5.1 The multiple...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 5 Multiple regression | Demystifying Structural Equation Modeling">
<meta property="og:type" content="book">
<meta property="og:url" content="https://vectorposse.github.io/sem_book/multiple.html">
<meta property="og:description" content="Preliminaries We will load the tidyverse package to work with tibbles, the broom package to calculate residuals, and lavaan. library(tidyverse) library(broom) library(lavaan)  5.1 The multiple...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 5 Multiple regression | Demystifying Structural Equation Modeling">
<meta name="twitter:description" content="Preliminaries We will load the tidyverse package to work with tibbles, the broom package to calculate residuals, and lavaan. library(tidyverse) library(broom) library(lavaan)  5.1 The multiple...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Demystifying Structural Equation Modeling</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Introduction</a></li>
<li><a class="" href="variables.html"><span class="header-section-number">1</span> Variables and measurement</a></li>
<li><a class="" href="variance.html"><span class="header-section-number">2</span> Variance</a></li>
<li><a class="" href="covariance.html"><span class="header-section-number">3</span> Covariance</a></li>
<li><a class="" href="simple.html"><span class="header-section-number">4</span> Simple regression</a></li>
<li><a class="active" href="multiple.html"><span class="header-section-number">5</span> Multiple regression</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">6</span> Mediation</a></li>
<li><a class="" href="path.html"><span class="header-section-number">7</span> Path analysis</a></li>
<li><a class="" href="latent.html"><span class="header-section-number">8</span> Latent variables</a></li>
<li><a class="" href="cfa.html"><span class="header-section-number">9</span> Confirmatory factor analysis</a></li>
<li><a class="" href="sem.html"><span class="header-section-number">10</span> Structural equation models</a></li>
<li><a class="" href="scm.html"><span class="header-section-number">11</span> Structural causal models</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="appendix-rules.html"><span class="header-section-number">A</span> Variance/covariance rules</a></li>
<li><a class="" href="appendix-lisrel.html"><span class="header-section-number">B</span> LISREL notation</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/VectorPosse/sem_book">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="multiple" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Multiple regression<a class="anchor" aria-label="anchor" href="#multiple"><i class="fas fa-link"></i></a>
</h1>
<div class="inline-figure"><img src="graphics/multiple_regression.png" width="450" style="display: block; margin: auto;"></div>
<div id="preliminaries-1" class="section level2 unnumbered">
<h2>Preliminaries<a class="anchor" aria-label="anchor" href="#preliminaries-1"><i class="fas fa-link"></i></a>
</h2>
<p>We will load the <code>tidyverse</code> package to work with tibbles, the <code>broom</code> package to calculate residuals, and <code>lavaan</code>.</p>
<div class="sourceCode" id="cb110"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://broom.tidymodels.org/">broom</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://lavaan.ugent.be">lavaan</a></span><span class="op">)</span></code></pre></div>
</div>
<div id="multiple-model" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> The multiple regression model<a class="anchor" aria-label="anchor" href="#multiple-model"><i class="fas fa-link"></i></a>
</h2>
<p>This chapter is an extension of all the ideas established in the <a href="simple.html#simple">last chapter</a>. Even if you have seen regression before reading this book, be sure to read and study the last chapter and this chapter thoroughly. If nothing else, you need to be comfortable with the notation and terminology established here. But we will also take special care to motivate and justify all the calculations that are taken for granted in some treatments of regression. This framework will be important as we move into mediation and path analysis in the next few chapters. If you are comfortable with the content of this chapter, there won’t be much “new” to say about mediation and path analysis more generally.</p>
<p>Multiple regression is like simple regression, but with more exogenous variables. There will still be only one endogenous variable. Although the archetype illustrated at the beginning of the chapter has three predictor variables, we will start with only two predictor variables to keep things simple. If you understand what happens with two variables, it’s fairly straightforward to generalize that knowledge to three or more predictors. The logic is the same.</p>
<p>Here is a multiple regression model with two predictors and with all paths given parameter labels:</p>
<div class="inline-figure"><img src="graphics/multiple_regression_2.png" width="438" style="display: block; margin: auto;"></div>
<div class="rmdnote">
<p>How many free parameters appear in this model?</p>
<p>How many fixed parameters appear in this model?</p>
</div>
<p>The equation describing the relationship among these variables can be written as either</p>
<p><span class="math display">\[
\hat{Y} = b_{1}X_{1} + b_{2}X_{2}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
Y = b_{1}X_{1} + b_{2}X_{2} + E
\]</span></p>
<div class="rmdnote">
<p>Why do we use <span class="math inline">\(\hat{Y}\)</span> in the first equation and <span class="math inline">\(Y\)</span> in the second equation?</p>
</div>
<p>Although we’ll work through the details for only two predictors, a multiple regression model with <span class="math inline">\(k\)</span> predictors will look like</p>
<p><span class="math display">\[
\hat{Y} = b_{1}X_{1} + b_{2}X_{2} + \dots + b_{k}X_{k}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
Y = b_{1}X_{1} + b_{2}X_{2}  + \dots + b_{k}X_{k} + E
\]</span></p>
</div>
<div id="multiple-assumptions" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Multiple regression assumptions<a class="anchor" aria-label="anchor" href="#multiple-assumptions"><i class="fas fa-link"></i></a>
</h2>
<p>Fortunately, the assumptions for multiple regression are basically the same as they are for simple regression with a few minor modifications and one addition:</p>
<ol style="list-style-type: decimal">
<li>The data should come from a “good” sample.</li>
<li>The relationship between <span class="math inline">\(X_{1}, \dots, X_{k}\)</span>, and <span class="math inline">\(Y\)</span> should be approximately linear.</li>
<li>The residuals should be independent of the <span class="math inline">\(X_{1}, \dots, X_{k}\)</span> values.</li>
<li>There should be no influential outliers.</li>
<li>The exogenous variables should not be highly correlated with one another.</li>
</ol>
<p>We discuss these briefly:</p>
<ol style="list-style-type: decimal">
<li>Nothing has changed here. Good analysis starts with good data collection practices.</li>
<li>With only <span class="math inline">\(Y\)</span> against <span class="math inline">\(X\)</span>, the regression model is a line. With <span class="math inline">\(Y\)</span> against <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>, the regression model is a plane (a 2-dimensional plane sitting in 3-dimensional space) which is a little challenging to graph. With more predictors, the regression model lives in even higher dimensions and it’s impossible to visualize. To check this condition, the best you can usually do is to check that the scatterplots of <span class="math inline">\(Y\)</span> against each <span class="math inline">\(X_{i}\)</span> individually are approximately linear.</li>
<li>Once we fit the model, we can check the residuals. Rather than plotting the residuals against each <span class="math inline">\(X_{i}\)</span> separately, we can employ a trick that we’ll explain later in the chapter.</li>
<li>Nothing changes here.</li>
<li>This is the new condition. When two or more predictors variables are highly correlated with each other, this induces a condition called <em>multicollinearity</em>.</li>
</ol>
<p>To illustrate why multicollinearity is a problem, think about the two-variable case:</p>
<p><span class="math display">\[
\hat{Y} = b_{1}X_{1} + b_{2}X_{2}
\]</span>
In general, we will be able to compute the values of <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> that best fit the model to data.</p>
<p>But now suppose that <span class="math inline">\(X_{2}\)</span> is just a multiple of <span class="math inline">\(X_{1}\)</span>, say <span class="math inline">\(X_{2} = 2X_{1}\)</span>. Now the equation looks more like</p>
<p><span class="math display">\[\begin{align}
\hat{Y} &amp;= b_{1}X_{1} + b_{2}X_{2} \\
        &amp;= b_{1}X_{1} + b_{2}(2 X_{1}) \\
        &amp;= (b_{1} + 2b_{2})X_{1}
\end{align}\]</span></p>
<p>So even though it “looked like” there were two distinct predictors variables, this is just a simple regression in disguise. Okay, so now let’s suppose we try to calculate the slope of this simple regression. Say it’s 10. What are the values of <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span>? In other words, what values of <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> solve the following equation?</p>
<p><span class="math display">\[
b_{1} + 2b_{2} = 10
\]</span></p>
<div class="rmdnote">
<p>Explain why it is impossible to pin down unique values for <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> that make the above equation true.</p>
<p>If you choose a large, negative value of <span class="math inline">\(b_{1}\)</span>, what does that imply about the value of <span class="math inline">\(b_{2}\)</span>?</p>
<p>If you choose a large, positive value of <span class="math inline">\(b_{1}\)</span>, what does that imply about the value of <span class="math inline">\(b_{2}\)</span>?</p>
</div>
<p>Multicollinearity works a lot like that. Even when variables are not exact multiples of each other, sets of highly correlated variables will result in equations with a large range of possible values that are consistent with the data. Even more dangerously, your fitting algorithm may estimate values for these coefficients, but those numbers will likely be meaningless. A completely different set of numbers may also be perfectly consistent with the data.</p>
<p>To be clear, it’s not a problem that there is covariance among our predictors. We expect that. The problem only arises when two or more predictors are <em>highly</em> correlated with each other.</p>
</div>
<div id="multiple-calculating" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> Calculating regression parameters<a class="anchor" aria-label="anchor" href="#multiple-calculating"><i class="fas fa-link"></i></a>
</h2>
<p>There is nothing new here, but the calculations do start to get a little messy. Everything that follows is for two predictors only. We will not do any calculations for three or more predictors. It gets out of hand pretty quickly.</p>
<p>First, let’s remember what we’re trying to do. From the data, we can calculate the sample covariance matrix. These are all the variances and covariances among the observed variables:</p>
<p><span class="math display">\[
\begin{bmatrix}
Var(X_{1})  &amp;   Cov(X_{1}, X_{2})   &amp;   Cov(X_{1}, Y) \\
\bullet     &amp;   Var(X_{2})          &amp;   Cov(X_{2}, Y) \\
\bullet     &amp;   \bullet             &amp;   Var(Y)
\end{bmatrix}
\]</span></p>
<p>Remember that these entries are all just numbers that we calculate directly from the data.</p>
<p>To get started on the model-implied matrix, let’s extend <a href="./covariance.html#Rule12"><strong>Rule 12</strong></a> a little.</p>
<div class="rmdimportant">
<p>For any three variables <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, and <span class="math inline">\(X_{3}\)</span>:</p>
<p><span class="math display">\[\begin{align}
Var(aX_{1} + bX_{2} + cX_{3}) &amp;=
    a^2Var(X_{1}) + b^2Var(X_{2}) + c^2Var(X_{3}) \\
    &amp; \quad + 2abCov(X_{1}, X_{2}) \\
    &amp; \quad + 2acCov(X_{1}, X_{3}) \\
    &amp; \quad + 2bcCov(X_{2}, X_{3})
\end{align}\]</span></p>
<p>This can be extended to any number of variables. Each variance appears with a coefficient squared and each pair of variables gets a covariance term with 2 times the product of the corresponding variable coefficients. (It’s hard to describe in words, but it’s still more trouble than it’s worth writing it down in formal mathematical notation. Hopefully you can see how the pattern of coefficients generalizes.)</p>
</div>
<p>Now we can compute, for example, <span class="math inline">\(Var(Y)\)</span>:</p>
<p><span class="math display">\[\begin{align}
Var(Y)  &amp;= Var(b_{1}X_{1} + b_{2}X_{2} + E) \\
    &amp;= b_{1}^{2} Var(X_{1}) + b_{2}^{2} Var(X_{2}) + Var(E) \\
    &amp; \quad + 2b_{1}b_{2} Cov(X_{1}, X_{2}) \\
    &amp; \quad + 2b_{1} Cov(X_{1}, E) \\
    &amp; \quad + 2b_{2} Cov(X_{2}, E)
\end{align}\]</span></p>
<div class="rmdnote">
<p>What happens to the last two lines above? Why?</p>
</div>
<p>Therefore,</p>
<p><span class="math display">\[
Var(Y) = b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e
\]</span></p>
<p><a href="./covariance.html#Rule8"><strong>Rule 8</strong></a> and <a href="./covariance.html#Rule9"><strong>Rule 9</strong></a> work the same way, but it’s even easier to apply. Just split up the covariance into as many pieces as there are terms to split.</p>
<div class="rmdnote">
<p>Your turn.</p>
<p>Calculate <span class="math inline">\(Cov(X_{1}, Y)\)</span>. You should get</p>
<p><span class="math display">\[
b_{1} v_{1} + b_{2} c_{12}
\]</span>
Calculate <span class="math inline">\(Cov(X_{2}, Y)\)</span>. You should get</p>
<p><span class="math display">\[
b_{2} v_{2} + b_{1} c_{12}
\]</span></p>
</div>
<p>That turns out to be all the computation we need to write down the model-implied matrix.</p>
<p>The first three entries are easy because they are just the parameters <span class="math inline">\(v_{1}\)</span>, <span class="math inline">\(c_{12}\)</span>, and <span class="math inline">\(v_{2}\)</span>. The last column contains the entries we just calculated above.</p>
<p>Therefore, the model-implied matrix is</p>
<p><span class="math display">\[
\begin{bmatrix}
v_{1}   &amp;    c_{12}  &amp;   b_{1} v_{1} + b_{2} c_{12} \\
\bullet &amp;    v_{2}   &amp;   b_{2} v_{2} + b_{1} c_{12} \\
\bullet &amp;    \bullet &amp;   b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e
\end{bmatrix}
\]</span>
If we set these expressions equal to the numbers from the sample covariance matrix, <em>in theory</em> we could then solve for the unknown parameters in the model-implied matrix above. Three of them are basically already done since we can just read off <span class="math inline">\(v_{1}\)</span>, <span class="math inline">\(c_{12}\)</span>, and <span class="math inline">\(v_{2}\)</span>. But solving for <span class="math inline">\(b_{1}\)</span>, <span class="math inline">\(b_{2}\)</span>, and <span class="math inline">\(e\)</span> is no joke! And even if we did, the resulting expressions are not particularly enlightening. This is where we are quite happy turning over the computational details to a computer.</p>
</div>
<div id="multiple-interpreting" class="section level2" number="5.4">
<h2>
<span class="header-section-number">5.4</span> Interpreting the coefficients<a class="anchor" aria-label="anchor" href="#multiple-interpreting"><i class="fas fa-link"></i></a>
</h2>
<p>Without explicit mathematical expressions for these parameters, it’s a bit challenging to explain their interpretation. For now, we’ll take it on faith that the following is true:</p>
<div class="rmdnote">
<p>In a multiple regression model, each <span class="math inline">\(b_{i}\)</span> represents the slope of the linear association between <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(Y\)</span> <em>while holding the value of all other predictors constant</em>.</p>
</div>
<p>What does this mean?</p>
<p>Let’s work with a concrete example. Suppose we think that college GPA can be predicted using high school GPA along with the number of hours per week spent studying in college. Here is what such a model might look like:</p>
<div class="inline-figure"><img src="graphics/multiple_regression_example.png" width="516" style="display: block; margin: auto;"></div>
<p>If high school GPA and hours per week studying are correlated (and they likely are), they influence each other, and some of the influence has the danger of “corrupting” the estimates of the path coefficients. For example, if <span class="math inline">\(b_{2}\)</span> is positive, that would suggest that hours spend studying is associated with predicted increases in college GPA. But how do we know that’s really due to the studying? Maybe students who did well in high school are just “smarter”. Sure, they also put in more hours studying, but maybe that doesn’t matter. Maybe those students would do just as well in college even if they didn’t study a whole lot. If that were the case, the coefficient <span class="math inline">\(b_{2}\)</span> would be positive just because that set of students (who happen to study more, even though it doesn’t matter) also are the ones who have high college GPAs.</p>
<p>This is why it’s important to <em>control</em> for other variables. All this means is that we need to temporarily fix the value of other variables to make the comparison fair. For example, we could look only at students with a 3.0 in high school. Among those students, there will be variability in the number of hours they study in college. If that variability is associated with variability in college GPA, we know that the hours spent studying is at least partly “responsible” for explaining that change. (There are lots of other factors too, but those will be swept up in the error variance.) The high school GPA can’t explain that because it was fixed at 3.0, so we’re comparing apples to apples. Students who got a 2.0 in high school may do more poorly overall, but the relative increase in GPA due to studying would be the same.</p>
<p>If the parameter <span class="math inline">\(b_{2}\)</span> is estimated to be 0.13, that suggests that each additional hour of study time per week predicts an increase of 0.13 points in the college GPA, holding high school GPA constant. This means that the increase of 0.13 is only predicted among groups of students with the same high school GPA.</p>
<p>If the parameter <span class="math inline">\(b_{1}\)</span> is estimated to be 1.2, that suggests that college GPA is predicted to increase 1.2 points for every point increase in high school GPA. This coefficient can only be interpreted while holding <code>HOURS</code> constant. This means that this estimate only holds among groups of students who put in the same number of hours of studying. That takes out the hours of studying as an explanation and only accounts for changes in high school GPA to explain changes in college GPA.</p>
</div>
<div id="multiple-standardized" class="section level2" number="5.5">
<h2>
<span class="header-section-number">5.5</span> Regression with standardized variables<a class="anchor" aria-label="anchor" href="#multiple-standardized"><i class="fas fa-link"></i></a>
</h2>
<p>Things get a little easier (although not completely trivial) with standardized variables.</p>
<p>First, a notational simplification. The correlations between our variables—according to our convention—would be called <span class="math inline">\(r_{X_{1}X_{2}}\)</span>, <span class="math inline">\(r_{X_{1}Y}\)</span>, and <span class="math inline">\(r_{X_{2}Y}\)</span>. These are a little hard to look at in complex expressions, so we will replace them with <span class="math inline">\(r_{12}\)</span>, <span class="math inline">\(r_{1Y}\)</span>, and <span class="math inline">\(r_{2Y}\)</span>.</p>
<p>Let’s look at the sample covariance matrix and the model-implied matrix for standardized variables:</p>
<p><span class="math display">\[
\begin{bmatrix}
1           &amp;   r_{12}      &amp;   r_{1Y} \\
\bullet     &amp;   1           &amp;   r_{2Y} \\
\bullet     &amp;   \bullet     &amp;   1
\end{bmatrix} =
\begin{bmatrix}
v_{1}   &amp;    c_{12}  &amp;   b_{1} v_{1} + b_{2} c_{12} \\
\bullet &amp;    v_{2}   &amp;   b_{2} v_{2} + b_{1} c_{12} \\
\bullet &amp;    \bullet &amp;   b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e
\end{bmatrix}
\]</span>
The entry in the upper-right corner yields</p>
<p><span class="math display">\[
r_{1Y} = b_{1} v_{1} + b_{2} c_{12}
\]</span>
which simplifies to
<span class="math display">\[
r_{1Y} = b_{1} + b_{2} r_{12}
\]</span>
The next entry below that yields</p>
<p><span class="math display">\[
r_{2Y} = b_{2} v_{2} + b_{1} c_{12}
\]</span>
which simplifies to
<span class="math display">\[
r_{2Y} = b_{2} + b_{1} r_{12}
\]</span></p>
<p>These two equations can be solved for the two unknown parameters <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span>.</p>
<div class="rmdnote">
<p>Are you feeling brave? Are your algebra skills sharp? Totally optional, but see if you can derive the final answers below:</p>
<p><span class="math display">\[
b_{1} = \frac{r_{1Y} - r_{2Y}r_{12}}{1 - r_{12}^{2}}
\]</span></p>
<p><span class="math display">\[
b_{2} = \frac{r_{2Y} - r_{1Y}r_{12}}{1 - r_{12}^{2}}
\]</span></p>
</div>
<p>These are still pretty gross, but there is some intuitive content to them. Look at the numerator of the fraction for <span class="math inline">\(b_{1}\)</span>. Essentially, this is just <span class="math inline">\(r_{1Y}\)</span> with some extra stuff. If this were simple regression, we would expect the slope <span class="math inline">\(b_{1}\)</span> to simply be the correlation between <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(Y\)</span>. But in multiple regression, we also have to <em>control</em> for any contribution to the model coming from <span class="math inline">\(X_{2}\)</span>. How do we do that? By subtracting off that contribution, which turns out to be <span class="math inline">\(r_{2Y}r_{12}\)</span>. And why does the latter term appear the way it does? Because we only need to control for the effect of <span class="math inline">\(X_{2}\)</span> if <span class="math inline">\(X_{2}\)</span> is providing some of the same “information” to the regression model as <span class="math inline">\(X_{1}\)</span>. Therefore, we don’t need to subtract <em>all</em> of <span class="math inline">\(r_{2Y}\)</span> to control for <span class="math inline">\(X_{2}\)</span>, just a fraction of <span class="math inline">\(r_{2Y}\)</span>. We just need the part of <span class="math inline">\(X_{2}\)</span> that it has in common with <span class="math inline">\(X_{1}\)</span>. We don’t want to “double-count” the contribution to the model that is common to both <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>.</p>
<div class="rmdnote">
<p>Here’s another way to think about it. What if <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> are independent? Calculate <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> from the above formulas in this much easier case. (Don’t overthink this. What is <span class="math inline">\(r_{12}\)</span> in this case?)</p>
</div>
<p>So if <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> are independent, they both offer a unique contribution to predicting <span class="math inline">\(Y\)</span> in the model. And that contribution is just their correlation with <span class="math inline">\(Y\)</span> (<span class="math inline">\(r_{1Y}\)</span> and <span class="math inline">\(r_{2Y}\)</span>, respectively). There is no overlap. But if <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> are correlated, then some of their “influence” is counted twice. We have to subtract out that influence so that <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> are only measuring the “pure” contribution of <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>, controlling for the other one.</p>
<p>What about the <span class="math inline">\(1 - r_{12}^{2}\)</span> in the denominator? There’s less of a good intuitive explanation here. It’s there because—mathematically speaking—it has to be there. It rescales the slope coefficients to make everything work out the way it has to.</p>
<p>The final equation is the one for <span class="math inline">\(Var(Y)\)</span> in the lower-right corner of the matrix. It says</p>
<p><span class="math display">\[
1 = b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e
\]</span>
which simplifies to
<span class="math display">\[
1 = b_{1}^{2} + b_{2}^{2} + 2b_{1}b_{2} r_{12} + e
\]</span>
Rearranging to solve for <span class="math inline">\(e\)</span>,
<span class="math display">\[
e = 1 - \left(b_{1}^{2} + b_{2}^{2} + 2b_{1}b_{2} r_{12}\right)
\]</span>
It is <em>not</em> enlightening in any way to replace <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> here with the earlier fractions. We can leave <span class="math inline">\(e\)</span> like this.</p>
<p>Since the variance of <span class="math inline">\(Y\)</span> is 1, the stuff inside the parentheses above represents the variance <em>explained by the model</em>. (That is subtracted from 1, then, to be left with <span class="math inline">\(e\)</span>, the error variance.) This is analogous to the <span class="math inline">\(R^{2}\)</span> term described in the <a href="#simple-error-correlation">last chapter</a>.</p>
<p>This makes some conceptual sense too. All the pieces of <span class="math inline">\(\left(b_{1}^{2} + b_{2}^{2} + 2b_{1}b_{2} r_{12}\right)\)</span> correspond to various pieces of the model. The first two relate to the direct effects of <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> and the third piece relates to an “indirect” effect shared between them.</p>
</div>
<div id="multiple-r" class="section level2" number="5.6">
<h2>
<span class="header-section-number">5.6</span> Multiple regression in R<a class="anchor" aria-label="anchor" href="#multiple-r"><i class="fas fa-link"></i></a>
</h2>
<p>Let’s fit a multiple regression model on some data about music. The data is a sample of 10,000 songs from the <a href="http://millionsongdataset.com/">Million Song Dataset</a>, a collection of metrics about the audio for a million contemporary popular music tracks.</p>
<p>This data set was downloaded from the <a href="https://corgis-edu.github.io/corgis/csv/">CORGIS Dataset Project</a> and more information about the variables in this data set can be found <a href="https://corgis-edu.github.io/corgis/csv/music/">here</a>.</p>
<div class="sourceCode" id="cb111"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">music</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://readr.tidyverse.org/reference/read_delim.html">read_csv</a></span><span class="op">(</span><span class="st">"https://raw.githubusercontent.com/VectorPosse/sem_book/main/data/music.csv"</span><span class="op">)</span></code></pre></div>
<pre><code>## Rows: 10000 Columns: 35
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: ","
## chr  (4): artist.id, artist.name, artist.terms, song.id
## dbl (31): artist.familiarity, artist.hotttnesss, artist.latitude, artist.loc...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<div class="sourceCode" id="cb113"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">music</span></code></pre></div>
<pre><code>## # A tibble: 10,000 × 35
##    artist.familiarity artist.hotttnes… artist.id artist.latitude artist.location
##                 &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;           &lt;dbl&gt;
##  1              0.582            0.402 ARD7TVE1…             0                 0
##  2              0.631            0.417 ARMJAGH1…            35.1               0
##  3              0.487            0.343 ARKRRTF1…             0                 0
##  4              0.630            0.454 AR7G5I41…             0                 0
##  5              0.651            0.402 ARXR32B1…             0                 0
##  6              0.535            0.385 ARKFYS91…             0                 0
##  7              0.556            0.262 ARD0S291…             0                 0
##  8              0.801            0.606 AR10USD1…             0                 0
##  9              0.427            0.332 AR8ZCNI1…             0                 0
## 10              0.551            0.423 ARNTLGG1…             0                 0
## # … with 9,990 more rows, and 30 more variables: artist.longitude &lt;dbl&gt;,
## #   artist.name &lt;chr&gt;, artist.similar &lt;dbl&gt;, artist.terms &lt;chr&gt;,
## #   artist.terms_freq &lt;dbl&gt;, release.id &lt;dbl&gt;, release.name &lt;dbl&gt;,
## #   song.artist_mbtags &lt;dbl&gt;, song.artist_mbtags_count &lt;dbl&gt;,
## #   song.bars_confidence &lt;dbl&gt;, song.bars_start &lt;dbl&gt;,
## #   song.beats_confidence &lt;dbl&gt;, song.beats_start &lt;dbl&gt;, song.duration &lt;dbl&gt;,
## #   song.end_of_fade_in &lt;dbl&gt;, song.hotttnesss &lt;dbl&gt;, song.id &lt;chr&gt;, …</code></pre>
<p>The endogenous variable of interest to us will be the measure of the song’s popularity, called <code>song.hotttnesss</code> (on a scale from 0 to 1).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Very important that there are three t’s and three s’s!&lt;/p&gt;"><sup>16</sup></a> There are many possible exogenous predictors, but let’s focus on three:</p>
<ul>
<li>
<code>artist.hotttnesss</code>
<ul>
<li>This is the popularity of the artist (on a scale from 0 to 1).</li>
</ul>
</li>
<li>
<code>song.loudness</code>
<ul>
<li>Not clear from the website what this is exactly, but it appears to be some kind of average dBFS (decibels relative to full scale). Numbers close to zero are actually as loud as recordings reasonably go and increasingly negative numbers represent softer volumes.</li>
</ul>
</li>
<li>
<code>song.tempo</code>
<ul>
<li>This is measured in beats per minute (BPM).</li>
</ul>
</li>
</ul>
<p>Let’s plot <code>song.hotttnesss</code> against each of the three proposed predictors to test the linearity assumption, starting with <code>artist.hotttnesss</code>:</p>
<div class="sourceCode" id="cb115"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">music</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">song.hotttnesss</span>,
                  x <span class="op">=</span> <span class="va">artist.hotttnesss</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="sem_book_files/figure-html/unnamed-chunk-77-1.png" width="672"></div>
<p>Uh, we’ve got some issues here to deal with. Since <code>song.hotttness</code> is supposed to be from 0 to 1, we can guess that the -1 values are likely coded to represent “missing” data. Even the values of 0 don’t seem valid given that there is a big gap between the row of zeros and any of the rest of the cluster of actual data. The <code>artist.hotttness</code> variable also seems to have some zeros that are disconnected from the rest of the data. These may be genuine outliers, but it’s more likely that these were artists for whom no data was collected.</p>
<p>While we’re suspecting issues, let’s also check <code>song.loudness</code> and <code>song.tempo</code>.</p>
<div class="sourceCode" id="cb116"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">music</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">song.hotttnesss</span>,
                  x <span class="op">=</span> <span class="va">song.loudness</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="sem_book_files/figure-html/unnamed-chunk-78-1.png" width="672"></div>
<p>The <code>song.loudness</code> distribution looks reasonable. It’s definitely skewed to the left, but there are no strict requirements about the predictor variables having any particular type of distribution.</p>
<div class="sourceCode" id="cb117"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">music</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">song.hotttnesss</span>,
                  x <span class="op">=</span> <span class="va">song.tempo</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="sem_book_files/figure-html/unnamed-chunk-79-1.png" width="672"></div>
<div class="rndnote">
<p>Is it possible for a song tempo to be 0 BPM?</p>
</div>
<p>To make it a little cleaner, the following code will <code>select</code> only the variables in which we’re interested. Then it will <code>filter</code> out the values we want to keep (discarding the ones that represent missing/invalid data). We’ll put this into a new tibble called <code>music_clean</code>.</p>
<div class="sourceCode" id="cb118"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">music_clean</span> <span class="op">&lt;-</span> <span class="va">music</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">song.hotttnesss</span>, <span class="va">artist.hotttnesss</span>,
           <span class="va">song.loudness</span>, <span class="va">song.tempo</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">song.hotttnesss</span> <span class="op">&gt;</span> <span class="fl">0</span>,
           <span class="va">artist.hotttnesss</span> <span class="op">&gt;</span> <span class="fl">0</span>,
           <span class="va">song.tempo</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span>
<span class="va">music_clean</span></code></pre></div>
<pre><code>## # A tibble: 4,157 × 4
##    song.hotttnesss artist.hotttnesss song.loudness song.tempo
##              &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;
##  1           0.602             0.402        -11.2        92.2
##  2           0.605             0.402         -4.50      130. 
##  3           0.266             0.332        -13.5        86.6
##  4           0.266             0.352         -7.54      118. 
##  5           0.405             0.448         -8.58      120. 
##  6           0.335             0.331        -16.1       128. 
##  7           0.684             0.513         -5.27      150. 
##  8           0.314             0.378         -8.05      112. 
##  9           0.667             0.542         -4.26      167. 
## 10           0.495             0.306        -12.3       138. 
## # … with 4,147 more rows</code></pre>
<p>This has reduced the number of rows to 4,157, but that is still a huge sample size.</p>
<p>Let’s check the scatterplots once more, now with the <code>music_clean</code> data.</p>
<div class="sourceCode" id="cb120"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">music_clean</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">song.hotttnesss</span>,
                        x <span class="op">=</span> <span class="va">artist.hotttnesss</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="sem_book_files/figure-html/unnamed-chunk-81-1.png" width="672"></div>
<div class="sourceCode" id="cb121"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">music_clean</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">song.hotttnesss</span>,
                        x <span class="op">=</span> <span class="va">song.loudness</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="sem_book_files/figure-html/unnamed-chunk-82-1.png" width="672"></div>
<div class="sourceCode" id="cb122"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">music_clean</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">song.hotttnesss</span>,
                        x <span class="op">=</span> <span class="va">song.tempo</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="sem_book_files/figure-html/unnamed-chunk-83-1.png" width="672"></div>
<p>There doesn’t appear to be much of an association with loudness or tempo. But that doesn’t violate any assumptions. (A violation of the assumptions would be a decidedly non-linear association, not just a near-zero association.) Given these graphs, we will expect the model to tell us that song popularity is somewhat associated with artist popularity, but not much with loudness or tempo.</p>
<div id="multiple-r-lm" class="section level3" number="5.6.1">
<h3>
<span class="header-section-number">5.6.1</span> Using <code>lm</code><a class="anchor" aria-label="anchor" href="#multiple-r-lm"><i class="fas fa-link"></i></a>
</h3>
<p>The <code>lm</code> model specification is a minor extension of what you learned for simple regression. Just use plus signs on the right side of the tilde ~ to add more predictors. Be sure to use <code>music_clean</code> and not <code>music</code>!</p>
<div class="sourceCode" id="cb123"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SONG_lm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">song.hotttnesss</span> <span class="op">~</span> <span class="va">artist.hotttnesss</span> <span class="op">+</span>
                  <span class="va">song.loudness</span> <span class="op">+</span>
                  <span class="va">song.tempo</span>,
              data <span class="op">=</span> <span class="va">music_clean</span><span class="op">)</span>
<span class="va">SONG_lm</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = song.hotttnesss ~ artist.hotttnesss + song.loudness + 
##     song.tempo, data = music_clean)
## 
## Coefficients:
##       (Intercept)  artist.hotttnesss      song.loudness         song.tempo  
##         0.1636446          0.7003692          0.0036969          0.0002057</code></pre>
<p>We didn’t go to the trouble of mean-centering the data this time, so the intercept is no longer 0. But we will not attempt to interpret the intercept anyway. The other three coefficients are <span class="math inline">\(b_{1}\)</span>, <span class="math inline">\(b_{2}\)</span> and <span class="math inline">\(b_{3}\)</span>, the path coefficients of the model. These are interpreted as follows:</p>
<ul>
<li>
<span class="math inline">\(b_{1}\)</span>:
<ul>
<li>Song popularity is predicted to increase 0.7 points for every point increase in artist popularity.</li>
</ul>
</li>
</ul>
<p>While this is mathematically true, it’s kind of nonsensical to report using numbers of that magnitude. Both scales only go from 0 to 1, so an increase in 1 point would be measuring the difference between an artist with 0 popularity (the least popular artists in the data set) to an artist with 1 popularity (the most popular artists in the data set).</p>
<p>A better way to report this would be to scale everything down by a factor of 10:</p>
<ul>
<li><p>Song popularity is predicted to increase 0.07 points for every 0.1 increase in artist popularity.</p></li>
<li>
<p><span class="math inline">\(b_{2}\)</span>:</p>
<ul>
<li>Song popularity is predicted to increase 0.004 points for every increase of 1 dB of loudness.</li>
</ul>
</li>
</ul>
<p>An increase of 1 dB is not very much, so again, we can scale the result to make it more meaningful. This time we’ll multiply by a factor of 10:</p>
<ul>
<li><p>Song popularity is predicted to increase 0.04 points for every increase of 10 dB of loudness.</p></li>
<li>
<p><span class="math inline">\(b_{3}\)</span>:</p>
<ul>
<li>Song popularity is predicted to increase 0.0002 points for every increase of 1 BPM in the tempo.</li>
</ul>
</li>
</ul>
<div class="rmdnote">
<p>Restate the interpretation of <span class="math inline">\(b_{3}\)</span> on a scale that makes sense. If you’re not familiar with BPM, Google it to get a sense of what a reasonable jump in tempo might be.</p>
</div>
<p>Now that we have the model fit, we can use <code>broom</code> to capture the residuals.</p>
<div class="sourceCode" id="cb125"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SONG_aug</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span><span class="va">SONG_lm</span><span class="op">)</span>
<span class="va">SONG_aug</span></code></pre></div>
<pre><code>## # A tibble: 4,157 × 10
##    song.hotttnesss artist.hotttnesss song.loudness song.tempo .fitted  .resid
##              &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1           0.602             0.402        -11.2        92.2   0.423  0.179 
##  2           0.605             0.402         -4.50      130.    0.455  0.149 
##  3           0.266             0.332        -13.5        86.6   0.364 -0.0984
##  4           0.266             0.352         -7.54      118.    0.406 -0.140 
##  5           0.405             0.448         -8.58      120.    0.470 -0.0652
##  6           0.335             0.331        -16.1       128.    0.362 -0.0273
##  7           0.684             0.513         -5.27      150.    0.535  0.150 
##  8           0.314             0.378         -8.05      112.    0.422 -0.108 
##  9           0.667             0.542         -4.26      167.    0.562  0.105 
## 10           0.495             0.306        -12.3       138.    0.361  0.134 
## # … with 4,147 more rows, and 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;,
## #   .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;</code></pre>
<p>But how do we graph them now that there are three predictor variables? We could graph the residuals against all three predictors separately, but there’s a more efficient method.</p>
<div class="rmdnote">
<p>Calculate</p>
<p><span class="math display">\[
Cov(E, \hat{Y})
\]</span>
by substituting</p>
<p><span class="math display">\[
\hat{Y} = b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}
\]</span></p>
<p>If we assume that <span class="math inline">\(E\)</span> is independent of all the predictors, what is the value of <span class="math inline">\(Cov(E, \hat{Y})\)</span>?</p>
</div>
<p>Of course, if <span class="math inline">\(Cov(E, \hat{Y}) = 0\)</span>, that does not necessarily imply that all the <span class="math inline">\(Cov(E, X_{i})\)</span> must be zero. And even if those are zero, that doesn’t imply independence. But if <span class="math inline">\(Cov(E, \hat{Y}) \neq 0\)</span>, then we know at least one of the <span class="math inline">\(Cov(E, X_{i})\)</span> also must be non-zero. <strong>Therefore, we can plot the residuals against the fitted values and this will serve as <em>disqualifying</em> evidence of a problem with the model.</strong></p>
<p>One of the nice features of the <code>augment</code> output is that it also has a column called <code>.fitted</code> that stores the <span class="math inline">\(\hat{Y}\)</span> values.</p>
<p>Here are the (standardized) residuals graphed against the fitted values:</p>
<div class="sourceCode" id="cb127"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">SONG_aug</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">.std.resid</span>, x <span class="op">=</span> <span class="va">.fitted</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>, color <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="sem_book_files/figure-html/unnamed-chunk-86-1.png" width="672"></div>
<p>The weirdness in the residuals is not ideal. It doesn’t prevent us from fitting the model, but we will state our results cautiously knowing that variance toward the left half of the graph is compressed relative to the right side of the graph. Therefore, the error variance is not “acting” in the model the same way across all combinations of the predictor variables.</p>
<div class="rmdnote">
<p>Go back and look at the original scatterplots of the data (from <code>music_clean</code> and see if you can figure out why the residuals are cut off funny like that in the lower left quadrant.)</p>
</div>
</div>
<div id="multiple-r-lavaan" class="section level3" number="5.6.2">
<h3>
<span class="header-section-number">5.6.2</span> Using <code>lavaan</code><a class="anchor" aria-label="anchor" href="#multiple-r-lavaan"><i class="fas fa-link"></i></a>
</h3>
<p>Model specification in <code>lavaan</code> happens in a separate step with the model in quotes:</p>
<div class="sourceCode" id="cb128"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SONG_model</span> <span class="op">&lt;-</span> <span class="st">"song.hotttnesss ~ artist.hotttnesss +
    song.loudness + 
    song.tempo"</span> </code></pre></div>
<p>Then the model is fit with the <code>sem</code> function.</p>
<div class="sourceCode" id="cb129"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SONG_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lavaan/man/sem.html">sem</a></span><span class="op">(</span><span class="va">SONG_model</span>, data <span class="op">=</span> <span class="va">music_clean</span><span class="op">)</span></code></pre></div>
<p>Here are the unstandardized parameter estimates:</p>
<div class="sourceCode" id="cb130"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/lavaan/man/parameterEstimates.html">parameterEstimates</a></span><span class="op">(</span><span class="va">SONG_fit</span><span class="op">)</span></code></pre></div>
<pre><code>##                  lhs op               rhs      est    se      z pvalue ci.lower
## 1    song.hotttnesss  ~ artist.hotttnesss    0.700 0.021 33.427  0.000    0.659
## 2    song.hotttnesss  ~     song.loudness    0.004 0.000  7.987  0.000    0.003
## 3    song.hotttnesss  ~        song.tempo    0.000 0.000  3.096  0.002    0.000
## 4    song.hotttnesss ~~   song.hotttnesss    0.021 0.000 45.591  0.000    0.020
## 5  artist.hotttnesss ~~ artist.hotttnesss    0.012 0.000     NA     NA    0.012
## 6  artist.hotttnesss ~~     song.loudness    0.112 0.000     NA     NA    0.112
## 7  artist.hotttnesss ~~        song.tempo    0.091 0.000     NA     NA    0.091
## 8      song.loudness ~~     song.loudness   25.426 0.000     NA     NA   25.426
## 9      song.loudness ~~        song.tempo   27.118 0.000     NA     NA   27.118
## 10        song.tempo ~~        song.tempo 1185.061 0.000     NA     NA 1185.061
##    ci.upper
## 1     0.741
## 2     0.005
## 3     0.000
## 4     0.022
## 5     0.012
## 6     0.112
## 7     0.091
## 8    25.426
## 9    27.118
## 10 1185.061</code></pre>
<p>Focus on the estimate column (<code>est</code>).</p>
<div class="rmdnote">
<p>Do you recognize the values from lines 1 through 3?</p>
<p>What does line 4 mean? (Hint: it’s not the variance of <code>song.hotttnesss</code> even though the notation makes it look like that.)</p>
<p>What’s going on in lines 5 through 10?</p>
</div>
<p>Here are the standardized parameter estimates:</p>
<div class="sourceCode" id="cb132"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/lavaan/man/standardizedSolution.html">standardizedSolution</a></span><span class="op">(</span><span class="va">SONG_fit</span><span class="op">)</span></code></pre></div>
<pre><code>##                  lhs op               rhs est.std    se      z pvalue ci.lower
## 1    song.hotttnesss  ~ artist.hotttnesss   0.459 0.012 39.345  0.000    0.436
## 2    song.hotttnesss  ~     song.loudness   0.111 0.014  8.051  0.000    0.084
## 3    song.hotttnesss  ~        song.tempo   0.042 0.014  3.100  0.002    0.016
## 4    song.hotttnesss ~~   song.hotttnesss   0.752 0.011 69.186  0.000    0.731
## 5  artist.hotttnesss ~~ artist.hotttnesss   1.000 0.000     NA     NA    1.000
## 6  artist.hotttnesss ~~     song.loudness   0.202 0.000     NA     NA    0.202
## 7  artist.hotttnesss ~~        song.tempo   0.024 0.000     NA     NA    0.024
## 8      song.loudness ~~     song.loudness   1.000 0.000     NA     NA    1.000
## 9      song.loudness ~~        song.tempo   0.156 0.000     NA     NA    0.156
## 10        song.tempo ~~        song.tempo   1.000 0.000     NA     NA    1.000
##    ci.upper
## 1     0.482
## 2     0.138
## 3     0.069
## 4     0.773
## 5     1.000
## 6     0.202
## 7     0.024
## 8     1.000
## 9     0.156
## 10    1.000</code></pre>
<p>Focus on the estimates again (<code>est.std</code>).</p>
<div class="rmdnote">
<p>Why is it easier to compare the values in lines 1 though 3 in this output than it was in the unstandardized table? (Hint: think about units of measurement or lack thereof.)</p>
<p>What does the value in line 4 tell you? (Hint: it’s closer to 1 than to 0.)</p>
<p>Why are lines 5, 8, and 10 equal to 1?</p>
<p>How do you interpret lines 6, 7, and 9?</p>
</div>
<p>This is the final model with all variables labeled and all unstandardized parameter estimates identified:</p>
<div class="inline-figure"><img src="graphics/multiple_regression_music.png" width="596" style="display: block; margin: auto;"></div>
<p>This is the same thing, but with standardized parameter estimates:</p>
<div class="inline-figure"><img src="graphics/multiple_regression_music_std.png" width="596" style="display: block; margin: auto;"></div>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="simple.html"><span class="header-section-number">4</span> Simple regression</a></div>
<div class="next"><a href="mediation.html"><span class="header-section-number">6</span> Mediation</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#multiple"><span class="header-section-number">5</span> Multiple regression</a></li>
<li><a class="nav-link" href="#preliminaries-1">Preliminaries</a></li>
<li><a class="nav-link" href="#multiple-model"><span class="header-section-number">5.1</span> The multiple regression model</a></li>
<li><a class="nav-link" href="#multiple-assumptions"><span class="header-section-number">5.2</span> Multiple regression assumptions</a></li>
<li><a class="nav-link" href="#multiple-calculating"><span class="header-section-number">5.3</span> Calculating regression parameters</a></li>
<li><a class="nav-link" href="#multiple-interpreting"><span class="header-section-number">5.4</span> Interpreting the coefficients</a></li>
<li><a class="nav-link" href="#multiple-standardized"><span class="header-section-number">5.5</span> Regression with standardized variables</a></li>
<li>
<a class="nav-link" href="#multiple-r"><span class="header-section-number">5.6</span> Multiple regression in R</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#multiple-r-lm"><span class="header-section-number">5.6.1</span> Using lm</a></li>
<li><a class="nav-link" href="#multiple-r-lavaan"><span class="header-section-number">5.6.2</span> Using lavaan</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/VectorPosse/sem_book/blob/main/05-Multiple_regression.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/VectorPosse/sem_book/edit/main/05-Multiple_regression.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Demystifying Structural Equation Modeling</strong>" was written by Jonathan Amburgey and Sean Raleigh, Westminster College (Salt Lake City, UT). It was last built on 2022-05-21.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
