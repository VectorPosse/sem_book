<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 6 Mediation | Demystifying Structural Equation Modeling</title>
<meta name="author" content="Jonathan Amburgey and Sean Raleigh, Westminster College (Salt Lake City, UT)">
<meta name="description" content="Preliminaries We will load the tidyverse package to work with tibbles and lavaan. library(tidyverse) library(lavaan)  6.1 Arrows going everywhere! To start off, let’s look at all possible paths...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 6 Mediation | Demystifying Structural Equation Modeling">
<meta property="og:type" content="book">
<meta property="og:url" content="https://vectorposse.github.io/sem_book/mediation.html">
<meta property="og:description" content="Preliminaries We will load the tidyverse package to work with tibbles and lavaan. library(tidyverse) library(lavaan)  6.1 Arrows going everywhere! To start off, let’s look at all possible paths...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 6 Mediation | Demystifying Structural Equation Modeling">
<meta name="twitter:description" content="Preliminaries We will load the tidyverse package to work with tibbles and lavaan. library(tidyverse) library(lavaan)  6.1 Arrows going everywhere! To start off, let’s look at all possible paths...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Demystifying Structural Equation Modeling</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Introduction</a></li>
<li><a class="" href="variables.html"><span class="header-section-number">1</span> Variables and measurement</a></li>
<li><a class="" href="variance.html"><span class="header-section-number">2</span> Variance</a></li>
<li><a class="" href="covariance.html"><span class="header-section-number">3</span> Covariance</a></li>
<li><a class="" href="simple.html"><span class="header-section-number">4</span> Simple regression</a></li>
<li><a class="" href="multiple.html"><span class="header-section-number">5</span> Multiple regression</a></li>
<li><a class="active" href="mediation.html"><span class="header-section-number">6</span> Mediation</a></li>
<li><a class="" href="path.html"><span class="header-section-number">7</span> Path analysis</a></li>
<li><a class="" href="latent.html"><span class="header-section-number">8</span> Latent variables</a></li>
<li><a class="" href="cfa.html"><span class="header-section-number">9</span> Confirmatory factor analysis</a></li>
<li><a class="" href="sem.html"><span class="header-section-number">10</span> Structural equation models</a></li>
<li><a class="" href="scm.html"><span class="header-section-number">11</span> Structural causal models</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="appendix-rules.html"><span class="header-section-number">A</span> Variance/covariance rules</a></li>
<li><a class="" href="appendix-lisrel.html"><span class="header-section-number">B</span> LISREL notation</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/VectorPosse/sem_book">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="mediation" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Mediation<a class="anchor" aria-label="anchor" href="#mediation"><i class="fas fa-link"></i></a>
</h1>
<div class="inline-figure"><img src="graphics/mediation.png" width="516" style="display: block; margin: auto;"></div>
<div id="preliminaries-2" class="section level2 unnumbered">
<h2>Preliminaries<a class="anchor" aria-label="anchor" href="#preliminaries-2"><i class="fas fa-link"></i></a>
</h2>
<p>We will load the <code>tidyverse</code> package to work with tibbles and <code>lavaan</code>.</p>
<div class="sourceCode" id="cb134"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://lavaan.ugent.be">lavaan</a></span><span class="op">)</span></code></pre></div>
</div>
<div id="mediation-arrows" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Arrows going everywhere!<a class="anchor" aria-label="anchor" href="#mediation-arrows"><i class="fas fa-link"></i></a>
</h2>
<p>To start off, let’s look at all possible paths that connect three variables with two arrows. (For the moment, we’ll leave out variances, covariances, and error terms.)</p>
<div class="inline-figure"><img src="graphics/mediator_right.png" width="440" style="display: block; margin: auto;"></div>
<div class="inline-figure"><img src="graphics/mediator_left.png" width="440" style="display: block; margin: auto;"></div>
<div class="inline-figure"><img src="graphics/confounder.png" width="440" style="display: block; margin: auto;"></div>
<div class="inline-figure"><img src="graphics/collider.png" width="440" style="display: block; margin: auto;"></div>
<p>The second model is just a copy of the first model reversed, so we can disregard it. The other three models are genuinely distinct models with somewhat different consequences for the relationship among the three variables:</p>
<ul>
<li>The first model represents a “mediator”.</li>
<li>The third model represents a “confounder”.</li>
<li>The fourth model represents a “collider”.</li>
</ul>
<p>The first part of this chapter will make these distinctions clear.</p>
</div>
<div id="mediation-exogenous-endogenous" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Exogenous and endogenous variables<a class="anchor" aria-label="anchor" href="#mediation-exogenous-endogenous"><i class="fas fa-link"></i></a>
</h2>
<p>Look at the first model above. It’s clear that the variable on the left is exogenous and the variable on the right is endogenous. The middle variable is called a <em>mediator</em>. Is it exogenous or endogenous?</p>
<p>Here we give a more specific definition of these two terms:</p>
<div class="rmdimportant">
<p>An <em>exogenous</em> variable is one that has no unidirectional arrows (so not counting double-headed arrows) entering it in the model diagram. It has only unidirectional arrows leaving it.</p>
<p>An <em>endogenous</em> variable is one that has at least one unidirectional arrow entering it (again, not counting double-headed arrows). It may have other unidirectional arrows both entering and/or leaving.</p>
</div>
<ul>
<li><p>The prefix <em>exo-</em> means “outside”. So whatever variability there is in an exogenous variable must come from “outside” the model. There are no unidirectional arrows coming in, so there is nothing in the model to account for its variance, or, for that matter, its covariance with other exogenous variables.</p></li>
<li><p>The prefix <em>endo-</em> means “within”. The variability of endogenous variables is accounted for by other variables (including error terms) inside the model. The fact that there might be arrows leaving endogenous variables is irrelevant for this definition. It’s only about arrows coming in.</p></li>
</ul>
<div class="rmdnote">
<p>According to the definition above, is a mediator exogenous or endogenous?</p>
</div>
<p>Here are the Really Important Rules (RIR™) for working with exogenous and endogenous variables in models. They come in three pairs:</p>
<div class="rmdimportant">
<ul>
<li>
<strong>Rule 1:</strong>
<ul>
<li>Every exogenous variable in a model requires a double-headed arrow pointing to itself, representing its variance.</li>
<li>No endogenous variable should have a double-headed arrow pointing to itself.</li>
</ul>
</li>
<li>
<strong>Rule 2:</strong>
<ul>
<li>Every pair of exogenous variables in a model—except error terms—requires a double-headed arrow joining them, representing their covariance.</li>
<li>No other pair of variables in a model (between exogenous and endogenous, or between endogenous) should have a double-headed arrow joining them.</li>
</ul>
</li>
<li>
<strong>Rule 3:</strong>
<ul>
<li>Every endogenous variable in a model requires an error term.</li>
<li>No exogenous variable in a model should have an error term.</li>
</ul>
</li>
</ul>
</div>
<p>These rules have important justifications. Don’t just memorize the rules blindly. Understand why they are imperative.</p>
<ul>
<li>
<strong>Rule 1:</strong>
<ul>
<li>Exogenous variables vary, but the source of their variance is not in the model. (That’s what makes them exogenous.) Therefore, we have to represent their variance “manually” in the model by indicating it with a double-headed arrow.</li>
<li>On the other hand, the variance of endogenous variables is accounted for by other variables in the model already, so it doesn’t need a separate parameter representing its variance.</li>
</ul>
</li>
<li>
<strong>Rule 2:</strong>
<ul>
<li>Pairs of exogenous variables co-vary. The source of that covariance is not in the model, so we have to represent it “manually” by indicating it with a double-headed arrow. Error terms are the exception to this rule. While it’s possible that error terms can co-vary, that usually isn’t sensible for most models. A future chapter [LINK] will cover how and when error terms can be correlated, but it should never be the default assumption of the model.</li>
<li>Covariances between other types of variables (exogenous to endogenous, or endogenous to endogenous) are consequences of the other arrows in the diagram that create direct and indirect paths among the variables, so their covariance is not separately drawn as a double-headed arrow.</li>
</ul>
</li>
<li>
<strong>Rule 3:</strong>
<ul>
<li>While the model is supposed to account for the variance of endogenous variables through incoming arrows, it will never be able to explain 100% of that variance just using other variables in the model. There will always be residuals, so these residuals have to be represented “manually” in the model using error terms.</li>
<li>Exogenous variables are assumed to be measured without error. While that assumption is not always very realistic in the real world, we don’t have much of a choice. By their very definition, the variance of exogenous variable isn’t accounted for by anything else in the model, so error terms just don’t make any sense for them.</li>
</ul>
</li>
</ul>
<div class="rmdnote">
<p>Here’s the first model of the four shown earlier:</p>
<div class="inline-figure"><img src="graphics/mediator_right.png" width="440" style="display: block; margin: auto;"></div>
<p>Draw this model on your own piece of paper. Following the rules above, draw in all variances, covariances, or error terms that should be present in the diagram. (Don’t worry about labeling anything with letters yet. Just draw the arrows and the circles.)</p>
</div>
</div>
<div id="mediation-naming" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Naming conventions<a class="anchor" aria-label="anchor" href="#mediation-naming"><i class="fas fa-link"></i></a>
</h2>
<p>We need to establish some conventions for naming things.</p>
<ul>
<li>We need to name our variables. When we model real-world data, we’ll use contextually meaningful names, but for abstract models we draw, we need a consistent way of labeling them.
<ul>
<li>Exogenous variables will be called <span class="math inline">\(X_{i}\)</span> (using numbers as subscripts).</li>
<li>Endogenous variables will be called <span class="math inline">\(Y_{i}\)</span> (also using numbers as subscripts).</li>
<li>Error terms will be called <span class="math inline">\(E_{i}\)</span> with subscripts matching the ones on the endogenous variables <span class="math inline">\(Y_{i}\)</span> to which they’re attached.</li>
</ul>
</li>
<li>We need to label the parameters along the various paths of the model:
<ul>
<li>Variances will be called <span class="math inline">\(v_{i}\)</span> with subscripts matching the exogenous variables <span class="math inline">\(X_{i}\)</span> to which they’re attached.</li>
<li>Error variances will be called <span class="math inline">\(e_{i}\)</span> with subscripts matching the error terms <span class="math inline">\(E_{i}\)</span> to which they’re attached.</li>
<li>Covariances will be called <span class="math inline">\(c_{ij}\)</span> connecting exogenous variables <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(X_{j}\)</span>. (Since covariance is symmetric, it could also be called <span class="math inline">\(c_{ji}\)</span>.)</li>
<li>Unidirectional arrows from error terms to their corresponding endogenous variables will always be fixed parameters labeled with “1”.</li>
<li>Thick, unidirectional arrows between an exogenous variable <span class="math inline">\(X_{i}\)</span> and an endogenous variable <span class="math inline">\(Y_{j}\)</span> will be called <span class="math inline">\(b_{ji}\)</span>. Note the order of the subscripts: we always start with the subscript of the target variable and end with the subscript of the predictor.</li>
<li>Thick, unidirectional arrows between an endogenous variable <span class="math inline">\(Y_{i}\)</span> and another endogenous variable <span class="math inline">\(Y_{j}\)</span> will be called <span class="math inline">\(a_{ji}\)</span>.</li>
</ul>
</li>
</ul>
<div class="rmdnote">
<p>Why do we not need a naming convention for thick arrows between two exogenous variables?</p>
</div>
</div>
<div id="mediation-mediators" class="section level2" number="6.4">
<h2>
<span class="header-section-number">6.4</span> Mediators<a class="anchor" aria-label="anchor" href="#mediation-mediators"><i class="fas fa-link"></i></a>
</h2>
<p>With all the rules in place for our diagrams, we can now revisit the model from above, but now, let’s include all the extra bits of the model required by the aforementioned rules: a variance term for the exogenous variable <span class="math inline">\(X_{1}\)</span>, error terms for the two endogenous variables <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span>, and parameter labels for everything.</p>
<div class="inline-figure"><img src="graphics/mediator_vars.png" width="440" style="display: block; margin: auto;"></div>
<div class="rmdnote">
<p>Why did we not include any covariances in the model above?</p>
</div>
<p>As a concrete example to illustrate this phenomenon, imagine that the variables measure the following:</p>
<ul>
<li>
<span class="math inline">\(X_{1}\)</span> is smoking.</li>
<li>
<span class="math inline">\(Y_{1}\)</span> is tar deposits in the lungs.</li>
<li>
<span class="math inline">\(Y_{2}\)</span> is lung cancer.</li>
</ul>
<p>The idea is that smoking is associated with lung cancer. But what smoking <em>really</em> does is cause specific chemical processes in the lungs (including the deposition of tar), and that—among other factors—is what contributes to lung cancer. Tar serves as a “mediator” for the process that connects smoking to lung cancer.</p>
<p>Since there are two endogenous variables present in this model, there are two regression equations we have to write down:</p>
<p><span class="math display">\[\begin{align}
Y_{1} &amp;= b_{11}X_{1} + E_{1}        \\
Y_{2} &amp;= a_{21}Y_{1} + E_{2}
\end{align}\]</span></p>
<p>The sample covariance matrix will look like</p>
<p><span class="math display">\[
\begin{bmatrix}
Var(X_{1})          &amp;   \bullet             &amp;   \bullet \\
Cov(Y_{1}, X_{1})   &amp;   Var(Y_{1})          &amp;   \bullet \\
Cov(Y_{2}, X_{1})   &amp;   Cov(Y_{2}, Y_{1})   &amp;   Var(Y_{2})
\end{bmatrix}
\]</span></p>
<div class="rmdnote">
<p>When working through covariance calculations in the past chapters, we’ve seen lots of terms pop out of the form <span class="math inline">\(Cov(E, X)\)</span>. We’ve gotten used to canceling these terms because they are zero. (Why must they be zero?)</p>
<p>In this model, some of the covariance calculations will result in terms of the form <span class="math inline">\(Cov(E, Y)\)</span>. These will not necessarily cancel, so we need to be more cautious.</p>
<p>Calculate <span class="math inline">\(Cov(E_{1}, Y_{1})\)</span> for the model above by substituting the regression equation <span class="math inline">\(Y_{1} = b_{11}X_{1} + E_{1}\)</span>. You should get <span class="math inline">\(e_{1}\)</span> (and <em>not</em> zero).</p>
<p>Without doing any calculations, why would we also expect <span class="math inline">\(Cov(E_{1}, Y_{2})\)</span> to be non-zero? (Hint: how are <span class="math inline">\(E_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> connected in the diagram?)</p>
<p>On the other hand, why would we expect <span class="math inline">\(Cov(E_{1}, E_{2})\)</span> to be zero in general? (Hint: look back to Really Important Rule 2 above.)</p>
<p>Finally, we <em>will</em> expect <span class="math inline">\(Cov(E_{2}, Y_{1})\)</span> to be zero. Why? If you’re stuck, go ahead and do the calculation to confirm.</p>
</div>
<div class="rmdnote">
<p>Calculate the full model-implied matrix. You should get the following:</p>
<p><span class="math display">\[
\begin{bmatrix}
v_{1}   &amp;   \bullet &amp;  \bullet  \\
b_{11}v_{1}  &amp;   b_{11}^{2}v_{1} + e_{1}   &amp;  \bullet \\
a_{21}b_{11}v_{1}  &amp;  a_{21}b_{11}^{2}v_{1} + a_{21}e_{1}  &amp;   a_{21}^{2}b_{11}^{2}v_{1} + a_{21}^2e_{1} + e_{2}
\end{bmatrix}
\]</span></p>
<p>If this is too tedious and time-consuming, just pick one or two of these entries to compute.</p>
</div>
<p>If we standardize our variables, the sample covariance matrix (which is now a correlation matrix) is</p>
<p><span class="math display">\[
\begin{bmatrix}
1                   &amp;   \bullet             &amp;   \bullet \\
Corr(Y_{1}, X_{1})  &amp;   1                   &amp;   \bullet \\
Corr(Y_{2}, X_{1})  &amp;   Corr(Y_{2}, Y_{1})   &amp;   1
\end{bmatrix}
\]</span>
We’ve switched to using <span class="math inline">\(Corr\)</span> instead of using the letter <span class="math inline">\(r\)</span> for this exercise. That’s because <span class="math inline">\(r_{Y_{1}X_{1}}\)</span>, <span class="math inline">\(r_{Y_{2}X_{1}}\)</span>, and <span class="math inline">\(r_{Y_{2}Y_{1}}\)</span> have subscripts inside of subscripts and are hard to read and process.</p>
<div class="rmdnote">
<p>Setting the correlation matrix equal to the model-implied matrix above, we get
<span class="math display">\[
v_{1} = 1
\]</span>
pretty much for free.</p>
<p>Now solve for <span class="math inline">\(b_{11}\)</span> and <span class="math inline">\(e_{1}\)</span> next using the two terms in the second row of the matrix. You should get:</p>
<p><span class="math display">\[\begin{align}
b_{11}  &amp;= Corr(Y_{1}, X_{1})       \\
e_{1}   &amp;= 1 - Corr(Y_{1}, X_{1})^{2}
\end{align}\]</span></p>
<p>Why is this not surprising? (Hint: if you ignore <span class="math inline">\(Y_{2}\)</span> altogether and only pay attention to relationships between <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(Y_{1}\)</span>, and <span class="math inline">\(E_{1}\)</span>, what kind of model is this?)</p>
</div>
<p>The parameter <span class="math inline">\(a_{21}\)</span> is interesting. The equation implied by the lower-left element of the matrix—corresponding to <span class="math inline">\(Corr(Y_{2}, X_{1})\)</span>—is</p>
<p><span class="math display">\[\begin{align}
Corr(Y_{2}, X_{1})  &amp;= a_{21}b_{11}v_{1}        \\
                    &amp;= a_{21}Corr(Y_{1}, X_{1})
\end{align}\]</span></p>
<p>Solving for <span class="math inline">\(a_{21}\)</span>:</p>
<p><span class="math display">\[
a_{21} = \frac{Corr(Y_{2}, X_{1})}{Corr(Y_{1}, X_{1})}
\]</span></p>
<p>On the other hand, the equation implied by the center element on the bottom row of the matrix—corresponding to <span class="math inline">\(Corr(Y_{2}, Y_{1})\)</span>—is</p>
<p><span class="math display">\[\begin{align}
Corr(Y_{2}, Y_{1})
    &amp;= a_{21}b_{11}^{2}v_{1} + a_{21}e_{1}  \\
    &amp;= a_{21}Corr(Y_{1}, X_{1})^{2} +
        a_{21}\left( 1 - Corr(Y_{1}, X_{1})^{2} \right) \\
    &amp;= a_{21}Corr(Y_{1}, X_{1})^{2} +
        a_{21} - a_{21}Corr(Y_{1}, X_{1})^{2}       \\
    &amp;= a_{21}
\end{align}\]</span></p>
<p>So we get two different answers for <span class="math inline">\(a_{21}\)</span>!</p>
<div class="rmdnote">
<p>Use <span class="math inline">\(a_{21} = Corr(Y_{2}, Y_{1})\)</span> along with everything else you’ve learned to solve for <span class="math inline">\(e_{2}\)</span> using the equation in the lower-right corner of the matrix. (This is the only one we can use because it’s the only term involving <span class="math inline">\(e_{2}\)</span>!)</p>
<p>It may look ugly, but you might be surprised at the simplicity of the answer that pops out. You should get
<span class="math display">\[
e_{2} = 1 - Corr(Y_{2}, Y_{1})^{2}
\]</span></p>
<p>Again, though, why is that not really all that surprising? (Hint: what if you ignore <span class="math inline">\(X_{1}\)</span> and treat the relationship between <span class="math inline">\(Y_{1}\)</span>, <span class="math inline">\(Y_{2}\)</span>, and <span class="math inline">\(E_{2}\)</span> as a simple regression?)</p>
</div>
<p>Since we got two different answers for <span class="math inline">\(a_{21}\)</span>, if this model is correct, they must be equal:</p>
<p><span class="math display">\[
a_{21} = Corr(Y_{2}, Y_{1}) = \frac{Corr(Y_{2}, X_{1})}{Corr(Y_{1}, X_{1})}
\]</span></p>
<p>which, if we rearrange the fraction, implies</p>
<p><span class="math display">\[
Corr(Y_{1}, X_{1}) Corr(Y_{2}, Y_{1}) = Corr(Y_{2}, X_{1})
\]</span></p>
<p>Another way to state this is that the correlation along the first path (<span class="math inline">\(b_{11}\)</span>) and the correlation along the second path (<span class="math inline">\(a_{21}\)</span>) multiply to give the correlation along both paths combined.</p>
<p>But these three correlations are numbers that are measured using the data. Is there any guarantee that the product of two of the correlations will necessarily equal the third?</p>
<p><strong>No!</strong></p>
<p>In fact, this will almost never be true with real data.</p>
<div class="rmdnote">
<p>So if the model <em>implies</em> that there must be a mathematical relationship among the correlations, but the data does not support that implication, what does that say about the model?</p>
</div>
<div class="rmdnote">
<p>Think about the ramifications of the above discussion for smoking and lung cancer. Smoking is correlated to tar deposits, and tar deposits are correlated with lung cancer. But if the product of those two correlations doesn’t equal the overall correlation between smoking and lung cancer, what does that say about the model? What other “path” might be missing in the model that would help account for the discrepancy?</p>
</div>
<p>We’ll return to this example in a moment. But first, let’s explore the other model configurations set up at the beginning of the chapter.</p>
</div>
<div id="mediation-confounders" class="section level2" number="6.5">
<h2>
<span class="header-section-number">6.5</span> Confounders<a class="anchor" aria-label="anchor" href="#mediation-confounders"><i class="fas fa-link"></i></a>
</h2>
<p>The variable in the middle of the diagram below is called a “confounder”:</p>
<div class="inline-figure"><img src="graphics/confounder.png" width="440" style="display: block; margin: auto;"></div>
<div class="rmdnote">
<p>Draw this model on your own piece of paper.</p>
<p>Identify which variables are exogenous or endogenous.</p>
<p>Following the Really Important Rules, draw in all variances, covariances, or error terms that should be present in the diagram.</p>
<p>Finally, see if you can label all paths with letters and subscripts according to the naming conventions described earlier.</p>
</div>
<p>Here is the final model:</p>
<div class="inline-figure"><img src="graphics/confounder_vars.png" width="440" style="display: block; margin: auto;"></div>
<p>As a concrete example to illustrate this phenomenon, imagine that the variables measure the following:</p>
<ul>
<li>
<span class="math inline">\(Y_{1}\)</span> is the presence of power lines near homes.</li>
<li>
<span class="math inline">\(Y_{2}\)</span> is the incidence of cancer.</li>
</ul>
<p>For a moment, we’re not going say what <span class="math inline">\(X_{1}\)</span> is.</p>
<div class="rmdnote">
<p>Does a positive correlation between power lines and cancer imply that living near power lines <em>causes</em> cancer?</p>
<p>Does a positive correlation between power lines and cancer imply that cancer <em>causes</em> people to live near power lines? (Okay, that one is a little ridiculous, but we’re making a point here.)</p>
<p>So if <span class="math inline">\(Y_{1}\)</span> doesn’t cause <span class="math inline">\(Y_{2}\)</span> and <span class="math inline">\(Y_{2}\)</span> doesn’t cause <span class="math inline">\(Y_{1}\)</span>, why else might they be correlated?</p>
</div>
<p>There may be several plausible answers to the last question above, but here is one possibility:</p>
<ul>
<li>
<span class="math inline">\(X_{1}\)</span> is poverty.</li>
</ul>
<div class="rmdnote">
<p>Give a plausible explanation for how poverty might be correlated to <em>both</em> living near power lines and cancer.</p>
</div>
<p>Now we turn our attention to the mathematics.</p>
<p>The two regression equations are</p>
<p><span class="math display">\[\begin{align}
Y_{1} &amp;= b_{11}X_{1} + E_{1}        \\
Y_{2} &amp;= b_{21}X_{1} + E_{2}
\end{align}\]</span></p>
<p>The sample covariance matrix will look exactly the same as it did for the mediation model above.</p>
<p><span class="math display">\[
\begin{bmatrix}
Var(X_{1})          &amp;   \bullet             &amp;   \bullet \\
Cov(Y_{1}, X_{1})   &amp;   Var(Y_{1})          &amp;   \bullet \\
Cov(Y_{2}, X_{1})   &amp;   Cov(Y_{2}, Y_{1})   &amp;   Var(Y_{2})
\end{bmatrix}
\]</span>
This is because there are still three observed variables and they have the same three names, even if they are connected with arrows in a different way.</p>
<p>This also means the sample correlation matrix is the same:</p>
<p><span class="math display">\[
\begin{bmatrix}
1                   &amp;   \bullet             &amp;   \bullet \\
Corr(Y_{1}, X_{1})  &amp;   1                   &amp;   \bullet \\
Corr(Y_{2}, X_{1})  &amp;   Corr(Y_{2}, Y_{1})   &amp;   1
\end{bmatrix}
\]</span></p>
<div class="rmdnote">
<p>Calculate the full model-implied matrix. You should get the following:</p>
<p><span class="math display">\[
\begin{bmatrix}
v_{1}   &amp;   \bullet &amp;  \bullet  \\
b_{11}v_{1}  &amp;   b_{11}^{2}v_{1} + e_{1}    &amp;  \bullet \\
b_{21}v_{1}  &amp;   b_{11}b_{21}v_{1}          &amp;  b_{21}^{2}v_{1} + e_{2}
\end{bmatrix}
\]</span></p>
<p>Don’t slack off on this one! Unlike the mediation example, all these terms are very straightforward to compute.</p>
</div>
<div class="rmdnote">
<p>Now calculate the standardized solution. In other words, solve for all the free parameters using the sample correlation matrix.</p>
<p>You should get the following:</p>
<p><span class="math display">\[\begin{align}
v_{1}   &amp;= 1                          \\
b_{11}  &amp;= Corr(Y_{1}, X_{1})         \\
e_{1}   &amp;= 1 - Corr(Y_{1}, X_{1})^{2} \\
e_{2}   &amp;= 1 - Corr(Y_{2}, X_{1})^{2}
\end{align}\]</span></p>
<p>Check that two of the equations give two different solutions for <span class="math inline">\(b_{21}\)</span>:</p>
<p><span class="math display">\[\begin{align}
b_{21}  &amp;= Corr(Y_{2}, X_{1})                             \\
b_{21}  &amp;= \frac{Corr(Y_{2}, Y_{1})}{Corr(Y_{1}, X_{1})}
\end{align}\]</span></p>
</div>
<p>The last calculation implies that</p>
<p><span class="math display">\[
Corr(Y_{1}, X_{1}) Corr(Y_{2}, X_{1}) = Corr(Y_{2}, Y_{1})
\]</span></p>
<p>Another way to state this is that the correlation along the first path (<span class="math inline">\(b_{11}\)</span>) and the correlation along the second path (<span class="math inline">\(b_{21}\)</span>) multiply to give the correlation along both paths combined.</p>
<p>But these three correlations are numbers that are measured using the data. Is there any guarantee that the product of two of the correlations will necessarily equal the third?</p>
<p><strong>No!</strong></p>
<p>In fact, this will almost never be true with real data.</p>
<div class="rmdnote">
<p>So if the model <em>implies</em> that there must be a mathematical relationship among the correlations, but the data does not support that implication, what does that say about the model?</p>
</div>
<p>Does this all sound familiar? It’s even more déjà vu than you think. Here are the standardized parameter solutions from the mediator example:</p>
<p><span class="math display">\[\begin{align}
v_{1}   &amp;= 1                            \\
e_{1}   &amp;= 1 - Corr(Y_{1}, X_{1})^{2}   \\
e_{2}   &amp;= 1 - Corr(Y_{2}, Y_{1})^{2}      \\
b_{11}  &amp;= Corr(Y_{1}, X_{1})           \\
a_{21}  &amp;= Corr(Y_{2}, Y_{1}) = \frac{Corr(Y_{2}, X_{1})}{Corr(Y_{1}, X_{1})}
\end{align}\]</span></p>
<p>And here are the standardized parameter solutions from the confounder example:</p>
<p><span class="math display">\[\begin{align}
v_{1}   &amp;= 1                            \\
e_{1}   &amp;= 1 - Corr(Y_{1}, X_{1})^{2}   \\
e_{2}   &amp;= 1 - Corr(Y_{2}, X_{1})^{2}   \\
b_{11}  &amp;= Corr(Y_{1}, X_{1})           \\
b_{21}  &amp;= Corr(Y_{2}, X_{1}) = \frac{Corr(Y_{2}, Y_{1})}{Corr(Y_{1}, X_{1})}
\end{align}\]</span></p>
<p>Other than just a change of notation—owing to the fact that the roles of <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(Y_{1}\)</span> are reversed in the confounder example—the solutions are <em>identical</em>.</p>
<div class="rmdnote">
<p>Think about the ramifications of the above discussion for living near power lines and cancer. Living near power lines is correlated to poverty, and poverty is correlated with cancer. But if the product of those two correlations doesn’t equal the overall correlation between power lines and cancer, what does that say about the model? What other “path” might be missing in the model that would help account for the discrepancy?</p>
<p>Now suppose that scientists are able to use a carefully controlled experiment (ethical considerations aside) to prove that there is no direct effect of power lines on cancer. Note that this is <em>not</em> the same thing as saying that the correlation between power lines and cancer is zero. How does the model explain this?</p>
</div>
<p>When a confounder accounts for all (or nearly all) the covariance between two variables, the resulting association is called <em>spurious</em>. The association exists, but it doesn’t exist due to any direct pathway.</p>
<div class="rmdnote">
<p>Given that the mediator model and the confounder model are <em>statistically identical</em>, why would you use one model versus the other? Are there “philosophical” differences between mediators and confounders, even though the two models give the same results?</p>
</div>
<p>One of the most important takeaways from this section is the realization that the arrows along an indirect path between two variables need not go in the same direction to imply a statistical relationship between those variables.</p>
<p>For a mediator, the arrows do go the same way:</p>
<p><span class="math display">\[
X_{1} \boldsymbol{\rightarrow} Y_{1} \boldsymbol{\rightarrow} Y_{2}
\]</span>
And it’s no surprise to anyone that <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> are associated. The association is “transmitted” from <span class="math inline">\(X_{1}\)</span> to <span class="math inline">\(Y_{1}\)</span> and then from <span class="math inline">\(Y_{1}\)</span> to <span class="math inline">\(Y_{2}\)</span> in an obvious way.</p>
<p>But for a confounder, the arrows don’t go the same way:</p>
<p><span class="math display">\[
Y_{1} \boldsymbol{\leftarrow} X_{1} \boldsymbol{\rightarrow} Y_{2}
\]</span>
And, yet, there is still an association between <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span>. Sometimes even “backwards” arrows can “transmit” an association through indirect pathways. This is often called a “backdoor path”.</p>
<p>But there are limits to that logic. The next example will illustrate.</p>
</div>
<div id="mediation-colliders" class="section level2" number="6.6">
<h2>
<span class="header-section-number">6.6</span> Colliders<a class="anchor" aria-label="anchor" href="#mediation-colliders"><i class="fas fa-link"></i></a>
</h2>
<p>The variable in the middle of the diagram below is called a “collider”:</p>
<div class="inline-figure"><img src="graphics/collider.png" width="440" style="display: block; margin: auto;"></div>
<div class="rmdnote">
<p>Draw this model on your own piece of paper.</p>
<p>Identify which variables are exogenous or endogenous.</p>
<p>Following the Really Important Rules, draw in all variances, covariances, or error terms that should be present in the diagram.</p>
<p>Finally, see if you can label all path with letters and subscripts according to the naming conventions described earlier.</p>
</div>
<p>Here is the final model:</p>
<div class="inline-figure"><img src="graphics/collider_vars.png" width="440" style="display: block; margin: auto;"></div>
<p>As a concrete example to illustrate this phenomenon, imagine that the variables measure the following:</p>
<ul>
<li>
<span class="math inline">\(X_{1}\)</span> is the height of basketball players.</li>
<li>
<span class="math inline">\(X_{2}\)</span> is the shooting accuracy of basketball players.</li>
<li>
<span class="math inline">\(Y_{1}\)</span> is the probability of being selected to play in a professional league.</li>
</ul>
<p>The paths <span class="math inline">\(b_{11}\)</span> and <span class="math inline">\(b_{12}\)</span> make sense. Taller players and players who shoot the ball better are more likely to make it to a professional level of play. These are positive associations. Do these two paths together create an indirect path between height and shooting that accounts for covariance between them?</p>
<p>It turns out the answer is no!</p>
<div class="rmdnote">
<p>This collider model is masquerading as another model that you have already studied in a previous chapter. It was drawn a little differently there, but the relationships among the variables and arrows are exactly the same. What is that model?</p>
</div>
<div class="rmdnote">
<p>Are there any restrictions on the value of <span class="math inline">\(c_{12}\)</span> in a multiple regression model?</p>
</div>
<p>While the value of <span class="math inline">\(c_{12}\)</span> does change the interpretation of the path coefficients in a multiple regression model, analysis of the model-implied matrix always results in</p>
<p><span class="math display">\[
c_{12} = Cov(X_{1}, X_{2})
\]</span></p>
<p>That value is just calculated directly from the data. It does not depend on any other parameter of the model. Since <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> are exogenous, the source of this covariance is independent of anything else in the model. In particular, it’s possible that <span class="math inline">\(c_{12} = 0\)</span>.</p>
<p>For example, in the basketball scenario, there’s no reason to believe that height and shooting ability are correlated in the general population. The fact that they are both correlated with a higher probability of being in a professional league is irrelevant to their correlation in the population. Even if they were correlated in the population (<span class="math inline">\(c_{12} \neq 0\)</span>), this would have nothing to do with the collider variable.</p>
<p>There’s no math to do in this section. All the math we need was already done in the <a href="multiple.html#multiple">previous chapter</a>.</p>
<p>The takeaway message here is that colliders do not transmit association through them. Pathways like the following do not imply anything about the association between <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>:</p>
<p><span class="math display">\[
X_{1} \boldsymbol{\rightarrow} Y_{1} \boldsymbol{\leftarrow} X_{2}
\]</span></p>
<p>This does not mean that <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> are uncorrelated. They may be correlated, but this correlation must arise from some other source—either “nature”, external to the model (exogenous covariance), or some other path in the model (perhaps through a mediator or confounder).</p>
<p>It is not a problem to have colliders in a model. In fact, as we’ll see below, every model we have analyzed in this course so far contains collider variables! The goal here is just to understand that they do not provide indirect paths for associations to be transmitted from one variable to another. Any such association must be accounted for some other way.</p>
<div class="rmdnote">
<p>Consider a simple regression model:</p>
<div class="inline-figure"><img src="graphics/simple_regression_params.png" width="337" style="display: block; margin: auto;"></div>
<p>In the <a href="simple.html#simple">simple regression chapter</a>, we explained that the error term <span class="math inline">\(E\)</span> is uncorrelated with the exogenous variable <span class="math inline">\(X\)</span> because there is no arrow connecting <span class="math inline">\(X\)</span> and <span class="math inline">\(E\)</span>. We can now admit that, while the fact about lack of correlation is true, the explanation we gave was a little misleading. Correlations can be created indirectly through sequences of paths. <span class="math inline">\(X\)</span> and and <span class="math inline">\(E\)</span> are connected through Y as follows:</p>
<p><span class="math display">\[
X \boldsymbol{\rightarrow} Y \boldsymbol{\leftarrow} E
\]</span></p>
<p>But why does this path not imply a correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(E\)</span>?</p>
</div>
<div class="rmdnote">
<p>Consider the multiple regression model:</p>
<div class="inline-figure"><img src="graphics/multiple_regression_2.png" width="438" style="display: block; margin: auto;"></div>
<p>How do we know that the only covariance between <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> is captured by <span class="math inline">\(c_{12}\)</span>? In other words, why is no additional covariance explained by the following path?</p>
<p><span class="math display">\[
X_{1} \boldsymbol{\rightarrow} Y \boldsymbol{\leftarrow} X_{2}
\]</span></p>
</div>
<div class="rmdnote">
<p>Consider the mediator example again:</p>
<div class="inline-figure"><img src="graphics/mediator_vars.png" width="440" style="display: block; margin: auto;"></div>
<p>One of the Really Important Rules was that the error terms should not (at least not by default) be correlated.</p>
<p>It’s true that we haven’t specified a double-headed arrow between <span class="math inline">\(E_{1}\)</span> and <span class="math inline">\(E_{2}\)</span>, but how do we know there isn’t an indirect path accounting for some covariance between them?</p>
<p>Hint: the only possible path would be</p>
<p><span class="math display">\[
E_{1} \rightarrow Y_{1} \boldsymbol{\rightarrow} Y_{2} \leftarrow E_{2}
\]</span>
Why is that path not a problem?</p>
<p>Why do we have to be more careful about making assumptions about possible covariances between <span class="math inline">\(E_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span>?</p>
</div>
</div>
<div id="mediation-simple" class="section level2" number="6.7">
<h2>
<span class="header-section-number">6.7</span> The simple mediation model<a class="anchor" aria-label="anchor" href="#mediation-simple"><i class="fas fa-link"></i></a>
</h2>
<p>We learned above that an indirect path through a mediator
<span class="math display">\[
X_{1} \rightarrow Y_{1} \boldsymbol{\rightarrow} Y_{2}
\]</span>
implies a mathematical relationship among the correlations:
<span class="math display">\[
Corr(Y_{1}, X_{1}) Corr(Y_{2}, Y_{1}) = Corr(Y_{2}, X_{1})
\]</span></p>
<p>If the correlations among these variables in the data do <em>not</em> satisfy this equation, then there is a problem with the model. There is some “left-over” association that isn’t explain by this pathway.</p>
<p>To accommodate this possibility (which, for real-world data, is almost always the case), we can simply add a direct path between <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> that will “soak up” any remaining association. The following model will be called the “simple mediation” model:</p>
<div class="inline-figure"><img src="graphics/mediation_vars.png" width="536" style="display: block; margin: auto;"></div>
<p>Another way to look at this model—maybe one that is more in line with typical scientific hypotheses—is to focus on the relationship between two variables, <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span>. A simple regression will produce an estimate for the slope <span class="math inline">\(b\)</span>. But is that path coefficient meaningful?</p>
<p>Yes, it represents the “total effect” of <span class="math inline">\(X_{1}\)</span> on <span class="math inline">\(Y_{2}\)</span>. (If we’re being careful about causal language, however, we might simply say that all covariance between <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> is accounted for by <span class="math inline">\(b\)</span>.)</p>
<p>But does that one path coefficient tell the whole story? Maybe not. There could be other variables that account for some of that covariance. Controlling for those variables will tell a richer story about the sources of covariation in our response variable.</p>
<p>To reprise the example from earlier, a scientist may want to know if smoking is associated with lung cancer. (Actually, that scientist probably wants to know if smoking <em>causes</em> lung cancer, but let’s set aside causal questions for now.) A study shows a strong association. But by what mechanism is that association created? What aspect of smoking is associated with lung cancer?</p>
<p>Someone posits that smoking leaves tar deposits in the lungs. So more data is collected and analyzed. The model above can now tell us how much of the association between smoking and lung cancer might be accounted for through an indirect pathway that passes through <span class="math inline">\(Y_{1}\)</span>, tar deposits in the lungs.</p>
<p>That’s not the end of the story, either, but to keep things simple, we’ll work with this simple mediation model with only three variables.</p>
<p>Here comes the math.</p>
<p>The regression equations are</p>
<p><span class="math display">\[\begin{align}
Y_{1}   &amp;= b_{11}X_{1} + E_{1}              \\
Y_{2}   &amp;= b_{21}X_{1} + a_{21}Y_{1} + E_{2}  
\end{align}\]</span></p>
<p>The sample correlation matrix is the same as for any three variables:</p>
<p><span class="math display">\[
\begin{bmatrix}
1                   &amp;   \bullet             &amp;   \bullet \\
Corr(Y_{1}, X_{1})  &amp;   1                   &amp;   \bullet \\
Corr(Y_{2}, X_{1})  &amp;   Corr(Y_{2}, Y_{1})   &amp;   1
\end{bmatrix}
\]</span></p>
<p>But the model-implied matrix is involved enough that it doesn’t even fit on the screen (nor are we making you compute it by hand). Here are the six equations separately:</p>
<p><span class="math display">\[\begin{align}
1                   &amp;= v_{1}                                \\
Corr(Y_{1}, X_{1})  &amp;= b_{11}v_{1}                          \\
1                   &amp;= b_{11}^{2}v_{1} + e_{1}              \\
Corr(Y_{2}, X_{1})  &amp;= b_{21}v_{1} + a_{21}b_{11}v_{1}      \\
Corr(Y_{2}, Y_{1})  &amp;= b_{11}b_{21}v_{1} +
                        a_{21}b_{11}^{2}v_{1} +
                        a_{21}e_{1}                         \\
1                   &amp;= b_{21}^2v_{1} +
                        a_{21}^{2}b_{11}^{2}v_{1} +
                        a_{21}^{2}e_{1} +
                        2a_{21}b_{11}b_{21}v_{1} +
                        e_{2}
\end{align}\]</span></p>
<p>Skipping some algebra, we get the following (standardized) solution:</p>
<p><span class="math display">\[\begin{align}
v_{1}               &amp;= 1                                \\
b_{11}              &amp;= Corr(Y_{1}, X_{1})               \\
e_{1}               &amp;= 1 - Corr(Y_{1}, X_{1})^{2}       \\
e_{2}               &amp;= 1 - \left(b_{21}^2 +
                        a_{21}^{2}b_{11}^{2} +
                        a_{21}^{2}e_{1} +
                        2a_{21}b_{11}b_{21}\right)      \\
a_{21}  &amp;= \frac{Corr(Y_{2}, Y_{1}) -
                    Corr(Y_{2}, X_{1})Corr(Y_{1}, X_{1})}
                {1 - Corr(Y_{1}, X_{1})^{2}}                \\
b_{21}  &amp;= \frac{Corr(Y_{2}, X_{1}) -
                    Corr(Y_{2}, Y_{1})Corr(Y_{1}, X_{1})}
                {1 - Corr(Y_{1}, X_{1})^{2}}
\end{align}\]</span></p>
<p>A few observations.</p>
<div class="rmdnote">
<p>The expressions for <span class="math inline">\(v_{1}\)</span>, <span class="math inline">\(b_{11}\)</span>, and <span class="math inline">\(e_{1}\)</span> are totally expected. Explain why.</p>
</div>
<p>The expression for <span class="math inline">\(e_{2}\)</span> is the only one not expressed in terms of all correlations. But substituting in the values of <span class="math inline">\(a_{21}\)</span>, <span class="math inline">\(b_{11}\)</span>, and <span class="math inline">\(b_{21}\)</span> would not be instructive in the slightest.</p>
<p>The expressions for <span class="math inline">\(a_{21}\)</span> and <span class="math inline">\(b_{21}\)</span> have some intuitive content.</p>
<div class="rmdnote">
<p>Review the content from last chapter called <a href="multiple.html#multiple-standardized">Regression with standardized variables</a>, in particular the formulas given for <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span>.</p>
<p>Even though we had to make the notation a little more complicated, do you see any similarities between those formulas and the ones shown above for <span class="math inline">\(a_{21}\)</span> and <span class="math inline">\(b_{21}\)</span>?</p>
<p>Why might that be? Compare the diagrams for the simple mediation model in this chapter and the multiple regression model from the last chapter. What are the similarities and differences?</p>
</div>
<p>Hopefully you could see past the notation to realize that the formulas for <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> in a multiple regression model are <em>identical</em> to the formulas for <span class="math inline">\(a_{21}\)</span> and <span class="math inline">\(b_{21}\)</span> in our simple mediation model.</p>
<p>This is useful because it helps us interpret these path coefficients. As they were for multiple regression, they are simply that part of the correlation due to a direct path, controlling for the correlation along the indirect path.</p>
<div class="rmdnote">
<p>If the main path of interest to our scientific hypothesis is
<span class="math display">\[
X_{1} \boldsymbol{\rightarrow} Y_{2}
\]</span>
the path coefficient <span class="math inline">\(b_{21}\)</span> is the estimate of interest. What other indirect path is being “controlled for” in that estimate?</p>
</div>
<p>For a mediation model, the activity above tells us the right way to think about the path coefficient <span class="math inline">\(b_{21}\)</span>. But it’s also instructive to consider <span class="math inline">\(a_{21}\)</span>.</p>
<div class="rmdnote">
<p>Suppose the substantive path of scientific interest was
<span class="math display">\[
Y_{1} \boldsymbol{\rightarrow} Y_{2}
\]</span></p>
<p>In that case, how do we interpret its path coefficient <span class="math inline">\(a_{21}\)</span>? It is the strength of the association between <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> controlling for the indirect path where?</p>
<p>What kind of indirect path is that? In other words, what role does <span class="math inline">\(X_{1}\)</span> play along the indirect path from <span class="math inline">\(Y_{1}\)</span> to <span class="math inline">\(Y_{2}\)</span>? (Go back and look at the diagram and the arrows.)</p>
</div>
<div class="rmdnote">
<p>Suppose the substantive path of scientific interest was
<span class="math display">\[
X_{1} \boldsymbol{\rightarrow} Y_{1}
\]</span></p>
<p>In that case, how do we interpret its path coefficient <span class="math inline">\(b_{11}\)</span>? It is the strength of the association between <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span>, but does it account for any indirect paths?</p>
<p>The answer is “no”, but why not? What role does <span class="math inline">\(Y_{2}\)</span> play along an indirect path from <span class="math inline">\(X_{1}\)</span> to <span class="math inline">\(Y_{1}\)</span> and why does that not introduce any additional association between them?</p>
</div>
<p>The three activities above illustrate the remarkable fact that the simple mediation model is actually an example of all three models we’ve studied in this chapter: there’s a mediator, a confounder, and a collider! (Each variable plays one of those roles with respect to the relationship between the other two.) So while we call it the “simple mediation model”, we can actually use the implications of the model for any of the three.</p>
</div>
<div id="mediation-r" class="section level2" number="6.8">
<h2>
<span class="header-section-number">6.8</span> Simple mediation in R<a class="anchor" aria-label="anchor" href="#mediation-r"><i class="fas fa-link"></i></a>
</h2>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="multiple.html"><span class="header-section-number">5</span> Multiple regression</a></div>
<div class="next"><a href="path.html"><span class="header-section-number">7</span> Path analysis</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#mediation"><span class="header-section-number">6</span> Mediation</a></li>
<li><a class="nav-link" href="#preliminaries-2">Preliminaries</a></li>
<li><a class="nav-link" href="#mediation-arrows"><span class="header-section-number">6.1</span> Arrows going everywhere!</a></li>
<li><a class="nav-link" href="#mediation-exogenous-endogenous"><span class="header-section-number">6.2</span> Exogenous and endogenous variables</a></li>
<li><a class="nav-link" href="#mediation-naming"><span class="header-section-number">6.3</span> Naming conventions</a></li>
<li><a class="nav-link" href="#mediation-mediators"><span class="header-section-number">6.4</span> Mediators</a></li>
<li><a class="nav-link" href="#mediation-confounders"><span class="header-section-number">6.5</span> Confounders</a></li>
<li><a class="nav-link" href="#mediation-colliders"><span class="header-section-number">6.6</span> Colliders</a></li>
<li><a class="nav-link" href="#mediation-simple"><span class="header-section-number">6.7</span> The simple mediation model</a></li>
<li><a class="nav-link" href="#mediation-r"><span class="header-section-number">6.8</span> Simple mediation in R</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/VectorPosse/sem_book/blob/main/06-Mediation.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/VectorPosse/sem_book/edit/main/06-Mediation.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Demystifying Structural Equation Modeling</strong>" was written by Jonathan Amburgey and Sean Raleigh, Westminster College (Salt Lake City, UT). It was last built on 2022-05-31.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
