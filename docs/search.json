[{"path":"index.html","id":"intro","chapter":"Introduction","heading":"Introduction","text":"Welcome book structural equation modeling!want, can also download book PDF EPUB file.","code":""},{"path":"index.html","id":"intro-history","chapter":"Introduction","heading":"Some history","text":"2016, Jonathan Sean embarked upon bold experiment, asking question, “possible teach structural equation modeling (SEM) undergraduates little statistical background?” make things even exciting, attempted special topics course lasting one month May Term Westminster College (Salt Lake City, UT).endeavor, temper expectations, course. goal produce competent practitioners subsequently go serious research using SEM techniques. quite happy , end May, undergraduates able put together simple final project required find data, posit model, fit model R, interpret output, check model fit statistics. exposure topic appreciation power satisfying enough. fact, think got little : reasonably confident students developed—point right taking course—ability read research article SEM model least idea article talking . called win!repeated experiment modifications materials pedagogy 2018. point, clear finding textbooks articles assign students challenging. great books , mostly aimed graduate students. Even ones labeled “introductory” often far typical undergraduate limited statistical training.decided write textbook fill hole literature. book follows fruit efforts.Sean granted sabbatical Spring 2020 proposed use time start writing book preparation running May Term course May, 2020. , well, know went…pandemic subsided enough us offer course person , attempted May, 2022. [CONTINUED]","code":""},{"path":"index.html","id":"intro-philosophy","chapter":"Introduction","heading":"Our philosophy","text":"mentioned , motivation writing book driven difficulty finding readings students. Perhaps begs question, one even try teaching topic “difficult” “advanced” structural equation modeling audience ? sure, traditional approaches already market seem assume lot background disposal. books claim assume less background…well, sometimes require let .prerequisite class intro stats class covers pretty standard material course: hypothesis testing confidence intervals one two proportions, one two means (paired means), ANOVA, chi-squared, simple linear regression. also benefit intro course introduces students R. (lacking R background, first four modules [FIX LINK DATA 220 BOOK ALSO FULLY ONLINE] suffice basic introduction R R Markdown sufficient success course.)respect students, made deliberate choices way book structured.Make book free open source.Students enough trouble lives shouldn’t exposed extortionate practices textbook publishers. book freely available online, ’s also published permissive open source license (MIT license) allows folks “use, copy, modify, merge, publish, distribute, sublicense, /sell” versions book desired. Furthermore, derivative book must also abide open standards. book libre gratis (, common parlance, “free speech” “free beer”).Start scratch.Explain everything beginning terms simple possible. first chapters may look like review students. Even , course, review gives students confidence tackle upcoming new material. might surprised novel ways explain seemingly familiar concepts. exposition eye toward direct application later chapters, might seem little idiosyncratic first motivated desire smooth pathways later concepts.Incorporate active learning everything.chapters structured work templates classroom experiences. intersperse conceptual explanation activities designed reinforce concepts lead students important conclusions. learning activities appear framed blue boxes [CHANGE ESTABLISH CUSTOM CALLOUT] like :Hey, kids! Stop activity !math well.One common thread see lot SEM books tendency sweep math rug. intention comes good place; mathematics can appear intimidating , therefore, may seem serve deterrent learning. sure, complex mathematical ideas SEM inaccessible audience. time—, fairness, may due Sean’s bias mathematician—truly believe mathematics, carefully explained, can illuminate student understanding. mathy sections may need additional instructor support students without strong math background. takes relatively straightforward algebra nail concepts books ignore. good example investing time rules manipulating variances covariances. allows students calculate “model-implied matrix” cryptically referenced textbooks. However, skip math sometimes. example, lot math behind model fit indices left unexplained. least, hope transparent choices include exclude certain mathematical details.Use “nice” data.Finding data hard, rely lot data sets textbooks R package authors make available (due attribution, course). keep things simple course, work almost exclusively numerical (quantitative) data. [MODIFY END WORKING BINARY CATEGORICAL EXOGENOUS VARIABLES (CODED 0/1) POINT.]careful diagrams.Learning complex models induces sizable cognitive load. Shortcuts diagrams tend confuse students. example, error terms truly latent variables, drawn circles ellipses hidden, even advanced practitioner “knows” ’re . Variances covariances among exogenous variables always appear well. take time build consistent pictographic representation every part model. (chapter introduced archetypal diagram illustrates chapter’s content.) stick representation throughout book.careful notation.may industry standard, LISREL notation needlessly complex undergraduate students. take consistent simple approach notation represents variables using UPPERCASE names path values using lowercase names. Abstract variables tend called something like X exogenous Y endogenous. Real-world variables contextually meaningful names. interested reading research literature, included appendix describing LISREL notation.","code":""},{"path":"index.html","id":"intro-structure","chapter":"Introduction","heading":"Course structure","text":"use book teach 2-credit-hour course. (Even though ’s special topics course May Term, number contact hours students equivalent semester-long, 2-credit-hour course.)[ADD INFO DECIDE MUCH REASONABLE COVER. WANT BOOK USABLE 4-CREDIT-HOUR COURSE, ADDITIONAL MATERIAL CONSIDER INCLUDING?]","code":""},{"path":"index.html","id":"intro-onward","chapter":"Introduction","heading":"Onward and upward","text":"hope enjoy textbook. Please send us feedback!–Jonathan Amburgey (jamburgey@westminstercollege.edu)–Sean Raleigh (sraleigh@westminstercollege.edu)","code":""},{"path":"variables.html","id":"variables","chapter":"1 Variables and measurement","heading":"1 Variables and measurement","text":"","code":""},{"path":"variables.html","id":"variables-first-section","chapter":"1 Variables and measurement","heading":"1.1 First section","text":"[SOMEWHERE NEED MENTION “CONSTANT” VARIABLES, VARIABLES TAKE ONE VALUE.]","code":""},{"path":"variance.html","id":"variance","chapter":"2 Variance","heading":"2 Variance","text":"","code":""},{"path":"variance.html","id":"variance-mean","chapter":"2 Variance","heading":"2.1 A quick refresher on the mean","text":"us taught calculate mean variable way back elementary school: add numbers divide size group numbers. statistics context, often use “bar” indicate mean variable; words, variable called \\(X\\), mean denoted \\(\\overline{X}\\). Remembering always use \\(n\\) represent sample size, formula \\[\n\\overline{X} = \\frac{\\sum{X}}{n}\n\\](case forgot, Greek letter Sigma \\(\\Sigma\\) stands “sum” means “add values thing follows”.)small data set ’ll use throughout chapter simple example can work “hand”:3, 4, 5, 6, 6, 7, 8, 9Calculate mean set eight numbers.","code":""},{"path":"variance.html","id":"variance-calculating","chapter":"2 Variance","heading":"2.2 Calculating variance","text":"Variance quantity meant capture information spread data .Let’s build step step.first thing note spread don’t care large small numbers absolute sense. care large small relative .Look numbers earlier exercise:3, 4, 5, 6, 6, 7, 8, 9What following numbers instead?1003, 1004, 1005, 1006, 1006, 1007, 1008, 1009Explain reasonable measure “spread” groups numbers.One way measure large small number relative whole set measure distance number mean.Recall mean following numbers 6:3, 4, 5, 6, 6, 7, 8, 9Create new list eight numbers measures distance numbers mean. words, subtract 6 numbers.numbers new list negative, zero, positive. make sense? words, mean number negative, zero, positive?original set numbers called \\(X\\), ’ve just calculated new list \\(\\left(X - \\overline{X}\\right)\\). Let’s start organizing table:numbers second columns “deviations” mean.One way might measure “spread” look average deviation. , deviations represent distances mean, set large spread large deviations set small spread small deviations.Go ahead take average (mean) numbers second column .Uh, oh! calculated zero. Explain always get zero, matter set numbers start .idea “average deviation” seems like work, clearly doesn’t. fix idea?Hopefully, identified negative deviations problem canceled positive deviations. deviations positive, wouldn’t issue .two ways making numbers positive:Taking absolute valuesWe just take absolute value make values positive. statistical procures just ,1 ’re going take slightly different approach…SquaringIf square value, become positive.Taking absolute value conceptually easier, historical mathematical reasons squaring little better.2Square numbers second column table . calculate new list \\(\\left(X - \\overline{X}\\right)^{2}\\)Putting new numbers previous table:Now take average (mean) numbers third column .number got (3.5) almost call variance. ’s one annoying wrinkle.took mean last column numbers, added divided 8 since 8 numbers list. fairly technical mathematical reasons, actually don’t want divide 8. Instead, divide one less number; words, divide 7.3Re-math , divide 7 instead dividing 8.number found variance, written \\(Var(X)\\). full formula \\[\nVar(X) = \\frac{\\sum{\\left(X - \\overline{X}\\right)^{2}}}{n - 1}\n\\]one-liner, formula may look little intimidating, break step step , ’s bad.full calculation table:diagrams, variance variable indicated curved, double-headed arrow, labeled value variance, like :Using tabular approach, calculate variance following set numbers:4, 3, 7, 2, 9, 4, 6Consider following two sets numbers:1, 2, 5, 8, 91, 2, 5, 8, 91, 4, 5, 6, 91, 4, 5, 6, 9Without calculations, sets larger variance?’ve decided, calculate variance sets check answer.","code":""},{"path":"variance.html","id":"variance-r","chapter":"2 Variance","heading":"2.3 Calculating variance in R","text":"’ve done hand times make sure understand formula works, can let R work us:also easier real-world data highly engineered 😉 produce whole numbers:","code":"\nX1 <- c(3, 4, 5, 6, 6, 7, 8, 9)\nvar(X1)## [1] 4\nX2 <- c(4, 3, 7, 2, 9, 4, 6)\nvar(X2)## [1] 6\nX3 <- c(1, 2, 5, 8, 9)\nvar(X3)## [1] 12.5\nX4 <- c(1, 4, 5, 6, 9)\nvar(X4)## [1] 8.5\nPlantGrowth$weight##  [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n## [16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\nvar(PlantGrowth$weight)## [1] 0.49167"},{"path":"variance.html","id":"variance-rules","chapter":"2 Variance","heading":"2.4 Variance rules","text":"course, need able calculate variance various combinations variables. example, \\(X_{1}\\) \\(X_{2}\\) two variables, can create new variable \\(X_{1} + X_{2}\\) adding values two variables. variance \\(X_{1} + X_{2}\\)?answer , let’s establish first rule.Rule 1Suppose \\(C\\) “constant” variable, meaning always value (rather variable contain lots different numbers). ,\\[\nVar\\left(C\\right) = 0\n\\]Rule 1 true? can either reason conceptually, based understand variance supposed measure, can sample calculation. (Make table starting column contains many copies single number work calculation.)Now, back example beginning section finding variance \\(X_{1} + X_{2}\\).Rule 2If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} + X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]’re going get formal definition independence . now, suffices think intuitive definition may already head means two things independent. idea , independent, \\(X_{1}\\) \\(X_{2}\\) nothing . Knowing values one give information values . next chapter [LINK], ’ll say rule.’s important note Rule 2 abstract mathematical rule holds theory. actual data, however, know statistics won’t always match theoretical values. example, even true population mean 42, samples drawn population sample means close 42, likely exactly 42.4Let’s test . two new variables defined using random numbers. first one normally distributed mean 1 standard deviation 2. (don’t remember standard deviation intro stats, talk next section.) second one normally distributed mean 4 standard deviation 3. [SEED INFO GO?] independent definition \\(X_{5}\\) depend way definition \\(X_{6}\\) vice versa.sample sizes (2000) large enough get pretty close theoretically correct results .Use R calculate variance \\(X_{5}\\) \\(X_{6}\\) separately. use R add two numbers just obtained (sum two variances). Finally, use R calculate variance sum two variables.’s example help think intuitively.Suppose someone comes along offers give random amount money, number $0 $100.5 variance measure spread, stands reason variance reflects something uncertain much money transaction. average, expect $50, know actual amount money receive can vary greatly.Okay, now second person comes along offers deal, random dollar gift $0 $100.6 end transactions, much money ? average, maybe $100, uncertainty? total amount result two random gifts, even less sure close $100 might . range possible values now $0 $200.7 uncertainty greater overall.course, explains variance sum two variables larger variance either variable individually. fact variance sum two independent variables exactly sum variances shown mathematically. hopefully, intuition clear.next rule consequence first two rules, give special number\\[\nVar\\left(X + C\\right) = Var\\left(X\\right)\n\\]Can apply Rule 2 followed Rule 1 see mathematically \\(Var\\left(X + C\\right) = Var\\left(X\\right)\\)?intuition behind statement \\(Var\\left(X + C\\right) = Var\\left(X\\right)\\)? words, can explain rule someone terms means shifting values data set constant amount?Rule 3 similar Rule 2, ’s quite counter-intuitive:Rule 3If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} - X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]common students think minus sign left translate minus sign right.8What gives?Let’s return example strangers giving money.9 first person still offers random amount $0 $100. , now, second person robber, forces give random dollar value $0 $100 (choosing, course). much money expect two events? average, $0. (first person gives , average, $50, second person takes away, average, $50.) certain amount?Imagine world wrong rule prevailed. \\(Var\\left(X_{1} - X_{2}\\right)\\) truly difference two variances. \\(Var\\left(X_{1}\\right)\\) \\(Var\\left(X_{2}\\right)\\) scenario. (Although one person giving money one taking, uncertainty dollar amount cases.) implies\n\\[\nVar\\left(X_{1}\\right) - Var\\left(X_{2}\\right) = 0\n\\]\nCan true? Zero variance means “spread” means exact certainty value. (Remember Rule 1?) 100% confident end transactions exactly $0? way!fact, amount money end ranges -$100 $100. larger range either transaction individually. uncertainty grown two random processes play, just like scenario two beneficent strangers. fact, width range possibilities scenarios: $0 $200 -$100 $100 span range $200.next rule, unfortunately, great intuitive explanation. make little sense next chapter [LINK], ’ll revisit .Rule 4If \\(\\) number,\\[\nVar\\left(aX\\right) = ^2 Var\\left(X\\right)\n\\]go back table, imagine multiplying every number first column \\(\\). Every number second column still factor \\(\\). square values, every number third column factor \\(^{2}\\). ’s gist rule anyway. , , ’s much intuition makes sense.can, least, check empirically rule works.’ll use \\(X_{5}\\) defined , normally distributed variable mean 1 standard deviation 2. variance data 4:Let’s use \\(= 3\\).R, calculate \\(Var\\left(3X_{5}\\right)\\). (Don’t forget R, can’t just type 3 X5. explicitly include multiplication sign: 3 * X5.)Now try calculating \\(3 Var\\left(X_{5}\\right)\\). ’ll see don’t get right answer.now try \\(9 Var\\left(X_{5}\\right)\\). work.’s variance rules ’ll need!","code":"\nset.seed(10101)\nX5 <- rnorm(2000, mean = 1, sd = 2)\nX6 <- rnorm(2000, mean = 4, sd = 3)\nhead(X5)## [1] -0.7535339 -0.4927789  3.7518296  1.4751639  1.2172549  3.4054426\nhead(X6)## [1] 2.297279 4.856377 6.661822 1.309892 2.270882 3.827944\nvar(X5)## [1] 4.15763"},{"path":"variance.html","id":"variance-sd","chapter":"2 Variance","heading":"2.5 Standard deviation","text":"variance nice obeys rules. one big downside ’s interpretable.example, think scenario people giving/taking money. case, values measures units dollars.\\(X\\) measured dollars, units measurement \\(\\overline{X}\\)? seems sensible, right?units \\(\\left(X - \\overline{X}\\right)\\)? Still sensible, right? (’s problem values positive negative. Negative dollars still make sense. Just think student loans.)Okay, now ’s things get weird. units \\(\\left(X - \\overline{X}\\right)^{2}\\)? longer makes sense.Variance nearly average bunch squared deviations, variable measured dollars, units variance “squared dollars”, whatever .Variances really interpretable directly. make interpretable? Well, variance “squared” units, can take square root get back natural units started .called standard deviation, \\(SD(X)\\).\\[\nSD(X) = \\sqrt{\\frac{\\sum{\\left(X - \\overline{X}\\right)^{2}}}\n{n - 1}}\n\\], said simply,\\[\nSD(X) = \\sqrt{Var(X)}\n\\]\nEquivalently,\\[\nVar(X) = SD(X)^2\n\\]Due interpretability, intro stats class focus far standard deviation variance. downside mathematical rules aren’t nice standard deviations. example, \n\\[\nSD\\left(X_{1} + X_{2}\\right)?\n\\]can work definition see \\[\nSD\\left(X_{1} + X_{2}\\right) = \\sqrt{\nSD\\left(X_{1}\\right)^{2} + SD\\left(X_{2}\\right)^{2}\n}\n\\]\n, eww, ’s gross.SEM, focus almost exclusively variance switch standard deviation two reasons:need communicate something spread meaningful units.need standardize variables. (See Section 2.7 .)","code":""},{"path":"variance.html","id":"variance-mean-centering","chapter":"2 Variance","heading":"2.6 Mean centering data","text":"Many statistical techniques taught intro stats course focus learning means variables. Structural equation modeling little different focused explaining variability data—changes one variables predict changes variables.10A habit ’ll start forming now mean center variables. subtracting mean variable values.Let’s use \\(X_{6}\\) defined , normally distributed variable mean 4 standard deviation 3. interpret values \\(X_{6} - \\overline{X_{6}}\\)? (Remember, just second column variance tables earlier.)shift \\(X_{6}\\) values left \\(\\overline{X_{6}}\\) units, mean new list numbers?Let’s verify R. ’ll use “suffix” mc indicate mean-centered variable.answer exactly agree “theoretical” answer came lines ? (don’t already know, e-16 expression scientific notation means “times \\(10^{-16}\\). ’s really small number!)Take guess variance X6_mc. Verify guess R.good news mean centering preserves variance. mean shifted 0, variance change, statistical model build analyzes variance affecting mean-centering.","code":"\nX6_mc <- X6 - mean(X6)\nmean(X6_mc)## [1] 2.851573e-16"},{"path":"variance.html","id":"variance-standardizing","chapter":"2 Variance","heading":"2.7 Standardizing data","text":"’ve mean centered data, can go one step divide standard deviation. results something often called z-score. process converting variables original units z-scores called standardizing data.\\[\nZ = \\frac{\\left(X - \\overline{X}\\right)}{SD(X)}\n\\]useful? One reason remove units measurement facilitate comparisons variables. Suppose \\(X\\) represents height inches. numerator (\\(X - \\overline{X}\\)) units inches. standard deviation \\(SD(X)\\) also units inches. divide, units go away z-score left without units, sometimes called “dimensionless quantity”.Suppose female United States 6 feet tall (72 inches). Suppose female China 5’8 tall (68 inches). absolute terms, American woman taller Chinese woman. ’re interested knowing woman taller relative respective population?mean height American woman 65” standard deviation 3.5” mean height Chinese woman 62” standard deviation 2.5”. (numbers aren’t perfectly correct, ’re probably close-ish.)Calculate z-scores women.woman taller relative population?Although z-scores don’t technically units, can think measuring many standard deviations value lies mean.z-score value equals mean?meaning negative z-score?z-score American woman 2. means height measures two standard deviations mean.real-world data, use technology . temperature measurements New York 1974. (daily highs across six-month period.)calculate mean standard deviation:average high 78 degrees Fahrenheit standard deviation 9.5 degrees Fahrenheit.just subtract mean, get mean-centered data.also divide standard deviation, get standardized variable (set z-scores). Note extra parentheses make sure get order operations right. subtract first, divide whole mean-centered quantity standard deviation.easier way R use scale command. (Sorry, output little long. Keep scrolling .)Although outputs formatted little differently, can go back check sets numbers match .mean standardized variable? know ?Let’s calculate variance standardized variable. , ’ll note mean \\(\\overline{X}\\) just number. Also, standard deviation \\(SD(X)\\) just number. make calculation easier understand, let’s just substitute letters easier work :\\(M = \\overline{X}\\)\\(S = SD(X)\\).Remember, \\(M\\) \\(S\\) constants.Now need calculate \\(Var(Z)\\). ’ll first couple steps. take , using variance rules earlier chapter, simplify expression get numerical answer. sure justify step citing rule invoked get .\\[\\begin{align}\nVar(Z) &= Var\\left(\\frac{\\left(X - \\overline{X}\\right)}{SD(X)}\\right) \\\\\n    &= Var\\left(\\frac{\\left(X - M\\right)}{S}\\right) \\\\\n    &= Var\\left(\\frac{1}{S}\\left(X - M\\right)\\right) \\\\\n    &= \\quad ???\n\\end{align}\\]get answer 1. standardized variable always variance 1. important fact future chapters.","code":"\nairquality$Temp##   [1] 67 72 74 62 56 66 65 59 61 69 74 69 66 68 58 64 66 57 68 62 59 73 61 61 57\n##  [26] 58 57 67 81 79 76 78 74 67 84 85 79 82 87 90 87 93 92 82 80 79 77 72 65 73\n##  [51] 76 77 76 76 76 75 78 73 80 77 83 84 85 81 84 83 83 88 92 92 89 82 73 81 91\n##  [76] 80 81 82 84 87 85 74 81 82 86 85 82 86 88 86 83 81 81 81 82 86 85 87 89 90\n## [101] 90 92 86 86 82 80 79 77 79 76 78 78 77 72 75 79 81 86 88 97 94 96 94 91 92\n## [126] 93 93 87 84 80 78 75 73 81 76 77 71 71 78 67 76 68 82 64 71 81 69 63 70 77\n## [151] 75 76 68\nmean(airquality$Temp)## [1] 77.88235\nsd(airquality$Temp)## [1] 9.46527\nairquality$Temp - mean(airquality$Temp)##   [1] -10.8823529  -5.8823529  -3.8823529 -15.8823529 -21.8823529 -11.8823529\n##   [7] -12.8823529 -18.8823529 -16.8823529  -8.8823529  -3.8823529  -8.8823529\n##  [13] -11.8823529  -9.8823529 -19.8823529 -13.8823529 -11.8823529 -20.8823529\n##  [19]  -9.8823529 -15.8823529 -18.8823529  -4.8823529 -16.8823529 -16.8823529\n##  [25] -20.8823529 -19.8823529 -20.8823529 -10.8823529   3.1176471   1.1176471\n##  [31]  -1.8823529   0.1176471  -3.8823529 -10.8823529   6.1176471   7.1176471\n##  [37]   1.1176471   4.1176471   9.1176471  12.1176471   9.1176471  15.1176471\n##  [43]  14.1176471   4.1176471   2.1176471   1.1176471  -0.8823529  -5.8823529\n##  [49] -12.8823529  -4.8823529  -1.8823529  -0.8823529  -1.8823529  -1.8823529\n##  [55]  -1.8823529  -2.8823529   0.1176471  -4.8823529   2.1176471  -0.8823529\n##  [61]   5.1176471   6.1176471   7.1176471   3.1176471   6.1176471   5.1176471\n##  [67]   5.1176471  10.1176471  14.1176471  14.1176471  11.1176471   4.1176471\n##  [73]  -4.8823529   3.1176471  13.1176471   2.1176471   3.1176471   4.1176471\n##  [79]   6.1176471   9.1176471   7.1176471  -3.8823529   3.1176471   4.1176471\n##  [85]   8.1176471   7.1176471   4.1176471   8.1176471  10.1176471   8.1176471\n##  [91]   5.1176471   3.1176471   3.1176471   3.1176471   4.1176471   8.1176471\n##  [97]   7.1176471   9.1176471  11.1176471  12.1176471  12.1176471  14.1176471\n## [103]   8.1176471   8.1176471   4.1176471   2.1176471   1.1176471  -0.8823529\n## [109]   1.1176471  -1.8823529   0.1176471   0.1176471  -0.8823529  -5.8823529\n## [115]  -2.8823529   1.1176471   3.1176471   8.1176471  10.1176471  19.1176471\n## [121]  16.1176471  18.1176471  16.1176471  13.1176471  14.1176471  15.1176471\n## [127]  15.1176471   9.1176471   6.1176471   2.1176471   0.1176471  -2.8823529\n## [133]  -4.8823529   3.1176471  -1.8823529  -0.8823529  -6.8823529  -6.8823529\n## [139]   0.1176471 -10.8823529  -1.8823529  -9.8823529   4.1176471 -13.8823529\n## [145]  -6.8823529   3.1176471  -8.8823529 -14.8823529  -7.8823529  -0.8823529\n## [151]  -2.8823529  -1.8823529  -9.8823529\n(airquality$Temp - mean(airquality$Temp))/sd(airquality$Temp)##   [1] -1.14971398 -0.62146702 -0.41016823 -1.67796094 -2.31185730 -1.25536337\n##   [7] -1.36101276 -1.99490912 -1.78361034 -0.93841519 -0.41016823 -0.93841519\n##  [13] -1.25536337 -1.04406459 -2.10055851 -1.46666216 -1.25536337 -2.20620791\n##  [19] -1.04406459 -1.67796094 -1.99490912 -0.51581762 -1.78361034 -1.78361034\n##  [25] -2.20620791 -2.10055851 -2.20620791 -1.14971398  0.32937752  0.11807873\n##  [31] -0.19886945  0.01242934 -0.41016823 -1.14971398  0.64632570  0.75197509\n##  [37]  0.11807873  0.43502691  0.96327387  1.28022205  0.96327387  1.59717023\n##  [43]  1.49152084  0.43502691  0.22372813  0.11807873 -0.09322005 -0.62146702\n##  [49] -1.36101276 -0.51581762 -0.19886945 -0.09322005 -0.19886945 -0.19886945\n##  [55] -0.19886945 -0.30451884  0.01242934 -0.51581762  0.22372813 -0.09322005\n##  [61]  0.54067630  0.64632570  0.75197509  0.32937752  0.64632570  0.54067630\n##  [67]  0.54067630  1.06892327  1.49152084  1.49152084  1.17457266  0.43502691\n##  [73] -0.51581762  0.32937752  1.38587145  0.22372813  0.32937752  0.43502691\n##  [79]  0.64632570  0.96327387  0.75197509 -0.41016823  0.32937752  0.43502691\n##  [85]  0.85762448  0.75197509  0.43502691  0.85762448  1.06892327  0.85762448\n##  [91]  0.54067630  0.32937752  0.32937752  0.32937752  0.43502691  0.85762448\n##  [97]  0.75197509  0.96327387  1.17457266  1.28022205  1.28022205  1.49152084\n## [103]  0.85762448  0.85762448  0.43502691  0.22372813  0.11807873 -0.09322005\n## [109]  0.11807873 -0.19886945  0.01242934  0.01242934 -0.09322005 -0.62146702\n## [115] -0.30451884  0.11807873  0.32937752  0.85762448  1.06892327  2.01976780\n## [121]  1.70281962  1.91411841  1.70281962  1.38587145  1.49152084  1.59717023\n## [127]  1.59717023  0.96327387  0.64632570  0.22372813  0.01242934 -0.30451884\n## [133] -0.51581762  0.32937752 -0.19886945 -0.09322005 -0.72711641 -0.72711641\n## [139]  0.01242934 -1.14971398 -0.19886945 -1.04406459  0.43502691 -1.46666216\n## [145] -0.72711641  0.32937752 -0.93841519 -1.57231155 -0.83276580 -0.09322005\n## [151] -0.30451884 -0.19886945 -1.04406459\nscale(airquality$Temp)##               [,1]\n##   [1,] -1.14971398\n##   [2,] -0.62146702\n##   [3,] -0.41016823\n##   [4,] -1.67796094\n##   [5,] -2.31185730\n##   [6,] -1.25536337\n##   [7,] -1.36101276\n##   [8,] -1.99490912\n##   [9,] -1.78361034\n##  [10,] -0.93841519\n##  [11,] -0.41016823\n##  [12,] -0.93841519\n##  [13,] -1.25536337\n##  [14,] -1.04406459\n##  [15,] -2.10055851\n##  [16,] -1.46666216\n##  [17,] -1.25536337\n##  [18,] -2.20620791\n##  [19,] -1.04406459\n##  [20,] -1.67796094\n##  [21,] -1.99490912\n##  [22,] -0.51581762\n##  [23,] -1.78361034\n##  [24,] -1.78361034\n##  [25,] -2.20620791\n##  [26,] -2.10055851\n##  [27,] -2.20620791\n##  [28,] -1.14971398\n##  [29,]  0.32937752\n##  [30,]  0.11807873\n##  [31,] -0.19886945\n##  [32,]  0.01242934\n##  [33,] -0.41016823\n##  [34,] -1.14971398\n##  [35,]  0.64632570\n##  [36,]  0.75197509\n##  [37,]  0.11807873\n##  [38,]  0.43502691\n##  [39,]  0.96327387\n##  [40,]  1.28022205\n##  [41,]  0.96327387\n##  [42,]  1.59717023\n##  [43,]  1.49152084\n##  [44,]  0.43502691\n##  [45,]  0.22372813\n##  [46,]  0.11807873\n##  [47,] -0.09322005\n##  [48,] -0.62146702\n##  [49,] -1.36101276\n##  [50,] -0.51581762\n##  [51,] -0.19886945\n##  [52,] -0.09322005\n##  [53,] -0.19886945\n##  [54,] -0.19886945\n##  [55,] -0.19886945\n##  [56,] -0.30451884\n##  [57,]  0.01242934\n##  [58,] -0.51581762\n##  [59,]  0.22372813\n##  [60,] -0.09322005\n##  [61,]  0.54067630\n##  [62,]  0.64632570\n##  [63,]  0.75197509\n##  [64,]  0.32937752\n##  [65,]  0.64632570\n##  [66,]  0.54067630\n##  [67,]  0.54067630\n##  [68,]  1.06892327\n##  [69,]  1.49152084\n##  [70,]  1.49152084\n##  [71,]  1.17457266\n##  [72,]  0.43502691\n##  [73,] -0.51581762\n##  [74,]  0.32937752\n##  [75,]  1.38587145\n##  [76,]  0.22372813\n##  [77,]  0.32937752\n##  [78,]  0.43502691\n##  [79,]  0.64632570\n##  [80,]  0.96327387\n##  [81,]  0.75197509\n##  [82,] -0.41016823\n##  [83,]  0.32937752\n##  [84,]  0.43502691\n##  [85,]  0.85762448\n##  [86,]  0.75197509\n##  [87,]  0.43502691\n##  [88,]  0.85762448\n##  [89,]  1.06892327\n##  [90,]  0.85762448\n##  [91,]  0.54067630\n##  [92,]  0.32937752\n##  [93,]  0.32937752\n##  [94,]  0.32937752\n##  [95,]  0.43502691\n##  [96,]  0.85762448\n##  [97,]  0.75197509\n##  [98,]  0.96327387\n##  [99,]  1.17457266\n## [100,]  1.28022205\n## [101,]  1.28022205\n## [102,]  1.49152084\n## [103,]  0.85762448\n## [104,]  0.85762448\n## [105,]  0.43502691\n## [106,]  0.22372813\n## [107,]  0.11807873\n## [108,] -0.09322005\n## [109,]  0.11807873\n## [110,] -0.19886945\n## [111,]  0.01242934\n## [112,]  0.01242934\n## [113,] -0.09322005\n## [114,] -0.62146702\n## [115,] -0.30451884\n## [116,]  0.11807873\n## [117,]  0.32937752\n## [118,]  0.85762448\n## [119,]  1.06892327\n## [120,]  2.01976780\n## [121,]  1.70281962\n## [122,]  1.91411841\n## [123,]  1.70281962\n## [124,]  1.38587145\n## [125,]  1.49152084\n## [126,]  1.59717023\n## [127,]  1.59717023\n## [128,]  0.96327387\n## [129,]  0.64632570\n## [130,]  0.22372813\n## [131,]  0.01242934\n## [132,] -0.30451884\n## [133,] -0.51581762\n## [134,]  0.32937752\n## [135,] -0.19886945\n## [136,] -0.09322005\n## [137,] -0.72711641\n## [138,] -0.72711641\n## [139,]  0.01242934\n## [140,] -1.14971398\n## [141,] -0.19886945\n## [142,] -1.04406459\n## [143,]  0.43502691\n## [144,] -1.46666216\n## [145,] -0.72711641\n## [146,]  0.32937752\n## [147,] -0.93841519\n## [148,] -1.57231155\n## [149,] -0.83276580\n## [150,] -0.09322005\n## [151,] -0.30451884\n## [152,] -0.19886945\n## [153,] -1.04406459\n## attr(,\"scaled:center\")\n## [1] 77.88235\n## attr(,\"scaled:scale\")\n## [1] 9.46527"},{"path":"covariance.html","id":"covariance","chapter":"3 Covariance","heading":"3 Covariance","text":"","code":""},{"path":"covariance.html","id":"covariance-calculating","chapter":"3 Covariance","heading":"3.1 Calculating covariance","text":"last chapter variance, measures spread single variable. Now extend idea pairs variables.say two variables “co-vary” spread one variable related spread another variable. relationship represents association two variables.’ll call two variables \\(X_{1}\\) \\(X_{2}\\). keep things simple, let’s assume already mean centered variables.\\(X_{1}\\) \\(X_{2}\\) already mean centered, \\(\\overline{X_{1}}\\) \\(\\overline{X_{2}}\\)?last chapter variance, ’ll build calculation covariance step--step using table keep track intermediate quantities need.two variables (\\(n = 7\\)) mean centered:Check mean columns truly zero.Something interesting happens look product \\(X_{1}X_{2}\\).\\(X_{1}\\) \\(X_{2}\\) lie means, positive numbers. Therefore, product positive.\\(X_{1}\\) \\(X_{2}\\) lie means? know values individually know product?chart , products listed new column:Now add products across seven data pairs:\\(X_{1}\\) \\(X_{2}\\) tend similar values (positive negative), product usually positive. ’s true every pair values table ; products negative. majority positive. Therefore, sum products positive.’re almost . Just like wanted average squared deviation calculate variance, want average products third column . just like case variance, ’s quite average calculate. Instead dividing \\(n\\), divide \\(n - 1\\) exactly esoteric reason. example, 7 data points (words, 7 rows data), divide 6.Putting together:diagrams, covariance two variables indicated curved, double-headed arrow pointing boxes labeled value covariance, like :Note still include variances individual variables. still important us. just one new type arrow now.Verify variances diagram correct example. can hand want, using R fine .final formula covariance, written \\(Cov\\left(X_{1}, X_{2}\\right)\\). works pairs variables, even aren’t mean centered. terms \\(\\left(X_{1} - \\overline{X_1}\\right)\\) \\(\\left(X_{2} - \\overline{X_2}\\right)\\) mean centering:\\[\nCov\\left(X_{1}, X_{2}\\right) = \\frac{\\sum \\left(X_{1} - \\overline{X_{1}}\\right)\\left(X_{2} - \\overline{X_{2}}\\right)}{n - 1}\n\\]Suppose \\(X_{1}\\) tends mean \\(X_{2}\\) mean \\(X_{1}\\) tends mean \\(X_{2}\\) mean. product \\(\\left(X_{1} - \\overline{X_{1}}\\right)\\left(X_{2} - \\overline{X_{2}}\\right)\\) usually ? Therefore, sum products likely ?general variables (necessarily mean centered), table actually look like :Calculate covariance hand making table like one . (variables mean centered, ’ll calculate mean variable order fill third fourth columns.)\\(X_{3}\\): 8, 10, 16, 7, 4, 3\\(X_{4}\\): 6, 5, 4, 9, 11, 7Explain intuitively covariance negative two variables.calculating variance, order data points matter. ?calculating covariance, order data points matter. ?keep pairs together, rearrange rows table. affect covariance?","code":""},{"path":"covariance.html","id":"covariance-r","chapter":"3 Covariance","heading":"3.2 Calculating covariance in R","text":"’ve done hand times make sure understand formula works, can let R work us:’s real world data:","code":"\nX1 <- c(-1,-2, 2, -3, 4, -1, 1)\nX2 <- c(-2, 2, -2, -1, 2, -2, 3)\ncov(X1, X2)## [1] 1.666667\nX3 <- c(8, 10, 16, 7, 4, 3)\nX4 <- c(6, 5, 4, 9, 11, 7)\ncov(X3, X4)## [1] -9.2\ncov(airquality$Temp, airquality$Wind)## [1] -15.27214"},{"path":"covariance.html","id":"covariance-rules","chapter":"3 Covariance","heading":"3.3 Covariance rules","text":"’ll think variance covariance rules one big list. left Rule 4, now ’ll introduce Rule 5.Rule 5\\[\nCov(X, X) = Var(X)\n\\]words, Rule 5 states covariance variable just thing variance variable. quite remarkable! means variance really just special case covariance.Explain Rule 5 true. (Hint: think calculate \\(Cov(X, X)\\) using either formula table—!)Rule 6\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{2}, X_{1}\\right)\n\\]words, say covariance symmetric.Explain Rule 6 true. (, think formula table—!)next four rules analogous similar rules variance (Rule 1, Rule 2, Rule 3, Rule 4).Rule 7Suppose \\(C\\) “constant” variable, meaning always value (rather variable contain lots different numbers). ,\\[\nCov\\left(X, C\\right) = 0\n\\]always, try explain rule. Give intuitive explanation rule “” true. think computationally, thinking either formula table—!Rule 8\\[\nCov\\left(X_{1} + X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) + Cov\\left(X_{2}, X_{3}\\right)  \n\\]appreciate longer restriction relationships among variables involved. Rule 2 worked two variables independent. hand, Rule 8 works combination variables, matter relation.Even satisfying next rule:Rule 9\\[\nCov\\left(X_{1} - X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) - Cov\\left(X_{2}, X_{3}\\right)  \n\\]Yay! minus sign behaves sensibly now! course, since covariances can positive negative (unlike variances always positive!) can safely subtract two without worry. rule, like Rule 8, depend \\(X_{1}\\) \\(X_{2}\\) independent. can two variables.versions rules addition subtraction side, just minor variations Rule 8 Rule 9, ’re worth mentioning separate rule. Remember covariance symmetric, can always swap things left right comma.\\[\nCov\\left(X_{1}, X_{2} \\pm X_{3}\\right) = Cov\\left(X_{1}, X_{2}\\right) \\pm Cov\\left(X_{1}, X_{3}\\right)  \n\\]Rule 10If \\(\\) number,\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{1}, X_{2}\\right) =  Cov\\left(X_{1}, X_{2}\\right)  \n\\]rule also sensible. Instead Rule 4 takes number \\(\\) pulls \\(^{2}\\), Rule 10 just pulls single factor \\(\\) (either slot).Just couple rules. talking independence conjunction Rule 8 Rule 9. leads directly interesting super-important rule:Rule 11If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nCov\\left(X_{1}, X_{2}\\right) = 0\n\\]Rule 11 true, intuitively?’s interesting note rule works one way. words, know two variables independent, can conclude covariance zero. However, know covariance zero, doesn’t necessarily mean two variables independent. ’ll see example later chapter.Finally, one rule rule :Rule 12For two variables \\(X_{1}\\) \\(X_{2}\\):\\[\nVar(aX_{1} + bX_{2}) =\n    ^2Var(X_{1}) + b^2Var(X_{2}) + 2abCov(X_{1}, X_{2})\n\\]brings practically everything know together one rule!Proving Rule 12 give us good practice type manipulation ’ll need future chapters. goes. first steps, name rule ’re invoking. , ’ll pick thread follow last steps .\\[\\begin{align}\nVar(aX_{1} + bX_{2}) &= Cov(aX_{1} + bX_{2}, aX_{1} + bX_{2}) \\\\\n    &= Cov(aX_{1} + bX_{2}, aX_{1}) + Cov(aX_{1} + bX_{2}, bX_{2}) \\\\\n    &=  Cov(aX_{1}, aX_{1}) +\n        Cov(bX_{2}, aX_{1}) + \\\\\n    &   \\qquad Cov(aX_{1}, bX_{2}) +\n        Cov(bX_{2}, bX_{2}) \\\\\n    &= \\quad ???\n\\end{align}\\]’ll need rules calculations future chapters. Rather search Chapter 2 chapter, ’ve gathered rules one convenient place Appendix .","code":""},{"path":"covariance.html","id":"covariance-correlation","chapter":"3 Covariance","heading":"3.4 Correlation","text":"","code":""},{"path":"covariance.html","id":"covariance-visualizing","chapter":"3 Covariance","heading":"3.5 Visualizing covariance and correlation","text":"","code":""},{"path":"simple.html","id":"simple","chapter":"4 Simple regression","heading":"4 Simple regression","text":"","code":""},{"path":"multiple.html","id":"multiple","chapter":"5 Multiple regression","heading":"5 Multiple regression","text":"","code":""},{"path":"mediation.html","id":"mediation","chapter":"6 Mediation","heading":"6 Mediation","text":"","code":""},{"path":"path-analysis.html","id":"path-analysis","chapter":"7 Path analysis","heading":"7 Path analysis","text":"","code":""},{"path":"latent-variables.html","id":"latent-variables","chapter":"8 Latent variables","heading":"8 Latent variables","text":"","code":""},{"path":"confirmatory-factor-analysis.html","id":"confirmatory-factor-analysis","chapter":"9 Confirmatory factor analysis","heading":"9 Confirmatory factor analysis","text":"","code":""},{"path":"structural-equation-models.html","id":"structural-equation-models","chapter":"10 Structural equation models","heading":"10 Structural equation models","text":"","code":""},{"path":"structural-causal-models.html","id":"structural-causal-models","chapter":"11 Structural causal models","heading":"11 Structural causal models","text":"","code":""},{"path":"appendix-rules.html","id":"appendix-rules","chapter":"A Variance/covariance rules","heading":"A Variance/covariance rules","text":"Rule 1If \\(C\\) constant, \\[\nVar\\left(C\\right) = 0\n\\]Rule 2If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} + X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]Consequence Rule 1 Rule 2:\\[\nVar\\left(X + C\\right) = Var\\left(X\\right)\n\\]Rule 3If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} - X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]Rule 4If \\(\\) number,\\[\nVar\\left(aX\\right) = ^2 Var\\left(X\\right)\n\\]Rule 5\\[\nCov(X, X) = Var(X)\n\\]Rule 6\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{2}, X_{1}\\right)\n\\]Rule 7If \\(C\\) constant, \\[\nCov\\left(X, C\\right) = 0\n\\]Rule 8\\[\nCov\\left(X_{1} + X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) + Cov\\left(X_{2}, X_{3}\\right)  \n\\]Rule 9\\[\nCov\\left(X_{1} - X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) - Cov\\left(X_{2}, X_{3}\\right)  \n\\]Consequence Rule 6, Rule 8, Rule 9:\\[\nCov\\left(X_{1}, X_{2} \\pm X_{3}\\right) = Cov\\left(X_{1}, X_{2}\\right) \\pm Cov\\left(X_{1}, X_{3}\\right)  \n\\]\n- Rule 10If \\(\\) number,\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{1}, X_{2}\\right) =  Cov\\left(X_{1}, X_{2}\\right)  \n\\]Rule 11If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nCov\\left(X_{1}, X_{2}\\right) = 0\n\\]Rule 12For two variables \\(X_{1}\\) \\(X_{2}\\):\\[\nVar(aX_{1} + bX_{2}) =\n    ^2Var(X_{1}) + b^2Var(X_{2}) + 2abCov(X_{1}, X_{2})\n\\]","code":""},{"path":"appendix-lisrel.html","id":"appendix-lisrel","chapter":"B LISREL notation","heading":"B LISREL notation","text":"","code":""}]
