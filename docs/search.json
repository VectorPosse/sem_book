[{"path":"index.html","id":"intro","chapter":"Introduction","heading":"Introduction","text":"Welcome book structural equation modeling![BOOK WORK PROGRESS. FEEL FREE PERUSE WHATEVER CONTENT FIND , FINAL VERSION READY SOMETIME 2023.]want, can also download book PDF EPUB file. aware print versions missing richer formatting online version.","code":""},{"path":"index.html","id":"intro-history","chapter":"Introduction","heading":"Some history","text":"2016, Jonathan Sean embarked upon bold experiment, asking question, “possible teach structural equation modeling (SEM) undergraduates little statistical background?” make things even exciting, attempted special topics course lasting one month May Term Westminster College (Salt Lake City, UT).endeavor, temper expectations, course. goal produce competent practitioners subsequently go serious research using SEM techniques. quite happy , end May, undergraduates able put together simple final project required find data, posit model, fit model R, interpret output, check model fit statistics. exposure topic appreciation power satisfying enough. fact, think got little : reasonably confident students developed—point right taking course—ability read research article SEM model least idea article talking . called win!repeated experiment modifications materials pedagogy 2018. point, clear finding textbooks articles assign students challenging. great books , mostly aimed graduate students. Even ones labeled “introductory” often far typical undergraduate limited statistical training.decided write textbook fill hole literature. book follows fruit efforts.Sean granted sabbatical Spring 2020 proposed use time start writing book preparation running May Term course May, 2020. , well, know went…pandemic subsided enough us offer course person , attempted May, 2022. [CONTINUED]","code":""},{"path":"index.html","id":"intro-philosophy","chapter":"Introduction","heading":"Our philosophy","text":"mentioned , motivation writing book driven difficulty finding readings students. Perhaps begs question, one even try teaching topic “difficult” “advanced” structural equation modeling audience ? sure, traditional approaches already market seem assume lot background disposal. books claim assume less background…well, sometimes require let .prerequisite class intro stats class covers pretty standard material course: hypothesis testing confidence intervals one two proportions, one two means (paired means), ANOVA, chi-squared, simple linear regression. also benefit intro course introduces students R. (lacking R background, first four modules [FIX LINK DATA 220 BOOK ALSO FULLY ONLINE] suffice basic introduction R R Markdown sufficient success course.)respect students, made deliberate choices way book structured.Make book free open source.Students enough trouble lives shouldn’t exposed extortionate practices textbook publishers. book freely available online, ’s also published permissive open source license (MIT license) allows folks “use, copy, modify, merge, publish, distribute, sublicense, /sell” versions book desired. Furthermore, derivative book must also abide open standards. book libre gratis (, common parlance, “free speech” “free beer”).Start scratch.Explain everything beginning terms simple possible. first chapters may look like review students. Even , course, review gives students confidence tackle upcoming new material. might surprised novel ways explain seemingly familiar concepts. exposition eye toward direct application later chapters, might seem little idiosyncratic first motivated desire smooth pathways later concepts.Incorporate active learning everything.chapters structured work templates classroom experiences. intersperse conceptual explanation activities designed reinforce concepts lead students important conclusions. learning activities appear framed blue boxes [CHANGE ESTABLISH CUSTOM CALLOUT] like :Hey, kids! Stop activity !math well.One common thread see lot SEM books tendency sweep math rug. intention comes good place; mathematics can appear intimidating , therefore, may seem serve deterrent learning. sure, complex mathematical ideas SEM inaccessible audience. time—, fairness, may due Sean’s bias mathematician—truly believe mathematics, carefully explained, can illuminate student understanding. mathy sections may need additional instructor support students without strong math background. takes relatively straightforward algebra nail concepts books ignore. good example investing time rules manipulating variances covariances. allows students calculate “model-implied matrix” cryptically referenced textbooks. However, skip math sometimes. example, lot math behind model fit indices left unexplained. least, hope transparent choices include exclude certain mathematical details.Use “nice” data.Finding data hard, rely lot data sets textbooks R package authors make available (due attribution, course). keep things simple course, work almost exclusively numerical (quantitative) data. [MODIFY END WORKING BINARY CATEGORICAL EXOGENOUS VARIABLES (CODED 0/1) POINT.]careful diagrams.Learning complex models induces sizable cognitive load. Shortcuts diagrams tend confuse students. example, error terms truly latent variables, drawn circles ellipses hidden, even advanced practitioner “knows” ’re . Variances covariances among exogenous variables always appear well. take time build consistent pictographic representation every part model. (chapter introduced archetypal diagram illustrates chapter’s content.) stick representation throughout book.careful notation.may industry standard, LISREL notation needlessly complex undergraduate students. take consistent simple approach notation represents variables using UPPERCASE names path values using lowercase names. Abstract variables tend called something like X exogenous Y endogenous. Real-world variables contextually meaningful names. interested reading research literature, included appendix describing LISREL notation.","code":""},{"path":"index.html","id":"intro-structure","chapter":"Introduction","heading":"Course structure","text":"use book teach 2-credit-hour course. (Even though ’s special topics course May Term, number contact hours students equivalent semester-long, 2-credit-hour course.)[ADD INFO DECIDE MUCH REASONABLE COVER. WANT BOOK USABLE 4-CREDIT-HOUR COURSE, ADDITIONAL MATERIAL CONSIDER INCLUDING?]","code":""},{"path":"index.html","id":"intro-onward","chapter":"Introduction","heading":"Onward and upward","text":"hope enjoy textbook. Please send us feedback!–Jonathan Amburgey (jamburgey@westminstercollege.edu)–Sean Raleigh (sraleigh@westminstercollege.edu)","code":""},{"path":"variables.html","id":"variables","chapter":"1 Variables and measurement","heading":"1 Variables and measurement","text":"","code":""},{"path":"variables.html","id":"variables-first-section","chapter":"1 Variables and measurement","heading":"1.1 First section","text":"[SOMEWHERE NEED MENTION “CONSTANT” VARIABLES, VARIABLES TAKE ONE VALUE.]","code":""},{"path":"variance.html","id":"variance","chapter":"2 Variance","heading":"2 Variance","text":"","code":""},{"path":"variance.html","id":"variance-mean","chapter":"2 Variance","heading":"2.1 A quick refresher on the mean","text":"us taught calculate mean variable way back elementary school: add numbers divide size group numbers. statistics context, often use “bar” indicate mean variable; words, variable called \\(X\\), mean denoted \\(\\overline{X}\\). Remembering always use \\(n\\) represent sample size, formula \\[\n\\overline{X} = \\frac{\\sum{X}}{n}\n\\](case forgot, Greek letter Sigma \\(\\Sigma\\) stands “sum” means “add values thing follows”.)small data set ’ll use throughout chapter simple example can work “hand”:3, 4, 5, 6, 6, 7, 8, 9Calculate mean set eight numbers.","code":""},{"path":"variance.html","id":"variance-calculating","chapter":"2 Variance","heading":"2.2 Calculating variance","text":"Variance quantity meant capture information spread data .Let’s build step step.first thing note spread don’t care large small numbers absolute sense. care large small relative .Look numbers earlier exercise:3, 4, 5, 6, 6, 7, 8, 9What following numbers instead?1003, 1004, 1005, 1006, 1006, 1007, 1008, 1009Explain reasonable measure “spread” groups numbers.One way measure large small number relative whole set measure distance number mean.Recall mean following numbers 6:3, 4, 5, 6, 6, 7, 8, 9Create new list eight numbers measures distance numbers mean. words, subtract 6 numbers.numbers new list negative, zero, positive. make sense? words, mean number negative, zero, positive?original set numbers called \\(X\\), ’ve just calculated new list \\(\\left(X - \\overline{X}\\right)\\). Let’s start organizing table:numbers second columns “deviations” mean.One way might measure “spread” look average deviation. , deviations represent distances mean, set large spread large deviations set small spread small deviations.Go ahead take average (mean) numbers second column .Uh, oh! calculated zero. Explain always get zero, matter set numbers start .idea “average deviation” seems like work, clearly doesn’t. fix idea?Hopefully, identified negative deviations problem canceled positive deviations. deviations positive, wouldn’t issue .two ways making numbers positive:Taking absolute valuesWe just take absolute value make values positive. statistical procures just ,1 ’re going take slightly different approach…SquaringIf square value, become positive.Taking absolute value conceptually easier, historical mathematical reasons squaring little better.2Square numbers second column table . calculate new list \\(\\left(X - \\overline{X}\\right)^{2}\\)Putting new numbers previous table:Now take average (mean) numbers third column .number got (3.5) almost call variance. ’s one annoying wrinkle.took mean last column numbers, added divided 8 since 8 numbers list. fairly technical mathematical reasons, actually don’t want divide 8. Instead, divide one less number; words, divide 7.3Re-math , divide 7 instead dividing 8.number found variance, written \\(Var(X)\\). full formula \\[\nVar(X) = \\frac{\\sum{\\left(X - \\overline{X}\\right)^{2}}}{n - 1}\n\\]one-liner, formula may look little intimidating, break step step , ’s bad.full calculation table:diagrams, variance variable indicated curved, double-headed arrow, labeled value variance, like :Using tabular approach, calculate variance following set numbers:4, 3, 7, 2, 9, 4, 6Consider following two sets numbers:1, 2, 5, 8, 91, 2, 5, 8, 91, 4, 5, 6, 91, 4, 5, 6, 9Without calculations, sets larger variance?’ve decided, calculate variance sets check answer.","code":""},{"path":"variance.html","id":"variance-r","chapter":"2 Variance","heading":"2.3 Calculating variance in R","text":"’ve done hand times make sure understand formula works, can let R work us:also easier real-world data highly engineered 😉 produce whole numbers:","code":"\nX1 <- c(3, 4, 5, 6, 6, 7, 8, 9)\nvar(X1)## [1] 4\nX2 <- c(4, 3, 7, 2, 9, 4, 6)\nvar(X2)## [1] 6\nX3 <- c(1, 2, 5, 8, 9)\nvar(X3)## [1] 12.5\nX4 <- c(1, 4, 5, 6, 9)\nvar(X4)## [1] 8.5\nPlantGrowth$weight##  [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n## [16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\nvar(PlantGrowth$weight)## [1] 0.49167"},{"path":"variance.html","id":"variance-rules","chapter":"2 Variance","heading":"2.4 Variance rules","text":"course, need able calculate variance various combinations variables. example, \\(X_{1}\\) \\(X_{2}\\) two variables, can create new variable \\(X_{1} + X_{2}\\) adding values two variables. variance \\(X_{1} + X_{2}\\)?answer , let’s establish first rule.Rule 1Suppose \\(C\\) “constant” variable, meaning always value (rather variable contain lots different numbers). ,\\[\nVar\\left(C\\right) = 0\n\\]Rule 1 true? can either reason conceptually, based understand variance supposed measure, can sample calculation. (Make table starting column contains many copies single number work calculation.)Now, back example beginning section finding variance \\(X_{1} + X_{2}\\).Rule 2If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} + X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]’re going get formal definition independence . now, suffices think intuitive definition may already head means two things independent. idea , independent, \\(X_{1}\\) \\(X_{2}\\) nothing . Knowing values one give information values . next chapter [LINK], ’ll say rule.’s important note Rule 2 abstract mathematical rule holds theory. actual data, however, know statistics won’t always match theoretical values. example, even true population mean 42, samples drawn population sample means close 42, likely exactly 42.4Let’s test . two new variables defined using random numbers. first one normally distributed mean 1 standard deviation 2. (don’t remember standard deviation intro stats, talk next section.) second one normally distributed mean 4 standard deviation 3. [SEED INFO GO?] independent definition \\(X_{5}\\) depend way definition \\(X_{6}\\) vice versa.sample sizes (2000) large enough get pretty close theoretically correct results .Use R calculate variance \\(X_{5}\\) \\(X_{6}\\) separately. use R add two numbers just obtained (sum two variances). Finally, use R calculate variance sum two variables.’s example help think intuitively.Suppose someone comes along offers give random amount money, number $0 $100.5 variance measure spread, stands reason variance reflects something uncertain much money transaction. average, expect $50, know actual amount money receive can vary greatly.Okay, now second person comes along offers deal, random dollar gift $0 $100.6 end transactions, much money ? average, maybe $100, uncertainty? total amount result two random gifts, even less sure close $100 might . range possible values now $0 $200.7 uncertainty greater overall.course, explains variance sum two variables larger variance either variable individually. fact variance sum two independent variables exactly sum variances shown mathematically. hopefully, intuition clear.next rule consequence first two rules, give special number\\[\nVar\\left(X + C\\right) = Var\\left(X\\right)\n\\]Can apply Rule 2 followed Rule 1 see mathematically \\(Var\\left(X + C\\right) = Var\\left(X\\right)\\)?intuition behind statement \\(Var\\left(X + C\\right) = Var\\left(X\\right)\\)? words, can explain rule someone terms means shifting values data set constant amount?Rule 3 similar Rule 2, ’s quite counter-intuitive:Rule 3If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} - X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]common students think minus sign left translate minus sign right.8What gives?Let’s return example strangers giving money.9 first person still offers random amount $0 $100. , now, second person robber, forces give random dollar value $0 $100 (choosing, course). much money expect two events? average, $0. (first person gives , average, $50, second person takes away, average, $50.) certain amount?Imagine world wrong rule prevailed. \\(Var\\left(X_{1} - X_{2}\\right)\\) truly difference two variances. \\(Var\\left(X_{1}\\right)\\) \\(Var\\left(X_{2}\\right)\\) scenario. (Although one person giving money one taking, uncertainty dollar amount cases.) implies\n\\[\nVar\\left(X_{1}\\right) - Var\\left(X_{2}\\right) = 0\n\\]\nCan true? Zero variance means “spread” means exact certainty value. (Remember Rule 1?) 100% confident end transactions exactly $0? way!fact, amount money end ranges -$100 $100. larger range either transaction individually. uncertainty grown two random processes play, just like scenario two beneficent strangers. fact, width range possibilities scenarios: $0 $200 -$100 $100 span range $200.next rule, unfortunately, great intuitive explanation. make little sense next chapter [LINK], ’ll revisit .Rule 4If \\(\\) number,\\[\nVar\\left(aX\\right) = ^2 Var\\left(X\\right)\n\\]go back table, imagine multiplying every number first column \\(\\). Every number second column still factor \\(\\). square values, every number third column factor \\(^{2}\\). ’s gist rule anyway. , , ’s much intuition makes sense.can, least, check empirically rule works.’ll use \\(X_{5}\\) defined , normally distributed variable mean 1 standard deviation 2. variance data 4:Let’s use \\(= 3\\).R, calculate \\(Var\\left(3X_{5}\\right)\\). (Don’t forget R, can’t just type 3 X5. explicitly include multiplication sign: 3 * X5.)Now try calculating \\(3 Var\\left(X_{5}\\right)\\). ’ll see don’t get right answer.now try \\(9 Var\\left(X_{5}\\right)\\). work.’s variance rules ’ll need!","code":"\nset.seed(10101)\nX5 <- rnorm(2000, mean = 1, sd = 2)\nX6 <- rnorm(2000, mean = 4, sd = 3)\nhead(X5)## [1] -0.7535339 -0.4927789  3.7518296  1.4751639  1.2172549  3.4054426\nhead(X6)## [1] 2.297279 4.856377 6.661822 1.309892 2.270882 3.827944\nvar(X5)## [1] 4.15763"},{"path":"variance.html","id":"variance-sd","chapter":"2 Variance","heading":"2.5 Standard deviation","text":"variance nice obeys rules. one big downside ’s interpretable.example, think scenario people giving/taking money. case, values measures units dollars.\\(X\\) measured dollars, units measurement \\(\\overline{X}\\)? seems sensible, right?units \\(\\left(X - \\overline{X}\\right)\\)? Still sensible, right? (’s problem values positive negative. Negative dollars still make sense. Just think student loans.)Okay, now ’s things get weird. units \\(\\left(X - \\overline{X}\\right)^{2}\\)? longer makes sense.Variance nearly average bunch squared deviations, variable measured dollars, units variance “squared dollars”, whatever .Variances really interpretable directly. make interpretable? Well, variance “squared” units, can take square root get back natural units started .called standard deviation, \\(SD(X)\\).\\[\nSD(X) = \\sqrt{\\frac{\\sum{\\left(X - \\overline{X}\\right)^{2}}}\n{n - 1}}\n\\], said simply,\\[\nSD(X) = \\sqrt{Var(X)}\n\\]\nEquivalently,\\[\nVar(X) = SD(X)^2\n\\]Due interpretability, intro stats class focus far standard deviation variance. downside mathematical rules aren’t nice standard deviations. example, \n\\[\nSD\\left(X_{1} + X_{2}\\right)?\n\\]can work definition see \\[\nSD\\left(X_{1} + X_{2}\\right) = \\sqrt{\nSD\\left(X_{1}\\right)^{2} + SD\\left(X_{2}\\right)^{2}\n}\n\\]\n, eww, ’s gross.SEM, focus almost exclusively variance switch standard deviation two reasons:need communicate something spread meaningful units.need standardize variables. (See Section 2.7 .)Although standard deviance just square root variance, worth knowing R command calculate . ’s just sd. example:can see sd right thing:","code":"\nsd(PlantGrowth$weight)## [1] 0.7011918\nsqrt(var(PlantGrowth$weight))## [1] 0.7011918"},{"path":"variance.html","id":"variance-mean-centering","chapter":"2 Variance","heading":"2.6 Mean centering data","text":"Many statistical techniques taught intro stats course focus learning means variables. Structural equation modeling little different focused explaining variability data—changes one variables predict changes variables.10A habit ’ll start forming now mean center variables. subtracting mean variable values.Let’s use \\(X_{6}\\) defined , normally distributed variable mean 4 standard deviation 3. interpret values \\(X_{6} - \\overline{X_{6}}\\)? (Remember, just second column variance tables earlier.)shift \\(X_{6}\\) values left \\(\\overline{X_{6}}\\) units, mean new list numbers?Let’s verify R. ’ll use “suffix” mc indicate mean-centered variable.answer exactly agree “theoretical” answer came lines ? (don’t already know, e-16 expression scientific notation means “times \\(10^{-16}\\). ’s really small number!)Take guess variance X6_mc. Verify guess R.good news mean centering preserves variance. mean shifted 0, variance change, statistical model build analyzes variance affecting mean-centering.","code":"\nX6_mc <- X6 - mean(X6)\nmean(X6_mc)## [1] 2.851573e-16"},{"path":"variance.html","id":"variance-standardizing","chapter":"2 Variance","heading":"2.7 Standardizing data","text":"’ve mean centered data, can go one step divide standard deviation. results something often called z-score. process converting variables original units z-scores called standardizing data.\\[\nZ = \\frac{\\left(X - \\overline{X}\\right)}{SD(X)}\n\\]useful? One reason remove units measurement facilitate comparisons variables. Suppose \\(X\\) represents height inches. numerator (\\(X - \\overline{X}\\)) units inches. standard deviation \\(SD(X)\\) also units inches. divide, units go away z-score left without units, sometimes called “dimensionless quantity”.Suppose female United States 6 feet tall (72 inches). Suppose female China 5’8 tall (68 inches). absolute terms, American woman taller Chinese woman. ’re interested knowing woman taller relative respective population?mean height American woman 65” standard deviation 3.5” mean height Chinese woman 62” standard deviation 2.5”. (numbers aren’t perfectly correct, ’re probably close-ish.)Calculate z-scores women.woman taller relative population?Although z-scores don’t technically units, can think measuring many standard deviations value lies mean.z-score value equals mean?meaning negative z-score?z-score American woman 2. means height measures two standard deviations mean.real-world data, use technology . temperature measurements New York 1974. (daily highs across six-month period.)calculate mean standard deviation:average high 78 degrees Fahrenheit standard deviation 9.5 degrees Fahrenheit.just subtract mean, get mean-centered data.also divide standard deviation, get standardized variable (set z-scores). Note extra parentheses make sure get order operations right. subtract first, divide whole mean-centered quantity standard deviation.easier way R use scale command. (Sorry, output little long. Keep scrolling .)Although outputs formatted little differently, can go back check sets numbers match .mean standardized variable? know ?Let’s calculate variance standardized variable. , ’ll note mean \\(\\overline{X}\\) just number. Also, standard deviation \\(SD(X)\\) just number. make calculation easier understand, let’s just substitute letters easier work :\\(M = \\overline{X}\\)\\(S = SD(X)\\)Remember, \\(M\\) \\(S\\) constants.Now need calculate \\(Var(Z)\\). ’ll first couple steps. take , using variance rules earlier chapter, simplify expression get numerical answer. sure justify step citing rule invoked get .\\[\\begin{align}\nVar(Z) &= Var\\left(\\frac{\\left(X - \\overline{X}\\right)}{SD(X)}\\right) \\\\\n    &= Var\\left(\\frac{\\left(X - M\\right)}{S}\\right) \\\\\n    &= Var\\left(\\frac{1}{S}\\left(X - M\\right)\\right) \\\\\n    &= \\quad ???\n\\end{align}\\]get answer 1. standardized variable always variance 1. important fact future chapters.","code":"\nairquality$Temp##   [1] 67 72 74 62 56 66 65 59 61 69 74 69 66 68 58 64 66 57 68 62 59 73 61 61 57\n##  [26] 58 57 67 81 79 76 78 74 67 84 85 79 82 87 90 87 93 92 82 80 79 77 72 65 73\n##  [51] 76 77 76 76 76 75 78 73 80 77 83 84 85 81 84 83 83 88 92 92 89 82 73 81 91\n##  [76] 80 81 82 84 87 85 74 81 82 86 85 82 86 88 86 83 81 81 81 82 86 85 87 89 90\n## [101] 90 92 86 86 82 80 79 77 79 76 78 78 77 72 75 79 81 86 88 97 94 96 94 91 92\n## [126] 93 93 87 84 80 78 75 73 81 76 77 71 71 78 67 76 68 82 64 71 81 69 63 70 77\n## [151] 75 76 68\nmean(airquality$Temp)## [1] 77.88235\nsd(airquality$Temp)## [1] 9.46527\nairquality$Temp - mean(airquality$Temp)##   [1] -10.8823529  -5.8823529  -3.8823529 -15.8823529 -21.8823529 -11.8823529\n##   [7] -12.8823529 -18.8823529 -16.8823529  -8.8823529  -3.8823529  -8.8823529\n##  [13] -11.8823529  -9.8823529 -19.8823529 -13.8823529 -11.8823529 -20.8823529\n##  [19]  -9.8823529 -15.8823529 -18.8823529  -4.8823529 -16.8823529 -16.8823529\n##  [25] -20.8823529 -19.8823529 -20.8823529 -10.8823529   3.1176471   1.1176471\n##  [31]  -1.8823529   0.1176471  -3.8823529 -10.8823529   6.1176471   7.1176471\n##  [37]   1.1176471   4.1176471   9.1176471  12.1176471   9.1176471  15.1176471\n##  [43]  14.1176471   4.1176471   2.1176471   1.1176471  -0.8823529  -5.8823529\n##  [49] -12.8823529  -4.8823529  -1.8823529  -0.8823529  -1.8823529  -1.8823529\n##  [55]  -1.8823529  -2.8823529   0.1176471  -4.8823529   2.1176471  -0.8823529\n##  [61]   5.1176471   6.1176471   7.1176471   3.1176471   6.1176471   5.1176471\n##  [67]   5.1176471  10.1176471  14.1176471  14.1176471  11.1176471   4.1176471\n##  [73]  -4.8823529   3.1176471  13.1176471   2.1176471   3.1176471   4.1176471\n##  [79]   6.1176471   9.1176471   7.1176471  -3.8823529   3.1176471   4.1176471\n##  [85]   8.1176471   7.1176471   4.1176471   8.1176471  10.1176471   8.1176471\n##  [91]   5.1176471   3.1176471   3.1176471   3.1176471   4.1176471   8.1176471\n##  [97]   7.1176471   9.1176471  11.1176471  12.1176471  12.1176471  14.1176471\n## [103]   8.1176471   8.1176471   4.1176471   2.1176471   1.1176471  -0.8823529\n## [109]   1.1176471  -1.8823529   0.1176471   0.1176471  -0.8823529  -5.8823529\n## [115]  -2.8823529   1.1176471   3.1176471   8.1176471  10.1176471  19.1176471\n## [121]  16.1176471  18.1176471  16.1176471  13.1176471  14.1176471  15.1176471\n## [127]  15.1176471   9.1176471   6.1176471   2.1176471   0.1176471  -2.8823529\n## [133]  -4.8823529   3.1176471  -1.8823529  -0.8823529  -6.8823529  -6.8823529\n## [139]   0.1176471 -10.8823529  -1.8823529  -9.8823529   4.1176471 -13.8823529\n## [145]  -6.8823529   3.1176471  -8.8823529 -14.8823529  -7.8823529  -0.8823529\n## [151]  -2.8823529  -1.8823529  -9.8823529\n(airquality$Temp - mean(airquality$Temp))/sd(airquality$Temp)##   [1] -1.14971398 -0.62146702 -0.41016823 -1.67796094 -2.31185730 -1.25536337\n##   [7] -1.36101276 -1.99490912 -1.78361034 -0.93841519 -0.41016823 -0.93841519\n##  [13] -1.25536337 -1.04406459 -2.10055851 -1.46666216 -1.25536337 -2.20620791\n##  [19] -1.04406459 -1.67796094 -1.99490912 -0.51581762 -1.78361034 -1.78361034\n##  [25] -2.20620791 -2.10055851 -2.20620791 -1.14971398  0.32937752  0.11807873\n##  [31] -0.19886945  0.01242934 -0.41016823 -1.14971398  0.64632570  0.75197509\n##  [37]  0.11807873  0.43502691  0.96327387  1.28022205  0.96327387  1.59717023\n##  [43]  1.49152084  0.43502691  0.22372813  0.11807873 -0.09322005 -0.62146702\n##  [49] -1.36101276 -0.51581762 -0.19886945 -0.09322005 -0.19886945 -0.19886945\n##  [55] -0.19886945 -0.30451884  0.01242934 -0.51581762  0.22372813 -0.09322005\n##  [61]  0.54067630  0.64632570  0.75197509  0.32937752  0.64632570  0.54067630\n##  [67]  0.54067630  1.06892327  1.49152084  1.49152084  1.17457266  0.43502691\n##  [73] -0.51581762  0.32937752  1.38587145  0.22372813  0.32937752  0.43502691\n##  [79]  0.64632570  0.96327387  0.75197509 -0.41016823  0.32937752  0.43502691\n##  [85]  0.85762448  0.75197509  0.43502691  0.85762448  1.06892327  0.85762448\n##  [91]  0.54067630  0.32937752  0.32937752  0.32937752  0.43502691  0.85762448\n##  [97]  0.75197509  0.96327387  1.17457266  1.28022205  1.28022205  1.49152084\n## [103]  0.85762448  0.85762448  0.43502691  0.22372813  0.11807873 -0.09322005\n## [109]  0.11807873 -0.19886945  0.01242934  0.01242934 -0.09322005 -0.62146702\n## [115] -0.30451884  0.11807873  0.32937752  0.85762448  1.06892327  2.01976780\n## [121]  1.70281962  1.91411841  1.70281962  1.38587145  1.49152084  1.59717023\n## [127]  1.59717023  0.96327387  0.64632570  0.22372813  0.01242934 -0.30451884\n## [133] -0.51581762  0.32937752 -0.19886945 -0.09322005 -0.72711641 -0.72711641\n## [139]  0.01242934 -1.14971398 -0.19886945 -1.04406459  0.43502691 -1.46666216\n## [145] -0.72711641  0.32937752 -0.93841519 -1.57231155 -0.83276580 -0.09322005\n## [151] -0.30451884 -0.19886945 -1.04406459\nscale(airquality$Temp)##               [,1]\n##   [1,] -1.14971398\n##   [2,] -0.62146702\n##   [3,] -0.41016823\n##   [4,] -1.67796094\n##   [5,] -2.31185730\n##   [6,] -1.25536337\n##   [7,] -1.36101276\n##   [8,] -1.99490912\n##   [9,] -1.78361034\n##  [10,] -0.93841519\n##  [11,] -0.41016823\n##  [12,] -0.93841519\n##  [13,] -1.25536337\n##  [14,] -1.04406459\n##  [15,] -2.10055851\n##  [16,] -1.46666216\n##  [17,] -1.25536337\n##  [18,] -2.20620791\n##  [19,] -1.04406459\n##  [20,] -1.67796094\n##  [21,] -1.99490912\n##  [22,] -0.51581762\n##  [23,] -1.78361034\n##  [24,] -1.78361034\n##  [25,] -2.20620791\n##  [26,] -2.10055851\n##  [27,] -2.20620791\n##  [28,] -1.14971398\n##  [29,]  0.32937752\n##  [30,]  0.11807873\n##  [31,] -0.19886945\n##  [32,]  0.01242934\n##  [33,] -0.41016823\n##  [34,] -1.14971398\n##  [35,]  0.64632570\n##  [36,]  0.75197509\n##  [37,]  0.11807873\n##  [38,]  0.43502691\n##  [39,]  0.96327387\n##  [40,]  1.28022205\n##  [41,]  0.96327387\n##  [42,]  1.59717023\n##  [43,]  1.49152084\n##  [44,]  0.43502691\n##  [45,]  0.22372813\n##  [46,]  0.11807873\n##  [47,] -0.09322005\n##  [48,] -0.62146702\n##  [49,] -1.36101276\n##  [50,] -0.51581762\n##  [51,] -0.19886945\n##  [52,] -0.09322005\n##  [53,] -0.19886945\n##  [54,] -0.19886945\n##  [55,] -0.19886945\n##  [56,] -0.30451884\n##  [57,]  0.01242934\n##  [58,] -0.51581762\n##  [59,]  0.22372813\n##  [60,] -0.09322005\n##  [61,]  0.54067630\n##  [62,]  0.64632570\n##  [63,]  0.75197509\n##  [64,]  0.32937752\n##  [65,]  0.64632570\n##  [66,]  0.54067630\n##  [67,]  0.54067630\n##  [68,]  1.06892327\n##  [69,]  1.49152084\n##  [70,]  1.49152084\n##  [71,]  1.17457266\n##  [72,]  0.43502691\n##  [73,] -0.51581762\n##  [74,]  0.32937752\n##  [75,]  1.38587145\n##  [76,]  0.22372813\n##  [77,]  0.32937752\n##  [78,]  0.43502691\n##  [79,]  0.64632570\n##  [80,]  0.96327387\n##  [81,]  0.75197509\n##  [82,] -0.41016823\n##  [83,]  0.32937752\n##  [84,]  0.43502691\n##  [85,]  0.85762448\n##  [86,]  0.75197509\n##  [87,]  0.43502691\n##  [88,]  0.85762448\n##  [89,]  1.06892327\n##  [90,]  0.85762448\n##  [91,]  0.54067630\n##  [92,]  0.32937752\n##  [93,]  0.32937752\n##  [94,]  0.32937752\n##  [95,]  0.43502691\n##  [96,]  0.85762448\n##  [97,]  0.75197509\n##  [98,]  0.96327387\n##  [99,]  1.17457266\n## [100,]  1.28022205\n## [101,]  1.28022205\n## [102,]  1.49152084\n## [103,]  0.85762448\n## [104,]  0.85762448\n## [105,]  0.43502691\n## [106,]  0.22372813\n## [107,]  0.11807873\n## [108,] -0.09322005\n## [109,]  0.11807873\n## [110,] -0.19886945\n## [111,]  0.01242934\n## [112,]  0.01242934\n## [113,] -0.09322005\n## [114,] -0.62146702\n## [115,] -0.30451884\n## [116,]  0.11807873\n## [117,]  0.32937752\n## [118,]  0.85762448\n## [119,]  1.06892327\n## [120,]  2.01976780\n## [121,]  1.70281962\n## [122,]  1.91411841\n## [123,]  1.70281962\n## [124,]  1.38587145\n## [125,]  1.49152084\n## [126,]  1.59717023\n## [127,]  1.59717023\n## [128,]  0.96327387\n## [129,]  0.64632570\n## [130,]  0.22372813\n## [131,]  0.01242934\n## [132,] -0.30451884\n## [133,] -0.51581762\n## [134,]  0.32937752\n## [135,] -0.19886945\n## [136,] -0.09322005\n## [137,] -0.72711641\n## [138,] -0.72711641\n## [139,]  0.01242934\n## [140,] -1.14971398\n## [141,] -0.19886945\n## [142,] -1.04406459\n## [143,]  0.43502691\n## [144,] -1.46666216\n## [145,] -0.72711641\n## [146,]  0.32937752\n## [147,] -0.93841519\n## [148,] -1.57231155\n## [149,] -0.83276580\n## [150,] -0.09322005\n## [151,] -0.30451884\n## [152,] -0.19886945\n## [153,] -1.04406459\n## attr(,\"scaled:center\")\n## [1] 77.88235\n## attr(,\"scaled:scale\")\n## [1] 9.46527"},{"path":"covariance.html","id":"covariance","chapter":"3 Covariance","heading":"3 Covariance","text":"","code":""},{"path":"covariance.html","id":"covariance-calculating","chapter":"3 Covariance","heading":"3.1 Calculating covariance","text":"last chapter variance, measures spread single variable. Now extend idea pairs variables.say two variables “co-vary” spread one variable related spread another variable. relationship represents association two variables.’ll call two variables \\(X_{1}\\) \\(X_{2}\\). keep things simple, let’s assume already mean centered variables.\\(X_{1}\\) \\(X_{2}\\) already mean centered, \\(\\overline{X_{1}}\\) \\(\\overline{X_{2}}\\)?last chapter variance, ’ll build calculation covariance step--step using table keep track intermediate quantities need.two variables (\\(n = 7\\)) mean centered:Check mean columns truly zero.Something interesting happens look product \\(X_{1}X_{2}\\).\\(X_{1}\\) \\(X_{2}\\) lie means, positive numbers. Therefore, product positive.\\(X_{1}\\) \\(X_{2}\\) lie means? know values individually know product?chart , products listed new column:Now add products across seven data pairs:\\(X_{1}\\) \\(X_{2}\\) tend similar values (positive negative), product usually positive. ’s true every pair values table ; products negative. majority positive. Therefore, sum products positive.’re almost . Just like wanted average squared deviation calculate variance, want average products third column . just like case variance, ’s quite average calculate. Instead dividing \\(n\\), divide \\(n - 1\\) exactly esoteric reason. example, 7 data points (words, 7 rows data), divide 6.Putting together:diagrams, covariance two variables indicated curved, double-headed arrow pointing boxes labeled value covariance, like :Note still include variances individual variables. still important us. just one new type arrow now.Verify variances diagram correct example. can hand want, using R fine .final formula covariance, written \\(Cov\\left(X_{1}, X_{2}\\right)\\). works pairs variables, even aren’t mean centered. terms \\(\\left(X_{1} - \\overline{X_1}\\right)\\) \\(\\left(X_{2} - \\overline{X_2}\\right)\\) mean centering:\\[\nCov\\left(X_{1}, X_{2}\\right) = \\frac{\\sum \\left(X_{1} - \\overline{X_{1}}\\right)\\left(X_{2} - \\overline{X_{2}}\\right)}{n - 1}\n\\]Suppose \\(X_{1}\\) tends mean \\(X_{2}\\) mean \\(X_{1}\\) tends mean \\(X_{2}\\) mean. product \\(\\left(X_{1} - \\overline{X_{1}}\\right)\\left(X_{2} - \\overline{X_{2}}\\right)\\) usually ? Therefore, sum products likely ?general variables (necessarily mean centered), table actually look like :Calculate covariance hand making table like one . (variables mean centered, ’ll calculate mean variable order fill third fourth columns.)\\(X_{3}\\): 8, 10, 16, 7, 4, 3\\(X_{4}\\): 6, 5, 4, 9, 11, 7Explain intuitively covariance negative two variables.calculating variance, order data points matter. ?calculating covariance, order data points matter. ?keep pairs together, rearrange rows table. affect covariance?","code":""},{"path":"covariance.html","id":"covariance-r","chapter":"3 Covariance","heading":"3.2 Calculating covariance in R","text":"’ve done hand times make sure understand formula works, can let R work us:’s real world data:","code":"\nX1 <- c(-1,-2, 2, -3, 4, -1, 1)\nX2 <- c(-2, 2, -2, -1, 2, -2, 3)\ncov(X1, X2)## [1] 1.666667\nX3 <- c(8, 10, 16, 7, 4, 3)\nX4 <- c(6, 5, 4, 9, 11, 7)\ncov(X3, X4)## [1] -9.2\ncov(airquality$Temp, airquality$Wind)## [1] -15.27214"},{"path":"covariance.html","id":"covariance-rules","chapter":"3 Covariance","heading":"3.3 Covariance rules","text":"’ll think variance covariance rules one big list. left Rule 4, now ’ll introduce Rule 5.Rule 5\\[\nCov(X, X) = Var(X)\n\\]words, Rule 5 states covariance variable just thing variance variable. quite remarkable! means variance really just special case covariance.Explain Rule 5 true. (Hint: think calculate \\(Cov(X, X)\\) using either formula table—!)Rule 6\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{2}, X_{1}\\right)\n\\]words, say covariance symmetric.Explain Rule 6 true. (, think formula table—!)next four rules analogous similar rules variance (Rule 1, Rule 2, Rule 3, Rule 4).Rule 7Suppose \\(C\\) “constant” variable, meaning always value (rather variable contain lots different numbers). ,\\[\nCov\\left(X, C\\right) = 0\n\\]always, try explain rule. Give intuitive explanation rule “” true. think computationally, thinking either formula table—!Rule 8\\[\nCov\\left(X_{1} + X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) + Cov\\left(X_{2}, X_{3}\\right)  \n\\]appreciate longer restriction relationships among variables involved. Rule 2 worked two variables independent. hand, Rule 8 works combination variables, matter relation.Even satisfying next rule:Rule 9\\[\nCov\\left(X_{1} - X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) - Cov\\left(X_{2}, X_{3}\\right)  \n\\]Yay! minus sign behaves sensibly now! course, since covariances can positive negative (unlike variances always positive!) can safely subtract two without worry. rule, like Rule 8, depend \\(X_{1}\\) \\(X_{2}\\) independent. can two variables.versions rules addition subtraction side, just minor variations Rule 8 Rule 9, ’re worth mentioning separate rule. Remember covariance symmetric, can always swap things left right comma.\\[\nCov\\left(X_{1}, X_{2} \\pm X_{3}\\right) = Cov\\left(X_{1}, X_{2}\\right) \\pm Cov\\left(X_{1}, X_{3}\\right)  \n\\]Rule 10If \\(\\) number,\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{1}, X_{2}\\right) =  Cov\\left(X_{1}, X_{2}\\right)  \n\\]rule also sensible. Instead Rule 4 takes number \\(\\) pulls \\(^{2}\\), Rule 10 just pulls single factor \\(\\) (either slot).Just couple rules. talking independence conjunction Rule 8 Rule 9. leads directly interesting super-important rule:Rule 11If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nCov\\left(X_{1}, X_{2}\\right) = 0\n\\]Rule 11 true, intuitively?’s interesting note rule works one way. words, know two variables independent, can conclude covariance zero. However, know covariance zero, doesn’t necessarily mean two variables independent. ’ll see example later chapter.Finally, one rule rule :Rule 12For two variables \\(X_{1}\\) \\(X_{2}\\):\\[\nVar(aX_{1} + bX_{2}) =\n    ^2Var(X_{1}) + b^2Var(X_{2}) + 2abCov(X_{1}, X_{2})\n\\]brings practically everything know together one rule!Proving Rule 12 give us good practice type manipulation ’ll need future chapters. goes. first steps, name rule ’re invoking. , ’ll pick thread follow last steps .\\[\\begin{align}\nVar(aX_{1} + bX_{2}) &= Cov(aX_{1} + bX_{2}, aX_{1} + bX_{2}) \\\\\n    &= Cov(aX_{1} + bX_{2}, aX_{1}) + Cov(aX_{1} + bX_{2}, bX_{2}) \\\\\n    &=  Cov(aX_{1}, aX_{1}) +\n        Cov(bX_{2}, aX_{1}) + \\\\\n    &   \\qquad Cov(aX_{1}, bX_{2}) +\n        Cov(bX_{2}, bX_{2}) \\\\\n    &= \\quad ???\n\\end{align}\\]’ll need rules calculations future chapters. Rather search Chapter 2 chapter, ’ve gathered rules one convenient place Appendix .","code":""},{"path":"covariance.html","id":"covariance-correlation","chapter":"3 Covariance","heading":"3.4 Correlation","text":"pros cons calculating covariance similar variance. mathematics much nicer covariance, lose interpretability.Let’s suppose \\(X_{1}\\) measures salary dollars \\(X_{2}\\) measures years education. expect association variables, calculate covariance. unit measurement resulting number?solution problem simple variance. Since variance squared units, take square root. Covariance weird product units, something clever.Following activity , let’s suppose covariance units “dollar-years”. divide number expressed dollars, get rid units ’re left years. seems unsatisfying; covariance express something variables went . Likewise, makes sense divide number expressed years leave us just dollars.solution dilemma accept aren’t going able keep units meaningful way. Therefore, want something standardized, meaning units.\\(X_{1}\\) expressed dollars, can think statistic measures spread also units dollars?Likewise, \\(X_{2}\\) measured years, statistic measures spread also units years?previous activity gives us idea. divide covariance standard deviation \\(X_{1}\\) standard deviation \\(X_{2}\\)?\\[\n\\frac{Cov(X_{1},X_{2})}{SD(X_{1}) SD(X_{2})}\n\\]Sometimes ’s written like :\\[\n\\frac{Cov(X_{1},X_{2})}{\\sqrt{Var(X_{1})} \\sqrt{Var(X_{2})}}\n\\]\n’s thing, right?quantity units. call correlation \\(X_{1}\\) \\(X_{2}\\). ’ll either write\n\\[\nCorr(X_{1}, X_{2})\n\\]\n, need concise,\n\\[\nr_{X_{1}X_{2}}\n\\]Yes, correlation coefficient learned intro stats class, although wasn’t likely presented quite way.11One great thing correlation units, serves sort “universal” measure two variables co-vary. best part nice intuitive meaning precisely factors pieces covariance spread two variables individually. words, fact \\(X_{1}\\) \\(X_{2}\\) variability actually complicates notion covariance. individual variances “corrupt” interpretation covariance. excising , ’s left correlation “pure” part covariance expresses relationship association \\(X_{1}\\) \\(X_{2}\\).","code":""},{"path":"covariance.html","id":"covariance-standardized","chapter":"3 Covariance","heading":"3.5 Covariance with standardized data","text":"last chapter, showed variance standardized variable 1. covariance two standardized variables?Let’s standardize \\(X_{1}\\) \\(X_{2}\\). make math little easier, ’ll use similar notation used end last chapter.\\(M_{1} = \\overline{X_{1}}\\)\\(S_{1} = SD(X_{1})\\)\\(M_{2} = \\overline{X_{2}}\\)\\(S_{2} = SD(X_{2})\\)’ll write z-scores way amenable mathematical manipulation (like ):\\[\nZ_{1} = \\frac{1}{S_{1}}\\left(X_{1} - M_{1}\\right)\n\\]\\[\nZ_{2} = \\frac{1}{S_{2}}\\left(X_{2} - M_{2}\\right)\n\\]looks little intimidating, apply rules, works :\\[\\begin{align}\nCov(Z_{1}, Z_{2}) &= Cov\\left( \\frac{1}{S_{1}}\\left(X_{1} - M_{1}\\right), \\frac{1}{S_{2}}\\left(X_{2} - M_{2}\\right) \\right) \\\\\n    &= \\quad ???\n\\end{align}\\]Work . Take time. Apply rules carefully. know ’re aiming , get\\[\nCov(Z_{1}, Z_{2}) = \\frac{Cov\\left( X_{1}, X_{2} \\right)}{S_{1} S_{2}}\n\\]Okay, now remember \\(S_{1}\\) just convenient substitute \\(SD(X_{1})\\) \\(S_{2}\\) just substitute \\(SD(X_{2})\\). Wait, answer look familiar?cool! Correlation simply covariance two variables ’ve standardized.also reinforces earlier comment interpreting covariance removing extraneous influence spread individual variables. Standardizing variables makes spread variables 1, covariance now pure representation just association .probably remember intro stats correlation takes values -1 1. fact obvious formula . fraction\n\\[\n\\frac{Cov(X_{1},X_{2})}{SD(X_{1}) SD(X_{2})}\n\\]\nbounded -1 1?Let’s go back standardized variable keep things simple. correlation just covariance two standardized variables:\\[\nCorr(X_{1}, X_{2}) = Cov(Z_{1}, Z_{2})\n\\]Use rules calculate :\\[\nVar(Z_{1} + Z_{2})\n\\]Remember \\(Z_{1}\\) \\(Z_{2}\\) necessarily independent. (fact, hope . Otherwise, care correlation? zero!) need Rule 12, Rule 2. Keep manipulating get\n\\[\n2 + 2Corr(X_{1}, X_{2})\n\\]Since variances always non-negative, now know \\[\n0 \\leq 2 + 2Corr(X_{1}, X_{2})\n\\]\nSolve inequality \\(Corr(X_{1}, X_{2})\\).Now follow exact steps \n\\[\nVar(Z_{1} - Z_{2})\n\\]little change answer, one small change. , solve resulting inequality. (Don’t forget key rule working inequalities multiplying dividing negative number changes direction inequality.)fact state without proof:Correlation interpretable strength linear associations.? Basically, boils fact “perfect” correlation 1 -1 achievable data points lie perfectly straight line. Therefore, thinking correlation lying 0 1 (0 -1) sensible judging close points lying straight line. ’ll see examples next section plot data.calculation correlation R, use cor command:Use R confirm number covariance divided product standard deviations.","code":"\ncor(airquality$Temp, airquality$Wind)## [1] -0.4579879"},{"path":"covariance.html","id":"covariance-visualizing","chapter":"3 Covariance","heading":"3.6 Visualizing correlation","text":"Covariance hard interpret, ’re visualizing data want understand association might exist two variables, correlation much better statistic calculate. Let’s see correlation relates graph two variables.getting graphing, need load packages. tidyverse whole set commonly used packages allow us work data frames (“tibbles” cool kids calling ) make graphs. sure load package typing following R going :fact, , ’ll start chapter loading necessary libraries R ’ll need.standard graph two numerical variables scatterplot. Let’s start straight line relationship. First, define two variables. ’ll use shortcuts make lives little easier. seq command just generates sequence numbers.can establish linear relationship just declaring one formula:put variables graph, helps make columns single tibble.graph:Now correlation:1, expected.perfectly straight line negative slope?Throw new variable tibble already (convenience). explain syntax , %>% symbol called “pipe” tells R pass linear_data tibble next row process . processing dictated bind_cols command tells R “bind new column” tibble. part says X7 = X7 may little confusing. says add new column X7, also still call X7., expected.happens plot random data? runif command just chooses random numbers uniformly 0 1.12What guess correlation \\(X_{8}\\) \\(X_{9}\\)?Now calculate using R? get exact answer guessed? , ?data follows perfect mathematical relationship straight line? example, part parabola.Now correlation:large correlation, exactly 1, even though points follow precise mathematical relationship. relationship linear.’s fascinating example. , ’ll want parabola goes .looking answer, guess correlation \\(X_{5}\\) \\(X_{11}\\)?Now calculate correlation R., ’s perfect mathematical relationship two variables. definitely associated. correlation 0?Recall earlier promise discuss Rule 11. two variables independent, covariance zero, , therefore, correlation also zero. However, rule doesn’t work way around. claim knowing covariance/correlation zero imply (necessarily) two variables independent. promised example phenomenon. \\(X_{5}\\) \\(X_{11}\\) zero correlation. yet, \\(X_{5}\\) \\(X_{11}\\) definitely independent.important enough fancy box:see correlation two variables zero near zero, careful conclude variables independent.zero near-zero correlation indicates lack linear association two variables. may nonlinear associations. ’s ’s always good idea graph data.Real data , course, much messier ’s just possible perfect correlations two variables measured real world. (find perfect correlation two columns data, chances either recorded column twice, second column simple transformation first column, like multiplying every value number something like .)plot temperature (degrees Fahrenheit) wind speed (mph) New York air quality data set.Just looking scatterplot (without calculating anything), correlation two variables positive negative? Try guessing exact value correlation.Now calculate exact value correlation see close .want practice looking scatterplots guessing correlation, try online game:Guess CorrelationTurn sound! whole class plays time, classroom sound like arcade. Compete classmates see can get high score.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.7     ✔ dplyr   1.0.9\n## ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nX5 <- seq(1, 9)\nX5## [1] 1 2 3 4 5 6 7 8 9\nX6 <- 3 + 0.5 * X5\nX6## [1] 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5\nlinear_data <- tibble(X5, X6)\nlinear_data## # A tibble: 9 × 2\n##      X5    X6\n##   <int> <dbl>\n## 1     1   3.5\n## 2     2   4  \n## 3     3   4.5\n## 4     4   5  \n## 5     5   5.5\n## 6     6   6  \n## 7     7   6.5\n## 8     8   7  \n## 9     9   7.5\nggplot(linear_data, aes(y = X6, x = X5)) +\n    geom_point()\ncor(X5, X6)## [1] 1\nX7 <- 5 - 0.2 * X5\nX7## [1] 4.8 4.6 4.4 4.2 4.0 3.8 3.6 3.4 3.2\nlinear_data <- linear_data %>%\n    bind_cols(X7 = X7)\nlinear_data## # A tibble: 9 × 3\n##      X5    X6    X7\n##   <int> <dbl> <dbl>\n## 1     1   3.5   4.8\n## 2     2   4     4.6\n## 3     3   4.5   4.4\n## 4     4   5     4.2\n## 5     5   5.5   4  \n## 6     6   6     3.8\n## 7     7   6.5   3.6\n## 8     8   7     3.4\n## 9     9   7.5   3.2\nggplot(linear_data, aes(y = X7, x = X5)) +\n    geom_point()\ncor(X5, X7)## [1] -1\nset.seed(1234)\nX8 <- runif(20)\nX9 <- runif(20)\nX8##  [1] 0.113703411 0.622299405 0.609274733 0.623379442 0.860915384 0.640310605\n##  [7] 0.009495756 0.232550506 0.666083758 0.514251141 0.693591292 0.544974836\n## [13] 0.282733584 0.923433484 0.292315840 0.837295628 0.286223285 0.266820780\n## [19] 0.186722790 0.232225911\nX9##  [1] 0.31661245 0.30269337 0.15904600 0.03999592 0.21879954 0.81059855\n##  [7] 0.52569755 0.91465817 0.83134505 0.04577026 0.45609148 0.26518667\n## [13] 0.30467220 0.50730687 0.18109621 0.75967064 0.20124804 0.25880982\n## [19] 0.99215042 0.80735234\nrandom_data <- tibble(X8, X9)\nrandom_data## # A tibble: 20 × 2\n##         X8     X9\n##      <dbl>  <dbl>\n##  1 0.114   0.317 \n##  2 0.622   0.303 \n##  3 0.609   0.159 \n##  4 0.623   0.0400\n##  5 0.861   0.219 \n##  6 0.640   0.811 \n##  7 0.00950 0.526 \n##  8 0.233   0.915 \n##  9 0.666   0.831 \n## 10 0.514   0.0458\n## 11 0.694   0.456 \n## 12 0.545   0.265 \n## 13 0.283   0.305 \n## 14 0.923   0.507 \n## 15 0.292   0.181 \n## 16 0.837   0.760 \n## 17 0.286   0.201 \n## 18 0.267   0.259 \n## 19 0.187   0.992 \n## 20 0.232   0.807\nggplot(random_data, aes(y = X9, x = X8)) +\n    geom_point()\nX10 <- 0.1 * X5^2\nX10## [1] 0.1 0.4 0.9 1.6 2.5 3.6 4.9 6.4 8.1\nnonlinear_data <- tibble(X5, X10)\nnonlinear_data## # A tibble: 9 × 2\n##      X5   X10\n##   <int> <dbl>\n## 1     1   0.1\n## 2     2   0.4\n## 3     3   0.9\n## 4     4   1.6\n## 5     5   2.5\n## 6     6   3.6\n## 7     7   4.9\n## 8     8   6.4\n## 9     9   8.1\nggplot(nonlinear_data, aes(y = X10, x = X5)) +\n    geom_point()\ncor(X5, X10)## [1] 0.975281\nX11 <- 0.5 * (X5 - 5)^2\nX11## [1] 8.0 4.5 2.0 0.5 0.0 0.5 2.0 4.5 8.0\nnonlinear_data <- nonlinear_data %>%\n    bind_cols(X11 = X11)\nnonlinear_data## # A tibble: 9 × 3\n##      X5   X10   X11\n##   <int> <dbl> <dbl>\n## 1     1   0.1   8  \n## 2     2   0.4   4.5\n## 3     3   0.9   2  \n## 4     4   1.6   0.5\n## 5     5   2.5   0  \n## 6     6   3.6   0.5\n## 7     7   4.9   2  \n## 8     8   6.4   4.5\n## 9     9   8.1   8\nggplot(nonlinear_data, aes(y = X11, x = X5)) +\n    geom_point()\nggplot(airquality, aes(y = Temp, x = Wind)) +\n    geom_point()"},{"path":"simple.html","id":"simple","chapter":"4 Simple regression","heading":"4 Simple regression","text":"","code":""},{"path":"simple.html","id":"preliminaries","chapter":"4 Simple regression","heading":"Preliminaries","text":"need load packages need chapter. tidyverse package sorts utilities working tibbles (data frames). also first introduction lavaan package used throughout rest book.","code":"\nlibrary(tidyverse)\nlibrary(lavaan)## This is lavaan 0.6-11\n## lavaan is FREE software! Please report any bugs."},{"path":"simple.html","id":"simple-prediction","chapter":"4 Simple regression","heading":"4.1 Prediction","text":"One important tasks statistics prediction. Given data, can predict value something important population interest?Suppose gathered data anxiety among Utah high school students. various instruments available measuring anxiety, say administered Beck Anxiety Inventory. instrument assigns score 0 63, lower numbers indicating less anxiety higher numbers indicating .take care make sure sample close simple random sample possible ’s representative population (high school students state Utah). sample data, can calculate summary statistics. example, might find mean anxiety score Utah high school students 7.1 standard deviation 3.9.random Utah high school student walks door. don’t know anything . Can say anything anxiety? best guess score might Beck Anxiety Inventory?can lot better another variable can measure. example, let’s suppose data records anxiety, also minutes smart phone usage per day.theory, information smart phone usage potentially help us make better predictions anxiety?suspect association anxiety smart phone usage positive negative? (can Google question check empirical evidence guess.)Now imagine another random Utah high school student walks door. time, tell smart phone usage average (sitting mean). best prediction anxiety score? (Give exact value.)told student walked door higher average smart phone usage? prediction anxiety score? (can’t give exact value , give qualitatively sensible answer.)told student walked door lower average smart phone usage? prediction anxiety score? (, just give qualitatively sensible answer.)","code":""},{"path":"simple.html","id":"simple-terminology","chapter":"4 Simple regression","heading":"4.2 Regression terminology","text":"one variable suspect may help us predict another variable, one way study using simple regression model.related , somewhat different , covariance. Covariance symmetric, expresses idea two variables mutually related. “directionality” relationship. way contrast, simple regression model asserts one variables “explanatory” “response”. words, start values properties explanatory variable try deduce can values properties response variable.Keep mind “directionality” “causality”. ’s possible one variable causes another, needs data collection process (often carefully controlled experiment) clear scientific rationale justifies causal relationship variables can start thinking inferring causality. purposes much book, directionality just means wish establish predictive relationship wherein start properties one variable try predict properties another variable. often “sensible” order based research questions asked hypotheses posed.many different terminological conventions statistics, aware “explanatory” variables also called—often depending discipline context—predictors, features, covariates, controls, regressors, inputs, independent variables. fact, context structural equation modeling, use term “exogenous” refer variables play role. (term much precise definition ’ll discuss future chapters.) “response” variables might called outcomes, outputs, targets, explained variables, dependent variables, among others. book, often use term “endogenous” (, specific way yet explained). data collection process clear scientific rationale justifies causal relationship variables, might able refer variables either “cause” “effect”.Keep mind ’s scientific question want ask determines explanatory/response relationship. different researcher different hypothesis might use two variables roles reversed.anxiety/smart-phone example , variable explanatory response, least according way stated scenario?","code":""},{"path":"simple.html","id":"simple-model","chapter":"4 Simple regression","heading":"4.3 The simple regression model","text":"figure top chapter, now decorated letters (number):goal section explain .variable names \\(X\\) \\(Y\\). \\(X\\) exogenous variable \\(Y\\) endogenous variable. example, \\(X\\) might smart phone usage \\(Y\\) might anxiety score example . section, ’re going concrete calculations using example last chapter wind speed temperature airquality data set. last chapter, simply calculated (symmetric) correlation wind speed temperature. , consider wind speed exogenous temperature endogenous. words, goal use wind speed predictor temperature.letter \\(v\\) requires explanation. variance variable \\(X\\), already know .parameter \\(b\\) supposed measure something predictive relationship \\(X\\) \\(Y\\). attached arrow drawn little thicker arrows diagram. say moment.really weird, new part circle right. “error” term.“error” ? illustrate, let’s plot wind speed temperature. plotting analyzing variables, going mean-center put tibble.Note exogenous variable (wind speed) x-axis endogenous variable (temperature) y-axis.can see negative reasonably linear association variables, let’s add line best fit data.line passes right origin \\((0, 0)\\). ?slope line \\(-1.23\\). ’ll see calculate slope bit. slope mean?Look help file airquality data set. (Either use Help tab RStudio type ?airquality Console.)units measurement \\(X\\)? units measurement \\(Y\\)? Since slope “rise run”, units slope?, idea every additional mile per hour wind speed, predict temperature goes 1.23 degrees Fahrenheit.following sentence incorrect?every additional mile per hour wind speed, temperature goes 1.23 degrees Fahrenheit.point line model makes predictions. long \\(X\\) \\(Y\\) mean-centered, equation line \\[\n\\hat{Y} = bX\n\\]new piece notation : \\(\\hat{Y}\\). symbol represents predicted value \\(Y\\) according model. get actual value \\(Y\\) piece model actual values \\(Y\\) differ model real-world data doesn’t lie perfect straight line. moment.According information , can estimate value \\(b\\) \\(-1.23\\):\\[\n\\hat{Y} = -1.23X\n\\]multiplicative effect. , long \\(X\\) \\(Y\\) mean-centered, knowing value \\(X\\) allows us predict value \\(Y\\) multiplying \\(b\\).predictions almost always wrong. given day, given increase 1 mile per hour wind wind speed, rarely happen temperature drop exactly 1.23 degrees. ’s just sort “average” time. average, ’s slight temperature change associated 1 mph change wind speed, number -1.23 best estimate average change across whole data set. need especially clear increase wind speed necessary cause drop temperature. mean, might partially true, can’t prove observational data. sorts reasons explain increase wind speed drop temperature (like cold front moving ).Since predictions average effects specific guarantees, every prediction make wrong amount. (get extremely lucky, even , ’s difficult imagine situation prediction precisely correct , say, 10 decimal places something like .) Therefore, error prediction. new equation—accounting error—\\[\nY = bX + E\n\\]\n\n\\[\nY = -1.23X + E\n\\]Now use \\(Y\\) instead \\(\\hat{Y}\\). include error, can recover exact value \\(Y\\), longer just prediction straight-line model. Remember :write regression equation endogenous variable includes incoming arrows, including residual error term, use \\(Y\\).write regression equation endogenous variable includes incoming arrows, excluding residual error term, use \\(\\hat{Y}\\).Error funny word negative connotation. sounds like made mistake. Well, model make mistakes. Every model prediction technically wrong. kind mistake results arithmetic wrong anything like . ’s simply “natural” error results messiness real world impossibility predicting anything certainty. reason, often prefer term “residual”. ’s “left ” made prediction. ’s extra change temperature, example, accounted model wind speed alone.residuals evident plot . residuals, every data point lie perfect straight line. data points either little line. vertical distances data line residuals errors. example two residuals plotted red.Points line negative residuals points line positive residuals.residuals appear observed measured variables data. consequence variety unmeasured factors determine temperature aside wind speed. unmeasured variable appears model called latent variable. discuss latent variables far greater detail Chapter 8. now, just know latent variables indicated circles diagram. ’s circle letter \\(E\\) inside.equation\\[\nY = bX + E\n\\]\ncan also written \n\\[\nY = bX + 1 \\cdot E\n\\]“1” represented diagram?letters \\(v\\), \\(b\\), \\(e\\) called free parameters free vary depending data. “1” called fixed parameter. attached arrow, ’s technically parameter model, parameter need calculate. “fixed” value 1 error term model represented \\(+E\\) fixed coefficient 1. general, thoughout book, word “parameter” used without qualification, can assume talking free parameters, ones need calculate.thing diagram hasn’t explained yet \\(e\\).\\(e\\) appear diagram? Given appears, represent mathematically?know curved arrows represent variance. mean measure variance variable can’t observe?scatterplot look like error variance small. error variance large?variability size residuals. small (points close line) large (points far line). spread residuals can estimated data, just like variance calculation.13 turns 70.8. ’ll see calculate .Let’s put everything together diagram.First, need variance \\(X\\) (wind speed). Calculate R. get 12.4.matter calculate variance variable Wind original airquality data, variance \\(X\\) variable airqualilty_mc? ?’s helpful see \\(X\\) \\(Y\\) generic prototypes simple regression model, applied problems now ’ll refer variables using contextually meaningful names. final diagram looks like :error variable \\(E\\) exogenous endogenous?One final note diagram: may noticed \\(Y\\) variable (\\(\\textit{TEMP}\\)) variance term attached. double-headed arrow box. ? point trying explain reasons \\(Y\\) varies using elements model. words, \\(Y\\) variance, variance partially explained \\(X\\). rest variance explained \\(X\\) explained error term \\(E\\). variance \\(Y\\) accounted contribution \\(X\\) \\(E\\) combined.","code":"\nX <- airquality$Wind - mean(airquality$Wind)\nY <- airquality$Temp - mean(airquality$Temp)\nairquality_mc <- tibble(X, Y)\nairquality_mc## # A tibble: 153 × 2\n##        X      Y\n##    <dbl>  <dbl>\n##  1 -2.56 -10.9 \n##  2 -1.96  -5.88\n##  3  2.64  -3.88\n##  4  1.54 -15.9 \n##  5  4.34 -21.9 \n##  6  4.94 -11.9 \n##  7 -1.36 -12.9 \n##  8  3.84 -18.9 \n##  9 10.1  -16.9 \n## 10 -1.36  -8.88\n## # … with 143 more rows\nggplot(airquality_mc, aes(y = Y, x = X)) +\n    geom_point()\nggplot(airquality_mc, aes(y = Y, x = X)) +\n    geom_point() +\n    geom_smooth(method = lm, se = FALSE)## `geom_smooth()` using formula 'y ~ x'\nggplot(airquality_mc, aes(y = Y, x = X)) +\n    geom_point() +\n    geom_smooth(method = lm, se = FALSE) +\n    annotate(\"segment\",\n             x = -2.56, y = 3.15,\n             xend = -2.56, yend = 3.15 - 14.03,\n             color = \"red\", size = 1.5) +\n    annotate(\"segment\",\n             x = 4.94, y = -6.08,\n             xend = 4.94, yend = -6.08 + 9.20,\n             color = \"red\", size = 1.5)## `geom_smooth()` using formula 'y ~ x'"},{"path":"simple.html","id":"simple-assumptions","chapter":"4 Simple regression","heading":"4.4 Simple regression assumptions","text":"calculations need , ability interpret results, depend certain assumptions met.look regression assumptions, might find huge list requirements. requirements relate calculating statistics like P-values regression parameters. now, content simply know makes sense interpret parameters model ., really need four assumptions:data come “good” sample.relationship \\(X\\) \\(Y\\) approximately linear.residuals independent \\(X\\) values.influential outliers.Let’s address one time:mean “good” sample? simple random sample gold standard, ’s usually possible obtain one real world. make sampling process random possible ensure resulting sample representative population ’re trying study possible.can check linearity scatterplot. Just make sure pattern dots doesn’t strong curvature .patterns residuals . randomly scattered around best-fit line average size residuals change radically one side graph .14Check scatterplot outliers. serious ones, assess make sure data entry mistakes. correspond valid data, just throw away.15 Often, solution run analysis including (temporarily) excluding outliers make sure presence doesn’t radically alter parameter estimates.","code":""},{"path":"simple.html","id":"simple-calculating","chapter":"4 Simple regression","heading":"4.5 Calculating regression parameters","text":"Now ’ll show one way calculate regression parameters (numbers) diagram. isn’t way . fact, approach used intro stats classes. approach helpful illustrate way calculations future chapters.Let’s go back diagram without numbers:letter \\(v\\) easy ’s just variance \\(X\\):\\[\nVar(X) = v\n\\]can estimate directly data. (already R temperature wind speed data.) completing activities , also calculate \\(b\\) \\(e\\).get parameters, set equations.\nfirst observation need make , important arrows diagram, ’s just important arrows .arrows directly connecting \\(X\\) \\(E\\)? imply relationship \\(X\\) \\(E\\)? regression assumption related question?Given , \\(Cov(X, E)\\)?Next, \\(Y\\) combination \\(X\\) \\(E\\), ’ve already seen can write\\[\nY = bX + E\n\\]Therefore, can calculate \\(Var(Y)\\) according formula using established rules. (convenient list one place located Appendix .)Keep simplifying following much possible:\\[\\begin{align}\nVar(Y)  &= Var(bX + E) \\\\\n        &= \\quad ???\n\\end{align}\\]end \\[\nb^{2}v + e\n\\]Don’t forget \\(Var(X) = v\\) \\(Var(E) = e\\) diagram!also need use information covariance \\(X\\) \\(Y\\). Keep simplifying calculation :\\[\\begin{align}\nCov(X, Y)  &= Cov(X, bX + E) \\\\\n        &= \\quad ???\n\\end{align}\\]end \\[\nbv\n\\]Use R calculate \\(Var(X)\\), \\(Var(Y)\\) \\(Cov(X, Y)\\).get 12.4, 89.6, -15.3, respectively.Now can set equations need solve various letters want. three equations established:\\[\\begin{align}\n12.4 &= v \\\\\n89.6 &= b^2v + e \\\\\n-15.3 &= bv\n\\end{align}\\]Time little algebra. know \\(v\\). Using value, solve \\(b\\) first (using last equation). , using values \\(b\\) \\(v\\), solve \\(e\\) second equation.Check values got ones earlier diagram (possibility little rounding error).Now let’s go , time, full generality:\\[\\begin{align}\nVar(X) &= v \\\\\nVar(Y) &= b^2v + e \\\\\nCov(X, Y) &= bv\n\\end{align}\\]Therefore,\\[\nv = Var(X)\n\\]\\[\nb = \\frac{Cov(X, Y)}{Var(X)}\n\\]\\[\ne = Var(Y) - \\left( \\frac{Cov(X, Y)}{Var(X)} \\right)^2 Var(X)\n\\]","code":""},{"path":"simple.html","id":"simple-mim","chapter":"4 Simple regression","heading":"4.6 The model-implied matrix","text":"convenient future chapters collect numbers need array terms called sample covariance matrix. (Sometimes called variance-covariance matrix.) idea take covariance possible pairs observed variables arrange follows:\\[\n\\begin{bmatrix}\nCov(X, X)    &    Cov(X, Y) \\\\\nCov(Y, X)    &    Cov(Y, Y) \\\\\n\\end{bmatrix}\n\\]immediate simplifications make.Since \\(Cov(Y, X) = Cov(X, Y)\\), point writing twice. just use dot (\\(\\bullet\\)) replace \\(Cov(Y, X)\\).can replace upper-left lower-right entries (entries -called “diagonal” matrix) variances.final sample covariance matrix:\\[\n\\begin{bmatrix}\nVar(X)       &    Cov(X, Y) \\\\\n\\bullet      &    Var(Y)    \\\\\n\\end{bmatrix}\n\\]data , can calculate numbers quantities.another important matrix called model-implied matrix. Given model, covariance matrix look like? calculations , know model-implied matrix \\[\n\\begin{bmatrix}\nv         &    bv         \\\\\n\\bullet   &    b^{2}v + e \\\\\n\\end{bmatrix}\n\\]letters \\(b\\), \\(v\\), \\(e\\) unknowns. Okay, \\(v\\) unknown. ’s unknown sense parameter model, don’t work hard find . point model parameters estimated equating covariance matrix (calculated data) model-implied matrix trying solve unknown parameters.new math section. matrices just convenient ways organize work ’ve already done. parameter estimation structural equation modeling essentially setting two matrices (sample covariance matrix model-implied matrix) equal attempting solve unknown parameters.\\[\n\\begin{bmatrix}\nVar(X)       &    Cov(X, Y) \\\\\n\\bullet      &    Var(Y)    \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nv         &    bv         \\\\\n\\bullet   &    b^{2}v + e \\\\\n\\end{bmatrix}\n\\]Don’t forget matrix left—sample covariance matrix—consists numbers calculate data. matrix right—model-implied matrix—contains letters, unknown parameters ’re trying find.","code":""},{"path":"simple.html","id":"simple-error-correlation","chapter":"4 Simple regression","heading":"4.7 Error variance in terms of correlation","text":"formulas derived fine far go. allow take quantities calculated data (variances covariances observed variables) translate estimates model parameters.formula error variance \\(e\\) little gross, though. little trickery, can simplify formula quite bit.starting point:\\[\ne = Var(Y) - \\left( \\frac{Cov(X, Y)}{Var(X)} \\right)^2 Var(X)\n\\]Explain right-hand side can rewritten \\[\nVar(Y) - \\frac{Cov(X, Y)^{2}}{Var(X)}\n\\]’re going multiply top bottom fraction right \\(Var(Y)\\):\\[\nVar(Y) - \\frac{Cov(X, Y)^{2} Var(Y)}{Var(X) Var(Y)}\n\\]can factor common term \\(Var(Y)\\) pieces:\\[\nVar(Y) \\left( 1 - \\frac{Cov(X, Y)^{2}}{Var(X) Var(Y)} \\right)\n\\]thing? words, new fraction right look familiar way?hope recognize fraction right just correlation coefficient squared. whole equation can now written \\[\ne = Var(Y) \\left( 1 - r_{XY}^2 \\right)\n\\]nice consequence last equation. term parentheses \\(\\left( 1 - r_{XY}^2 \\right)\\) number 0 1, right? Since multiplying variance \\(Y\\), can think term parentheses proportion. variance \\(Y\\) explained model one two ways. thick arrow coming left uses \\(X\\) predict variance \\(Y\\). rest variance \\(Y\\) left error term \\(e\\). Therefore, \\(\\left( 1 - r_{XY}^2 \\right)\\) proportion variance \\(Y\\) left error.true, must also case \\(r_{XY}^2\\) proportion variance \\(Y\\) explained \\(X\\). Calculating one minus proportion gives complementary proportion. example, \\(\\left( 1 - r_{XY}^2 \\right) = 0.3\\), 30% variance \\(Y\\) left error. implies 70% variance \\(Y\\) explained \\(X\\). \\(1 - 0.3 = 0.7\\).authors write \\(R^{2}\\) instead \\(r^{2}\\) reason.","code":""},{"path":"simple.html","id":"simple-standardized","chapter":"4 Simple regression","heading":"4.8 Regression with standardized variables","text":"Recall convert variables z-scores, variances 1 covariances become correlation coefficients. words, covariance matrix becomes correlation matrix looks like :\\[\n\\begin{bmatrix}\n1       &    r_{XY} \\\\\n\\bullet &    1      \\\\\n\\end{bmatrix}\n\\]model-implied matrix change. Solving parameters , , except can now replace \\(Var(X)\\) \\(Var(Y)\\) 1, \\(Cov(X, Y)\\) \\(r_{XY}\\).\\[\n\\begin{bmatrix}\n1       &    r_{XY}  \\\\\n\\bullet      &    1  \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nv         &    bv         \\\\\n\\bullet   &    b^{2}v + e \\\\\n\\end{bmatrix}\n\\]\\[\\begin{align}\n1 &= v \\\\\nr_{XY} &= bv \\\\\n1 &= b^2v + e\n\\end{align}\\]Therefore,\\[\nv = 1\n\\]\\[\nb = r_{XY}\n\\]\\[\ne = 1 - r_{XY}^{2}\n\\]’ll use scale command create standardized variables temperature wind speed put new tibble.Modify ggplot code earlier chapter create scatterplot new standardized variables along best-fit line. slope line? (Hint: calculate correlation coefficient two standardized variables.)","code":"\nX_std <- scale(airquality$Wind)\nY_std <- scale(airquality$Temp)\nairquality_std <- tibble(X_std, Y_std)\nairquality_std## # A tibble: 153 × 2\n##    X_std[,1] Y_std[,1]\n##        <dbl>     <dbl>\n##  1    -0.726    -1.15 \n##  2    -0.556    -0.621\n##  3     0.750    -0.410\n##  4     0.438    -1.68 \n##  5     1.23     -2.31 \n##  6     1.40     -1.26 \n##  7    -0.385    -1.36 \n##  8     1.09     -1.99 \n##  9     2.88     -1.78 \n## 10    -0.385    -0.938\n## # … with 143 more rows"},{"path":"simple.html","id":"simple-r","chapter":"4 Simple regression","heading":"4.9 Simple regression in R","text":"","code":""},{"path":"simple.html","id":"simple-r-lm","chapter":"4 Simple regression","heading":"4.9.1 Using lm","text":"straightforward way run regression R use lm command. stands “linear model”. uses special symbol, tilde ~, express relationship endogenous variable exogenous variable. endogenous (response) variable always goes left, tilde. exogenous (predictor) variable goes right, tilde. Finally, data argument tell lm find variables model.two numbers slope \\(b\\)?haven’t talked intercept yet, according output, ? (Hint: ’s literally \\(1.117 \\times 10^{-14}\\). number really mean?)Run lm command, time using standardized variables airquality_std tibble. value slope surprise . Explain .","code":"\nlm(Y ~ X, data = airquality_mc)## \n## Call:\n## lm(formula = Y ~ X, data = airquality_mc)\n## \n## Coefficients:\n## (Intercept)            X  \n##   1.117e-14   -1.230e+00"},{"path":"simple.html","id":"simple-r-lavaan","chapter":"4 Simple regression","heading":"4.9.2 Using lavaan","text":"also introduce briefly lavaan package. ’s totally overkill simple regression, getting used syntax now make easier continue build confidence using tool use.lavaan model built similar way lm using tilde ~ notation. One big difference model needs specified inside quotation marks first assigned name like :pass model text sem function lavaan:model now stored TEMP_WIND_fit. One way learn model use parameterEstimates function.lot output , ’re going talk now. Focus “est” column.recognize three estimates. Explain numbers represent.particular, pay close attention second line. hasty, may think variance \\(Y\\), correct.can also produce standardized estimates., explain three numbers. (now listed column called est.std “standardized estimates”.)Verify second line actually error variance. (Hint: remember \\(1 - r_{XY}^{2}\\).) know ’s standardized variance \\(Y\\)? (words, actually know standardized variance \\(Y\\)?)","code":"\nTEMP_WIND_model <- \"Y ~ X\" \nTEMP_WIND_fit <- sem(TEMP_WIND_model, data = airquality_mc)\nparameterEstimates(TEMP_WIND_fit)##   lhs op rhs    est    se      z pvalue ci.lower ci.upper\n## 1   Y  ~   X -1.230 0.193 -6.373      0   -1.609   -0.852\n## 2   Y ~~   Y 70.337 8.042  8.746      0   54.575   86.098\n## 3   X ~~   X 12.330 0.000     NA     NA   12.330   12.330\nstandardizedSolution(TEMP_WIND_fit)##   lhs op rhs est.std    se      z pvalue ci.lower ci.upper\n## 1   Y  ~   X  -0.458 0.060 -7.577      0   -0.576   -0.340\n## 2   Y ~~   Y   0.790 0.055 14.273      0    0.682    0.899\n## 3   X ~~   X   1.000 0.000     NA     NA    1.000    1.000"},{"path":"simple.html","id":"simple-intercepts","chapter":"4 Simple regression","heading":"4.10 What about intercepts?","text":"familiar regression another course, may wondering intercepts went. mean-centered /standardized data, intercepts. regression line always passes \\((0, 0)\\) mean-centered standardized data.[PUT REFERENCE DECIDE COVER MEAN STRUCTURE FUTURE CHAPTER.]","code":""},{"path":"multiple.html","id":"multiple","chapter":"5 Multiple regression","heading":"5 Multiple regression","text":"","code":""},{"path":"multiple.html","id":"preliminaries-1","chapter":"5 Multiple regression","heading":"Preliminaries","text":"load tidyverse package lavaan.","code":"\nlibrary(tidyverse)\nlibrary(lavaan)"},{"path":"multiple.html","id":"multiple-model","chapter":"5 Multiple regression","heading":"5.1 The multiple regression model","text":"chapter extension ideas established last chapter. Even seen regression reading book, sure read study last chapter chapter thoroughly. nothing else, need comfortable notation terminology established . also take special care motivate justify calculations taken granted treatments regression. framework important move mediation path analysis next chapters. comfortable content chapter, won’t much “new” say mediation path analysis generally.Multiple regression like simple regression, exogenous variables. still one endogenous variable. Although archetype illustrated beginning chapter three predictor variables, start two predictor variables keep things simple. understand happens two variables, ’s fairly straightforward generalize knowledge three predictors. logic .multiple regression model two predictors paths given parameter labels:many free parameters appear model?many fixed parameters appear model?equation describing relationship among variables can written either\\[\n\\hat{Y} = b_{1}X_{1} + b_{2}X_{2}\n\\]\\[\nY = b_{1}X_{1} + b_{2}X_{2} + E\n\\]use \\(\\hat{Y}\\) first equation \\(Y\\) second equation?Although ’ll work details two predictors, multiple regression model \\(k\\) predictors look like\\[\n\\hat{Y} = b_{1}X_{1} + b_{2}X_{2} + \\dots + b_{k}X_{k}\n\\]\\[\nY = b_{1}X_{1} + b_{2}X_{2}  + \\dots + b_{k}X_{k} + E\n\\]","code":""},{"path":"multiple.html","id":"multiple-assumptions","chapter":"5 Multiple regression","heading":"5.2 Multiple regression assumptions","text":"Fortunately, assumptions multiple regression basically simple regression minor modifications one addition:data come “good” sample.relationship \\(X_{1}, \\dots, X_{k}\\), \\(Y\\) approximately linear.residuals independent \\(X_{1}, \\dots, X_{k}\\) values.influential outliers.exogenous variables highly correlated one another.discuss briefly:Nothing changed . Good analysis starts good data collection practices.\\(Y\\) \\(X\\), regression model line. \\(Y\\) \\(X_{1}\\) \\(X_{2}\\), regression model plane (2-dimensional plane sitting 3-dimensional space) little challenging graph. predictors, regression model lives even higher dimensions ’s impossible visualize. check condition, best can usually check scatterplots \\(Y\\) \\(X_{}\\) individually approximately linear.Nothing changes .Nothing changes .new condition. two predictors variables highly correlated , induces condition called multicollinearity.illustrate multicollinearity problem, think two-variable case:\\[\n\\hat{Y} = b_{1}X_{1} + b_{2}X_{2}\n\\]\ngeneral, able compute values \\(b_{1}\\) \\(b_{2}\\) best fit model data.now suppose \\(X_{2}\\) just multiple \\(X_{1}\\), say \\(X_{2} = 2X_{1}\\). Now equation looks like\\[\\begin{align}\n\\hat{Y} &= b_{1}X_{1} + b_{2}X_{2} \\\\\n        &= b_{1}X_{1} + b_{2}(2 X_{1}) \\\\\n        &= (b_{1} + 2b_{2})X_{1}\n\\end{align}\\]even though “looked like” two distinct predictors variables, just simple regression disguise. Okay, now let’s suppose try calculate slope simple regression. Say ’s 10. values \\(b_{1}\\) \\(b_{2}\\)? words, values \\(b_{1}\\) \\(b_{2}\\) solve following equation?\\[\nb_{1} + 2b_{2} = 10\n\\]Explain impossible pin unique values \\(b_{1}\\) \\(b_{2}\\) make equation true.choose large, negative value \\(b_{1}\\), imply value \\(b_{2}\\)?choose large, positive value \\(b_{1}\\), imply value \\(b_{2}\\)?Multicollinearity works lot like . Even variables exact multiples , sets highly correlated variables result equations large range possible values consistent data. Even dangerously, fitting algorithm may estimate values coefficients, numbers likely meaningless. completely different set numbers may also perfectly consistent data.clear, ’s problem covariance among predictors. expect . problem arises two predictors highly correlated .","code":""},{"path":"multiple.html","id":"multiple-calculating","chapter":"5 Multiple regression","heading":"5.3 Calculating regression parameters","text":"nothing new , calculations start get little messy. Everything follows two predictors . calculations three predictors. gets hand pretty quickly.First, let’s remember ’re trying . data, can calculate sample covariance matrix. variances covariances among observed variables:\\[\n\\begin{bmatrix}\nVar(X_{1})  &   Cov(X_{1}, X_{2})   &   Cov(X_{1}, Y) \\\\\n\\bullet     &   Var(X_{2})          &   Cov(X_{2}, Y) \\\\\n\\bullet     &   \\bullet             &   Var(Y)\n\\end{bmatrix}\n\\]Remember entries just numbers calculate directly data.get started model-implied matrix, let’s extend Rule 12 little.three variables \\(X_{1}\\), \\(X_{2}\\), \\(X_{3}\\):\\[\\begin{align}\nVar(aX_{1} + bX_{2} + cX_{3}) &=\n    ^2Var(X_{1}) + b^2Var(X_{2}) + c^2Var(X_{3}) \\\\\n    & \\quad + 2abCov(X_{1}, X_{2}) \\\\\n    & \\quad + 2acCov(X_{1}, X_{3}) \\\\\n    & \\quad + 2bcCov(X_{2}, X_{3})\n\\end{align}\\]can extended number variables. variance appears coefficient squared pair variables gets covariance term 2 times product corresponding variable coefficients. (’s hard describe words, ’s still trouble ’s worth writing formal mathematical notation. Hopefully can see pattern coefficients generalizes.)Now can compute, example, \\(Var(Y)\\):\\[\\begin{align}\nVar(Y)  &= Var(b_{1}X_{1} + b_{2}X_{2} + E) \\\\\n    &= b_{1}^{2} Var(X_{1}) + b_{2}^{2} Var(X_{2}) + Var(E) \\\\\n    & \\quad + 2b_{1}b_{2} Cov(X_{1}, X_{2}) \\\\\n    & \\quad + 2b_{1} Cov(X_{1}, E) \\\\\n    & \\quad + 2b_{2} Cov(X_{2}, E)\n\\end{align}\\]happens last two lines ? ?Therefore,\\[\nVar(Y) = b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e\n\\]Rule 8 Rule 9 work way, ’s even easier apply. Just split covariance many pieces terms split.turn.Calculate \\(Cov(X_{1}, Y)\\). get\\[\nb_{1} v_{1} + b_{2} c_{12}\n\\]\nCalculate \\(Cov(X_{2}, Y)\\). get\\[\nb_{2} v_{2} + b_{1} c_{12}\n\\]turns computation need write model-implied matrix.first three entries easy just parameters \\(v_{1}\\), \\(c_{12}\\), \\(v_{2}\\). last column contains entries just calculated .Therefore, model-implied matrix \\[\n\\begin{bmatrix}\nv_{1}   &    c_{12}  &   b_{1} v_{1} + b_{2} c_{12} \\\\\n\\bullet &    v_{2}   &   b_{2} v_{2} + b_{1} c_{12} \\\\\n\\bullet &    \\bullet &   b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e\n\\end{bmatrix}\n\\]\nset expressions equal numbers sample covariance matrix, theory solve unknown parameters model-implied matrix . Three basically already done since can just read \\(v_{1}\\), \\(c_{12}\\), \\(v_{2}\\). solving \\(b_{1}\\), \\(b_{2}\\), \\(e\\) joke! even , resulting expressions particularly enlightening. quite happy turning computational details computer.","code":""},{"path":"multiple.html","id":"multiple-interpreting","chapter":"5 Multiple regression","heading":"5.4 Interpreting the coefficients","text":"Without explicit mathematical expressions parameters, ’s bit challenging explain interpretation. now, ’ll take faith following true:multiple regression model, \\(b_{}\\) represents slope linear association \\(X_{}\\) \\(Y\\) holding value predictors constant.mean?Let’s work concrete example. Suppose think college GPA can predicted using high school GPA along number hours per week spent studying college. model might look like:high school GPA hours per week studying correlated (likely ), influence , influence danger “corrupting” estimates path coefficients. example, \\(b_{2}\\) positive, suggest hours spend studying associated predicted increases college GPA. know ’s really due studying? Maybe students well high school just “smarter”. Sure, also put hours studying, maybe doesn’t matter. Maybe students just well college even didn’t study whole lot. case, coefficient \\(b_{2}\\) positive just set students (happen study , even though doesn’t matter) also ones high college GPAs.’s important control variables. means need temporarily fix value variables make comparison fair. example, look students 3.0 high school. Among students, variability number hours study college. variability associated variability college GPA, know hours spent studying least partly “responsible” explaining change. (lots factors , swept error variance.) high school GPA can’t explain fixed 3.0, ’re comparing apples apples. Students got 2.0 high school may poorly overall, relative increase GPA due studying .parameter \\(b_{2}\\) estimated 0.13, suggests additional hour study time per week predicts increase 0.13 points college GPA, holding high school GPA constant. means increase 0.13 predicted among groups students high school GPA.Let’s say \\(b_{1}\\) estimated 1.2. means college GPA predicted increase 1.2 points every point increase high school GPA. multiple regression model, coefficient can interpreting holding HOURS constant. means coefficient holds among groups students put number hours studying. takes hours studying explanation accounts changes high school GPA.","code":""},{"path":"multiple.html","id":"multiple-standardized","chapter":"5 Multiple regression","heading":"5.5 Regression with standardized variables","text":"Things get little easier (although completely trivial) standardized variables.First, notational simplification. correlations variables—according convention—called \\(r_{X_{1}X_{2}}\\), \\(r_{X_{1}Y}\\), \\(r_{X_{2}Y}\\). little hard look complex expressions, replace \\(r_{12}\\), \\(r_{1Y}\\), \\(r_{2Y}\\).Let’s look sample covariance matrix model-implied matrix standardized variables:\\[\n\\begin{bmatrix}\n1           &   r_{12}      &   r_{1Y} \\\\\n\\bullet     &   1           &   r_{2Y} \\\\\n\\bullet     &   \\bullet     &   1\n\\end{bmatrix} =\n\\begin{bmatrix}\nv_{1}   &    c_{12}  &   b_{1} v_{1} + b_{2} c_{12} \\\\\n\\bullet &    v_{2}   &   b_{2} v_{2} + b_{1} c_{12} \\\\\n\\bullet &    \\bullet &   b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e\n\\end{bmatrix}\n\\]\nentry upper-right corner yields\\[\nr_{1Y} = b_{1} v_{1} + b_{2} c_{12}\n\\]\nsimplifies \n\\[\nr_{1Y} = b_{1} + b_{2} r_{12}\n\\]\nnext entry yields\\[\nr_{2Y} = b_{2} v_{2} + b_{1} c_{12}\n\\]\nsimplifies \n\\[\nr_{2Y} = b_{2} + b_{1} r_{12}\n\\]two equations can solved two unknown parameters \\(b_{1}\\) \\(b_{2}\\).feeling brave? algebra skills sharp? Totally optional, see can derive final answers :\\[\nb_{1} = \\frac{r_{1Y} - r_{2Y}r_{12}}{1 - r_{12}^{2}}\n\\]\\[\nb_{2} = \\frac{r_{2Y} - r_{1Y}r_{12}}{1 - r_{12}^{2}}\n\\]still pretty gross, intuitive content . Look numerator fraction \\(b_{1}\\). Essentially, just \\(r_{1Y}\\) extra stuff. simple regression, expect slope \\(b_{1}\\) simply correlation \\(X_{1}\\) \\(Y\\). multiple regression, also control contribution model coming \\(X_{2}\\). ? subtracting contribution, turns \\(r_{2Y}r_{12}\\). latter term appear way ? need control effect \\(X_{2}\\) \\(X_{2}\\) providing “information” regression model \\(X_{1}\\). Therefore, don’t need subtract \\(r_{2Y}\\) control \\(X_{2}\\), just fraction \\(r_{2Y}\\). just need part \\(X_{2}\\) common \\(X_{1}\\). don’t want “double-count” contribution model common \\(X_{1}\\) \\(X_{2}\\).’s another way think . \\(X_{1}\\) \\(X_{2}\\) independent? Calculate \\(b_{1}\\) \\(b_{2}\\) formulas much easier case. (Don’t overthink . \\(r_{12}\\) case?)\\(X_{1}\\) \\(X_{2}\\) independent, offer unique contribution predicting \\(Y\\) model. contribution just correlation \\(Y\\) (\\(r_{1Y}\\) \\(r_{2Y}\\), respectively). overlap. \\(X_{1}\\) \\(X_{2}\\) correlated, “influence” counted twice. subtract influence \\(b_{1}\\) \\(b_{2}\\) measuring “pure” contribution \\(X_{1}\\) \\(X_{2}\\), controlling one.\\(1 - r_{12}^{2}\\) denominator? ’s less good intuitive explanation . ’s —mathematically speaking—. rescales slope coefficients make everything work way .final equation one \\(Var(Y)\\) lower-right corner matrix. says\\[\n1 = b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e\n\\]\nsimplifies \n\\[\n1 = b_{1}^{2} + b_{2}^{2} + 2b_{1}b_{2} r_{12} + e\n\\]\nRearranging solve \\(e\\),\n\\[\ne = 1 - \\left(b_{1}^{2} + b_{2}^{2} + 2b_{1}b_{2} r_{12}\\right)\n\\]\nenlightening way replace \\(b_{1}\\) \\(b_{2}\\) earlier fractions. can leave \\(e\\) like .Since variance \\(Y\\) 1, stuff inside parentheses represents variance explained model. (subtracted 1, , left \\(e\\), error variance.) analogous \\(R^{2}\\) term described last chapter.makes conceptual sense . pieces \\(\\left(b_{1}^{2} + b_{2}^{2} + 2b_{1}b_{2} r_{12}\\right)\\) correspond various pieces model. first two relate direct effects \\(X_{1}\\) \\(X_{2}\\) third piece relates “indirect” effect shared .","code":""},{"path":"multiple.html","id":"multiple-r","chapter":"5 Multiple regression","heading":"5.6 Multiple regression in R","text":"Let’s fit multiple regression model data music. data sample 10,000 songs Million Song Dataset, collection metrics audio million contemporary popular music tracks.data set downloaded CORGIS Dataset Project information variables data set can found .endogenous variable interest us measure song’s popularity, called song.hotttnesss (scale 0 1).16 many possible exogenous predictors, let’s focus three:artist.hotttnesss\npopularity artist (scale 0 1).\npopularity artist (scale 0 1).song.loudness\nclear website exactly, appears kind average dBFS (decibels relative full scale). Numbers close zero actually loud recordings reasonably go increasingly negative numbers represent softer volumes.\nclear website exactly, appears kind average dBFS (decibels relative full scale). Numbers close zero actually loud recordings reasonably go increasingly negative numbers represent softer volumes.song.tempo\nmeasured beats per minute (BPM).\nmeasured beats per minute (BPM).","code":"\nmusic <- read_csv(\"https://raw.githubusercontent.com/VectorPosse/sem_book/main/data/music.csv\")## Rows: 10000 Columns: 35\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (4): artist.id, artist.name, artist.terms, song.id\n## dbl (31): artist.familiarity, artist.hotttnesss, artist.latitude, artist.loc...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"multiple.html","id":"multiple-r-lm","chapter":"5 Multiple regression","heading":"5.6.1 Using lm","text":"lm model specification minor extension learned simple regression. Just use plus signs right side tilde ~ add predictors.didn’t go trouble mean-centering data time, intercept longer 0. attempt interpret intercept anyway. three coefficients \\(b_{1}\\), \\(b_{2}\\) \\(b_{3}\\), path coefficients model. interpreted follows:\\(b_{1}\\):\nSong popularity predicted increase 1.157 points every point increase artist popularity.\nSong popularity predicted increase 1.157 points every point increase artist popularity.mathematically true, ’s kind nonsensical report using numbers magnitude. scales go 0 1, increase 1 point measuring difference artist 0 popularity (least popular artists data set) artist 1 popularity (popular artists data set).better way report scale everything factor 10:Song popularity predicted increase 0.1157 points every 0.1 increase artist popularity.Song popularity predicted increase 0.1157 points every 0.1 increase artist popularity.\\(b_{2}\\):\nSong popularity predicted increase 0.01187 points every increase 1 dB loudness.\n\\(b_{2}\\):Song popularity predicted increase 0.01187 points every increase 1 dB loudness.increase 1 dB much, , can scale result make meaningful. time ’ll multiply factor 10:Song popularity predicted increase 0.1187 points every increase 10 dB loudness.Song popularity predicted increase 0.1187 points every increase 10 dB loudness.\\(b_{3}\\):\nSong popularity predicted increase 0.00006415 points every increase 1 BPM tempo.\n\\(b_{3}\\):Song popularity predicted increase 0.00006415 points every increase 1 BPM tempo.Restate interpretation \\(b_{3}\\) scale makes sense. ’re familiar BPM, Google get sense reasonably jump tempo might .","code":"\nlm(song.hotttnesss ~ artist.hotttnesss + song.loudness + \n       song.tempo,\n   data = music)## \n## Call:\n## lm(formula = song.hotttnesss ~ artist.hotttnesss + song.loudness + \n##     song.tempo, data = music)\n## \n## Coefficients:\n##       (Intercept)  artist.hotttnesss      song.loudness         song.tempo  \n##        -5.708e-01          1.157e+00          1.187e-02          6.415e-05"},{"path":"multiple.html","id":"multiple-r-lavaan","chapter":"5 Multiple regression","heading":"5.6.2 Using lavaan","text":"Model specification lavaan happens separate step.model fit sem function.unstandardized parameter estimates:Focus estimate column (est).recognize values lines 1 3?line 4 mean? (Hint: ’s variance song.hotttnesss even though notation makes look like .)’s going lines 5 10?standardized parameter estimates:Focus estimates (est.std).easier compare values lines 1 though 3 output unstandardized table? (Hint: think units measurement lack thereof.)value line 4 tell ? (Hint: ’s awfully close 1.)lines 5, 8, 10 equal 1?interpret lines 6, 7, 9?final model variables labeled unstandardized parameter estimates identified:thing, standardized parameter estimates:","code":"\nSONG_model <- \"song.hotttnesss ~ artist.hotttnesss +\n    song.loudness + \n    song.tempo\" \nSONG_fit <- sem(SONG_model, data = music)\nparameterEstimates(SONG_fit)##                  lhs op               rhs      est    se      z pvalue ci.lower\n## 1    song.hotttnesss  ~ artist.hotttnesss    1.157 0.047 24.546  0.000    1.064\n## 2    song.hotttnesss  ~     song.loudness    0.012 0.001  9.333  0.000    0.009\n## 3    song.hotttnesss  ~        song.tempo    0.000 0.000  0.334  0.738    0.000\n## 4    song.hotttnesss ~~   song.hotttnesss    0.442 0.006 70.711  0.000    0.430\n## 5  artist.hotttnesss ~~ artist.hotttnesss    0.021 0.000     NA     NA    0.021\n## 6  artist.hotttnesss ~~     song.loudness    0.145 0.000     NA     NA    0.145\n## 7  artist.hotttnesss ~~        song.tempo    0.255 0.000     NA     NA    0.255\n## 8      song.loudness ~~     song.loudness   29.154 0.000     NA     NA   29.154\n## 9      song.loudness ~~        song.tempo   33.970 0.000     NA     NA   33.970\n## 10        song.tempo ~~        song.tempo 1239.250 0.000     NA     NA 1239.250\n##    ci.upper\n## 1     1.249\n## 2     0.014\n## 3     0.000\n## 4     0.454\n## 5     0.021\n## 6     0.145\n## 7     0.255\n## 8    29.154\n## 9    33.970\n## 10 1239.250\nstandardizedSolution(SONG_fit)##                  lhs op               rhs est.std    se       z pvalue ci.lower\n## 1    song.hotttnesss  ~ artist.hotttnesss   0.240 0.009  25.612  0.000    0.222\n## 2    song.hotttnesss  ~     song.loudness   0.093 0.010   9.388  0.000    0.073\n## 3    song.hotttnesss  ~        song.tempo   0.003 0.010   0.334  0.738   -0.016\n## 4    song.hotttnesss ~~   song.hotttnesss   0.925 0.005 186.180  0.000    0.915\n## 5  artist.hotttnesss ~~ artist.hotttnesss   1.000 0.000      NA     NA    1.000\n## 6  artist.hotttnesss ~~     song.loudness   0.187 0.000      NA     NA    0.187\n## 7  artist.hotttnesss ~~        song.tempo   0.050 0.000      NA     NA    0.050\n## 8      song.loudness ~~     song.loudness   1.000 0.000      NA     NA    1.000\n## 9      song.loudness ~~        song.tempo   0.179 0.000      NA     NA    0.179\n## 10        song.tempo ~~        song.tempo   1.000 0.000      NA     NA    1.000\n##    ci.upper\n## 1     0.259\n## 2     0.112\n## 3     0.022\n## 4     0.935\n## 5     1.000\n## 6     0.187\n## 7     0.050\n## 8     1.000\n## 9     0.179\n## 10    1.000"},{"path":"mediation.html","id":"mediation","chapter":"6 Mediation","heading":"6 Mediation","text":"","code":""},{"path":"path.html","id":"path","chapter":"7 Path analysis","heading":"7 Path analysis","text":"","code":""},{"path":"latent.html","id":"latent","chapter":"8 Latent variables","heading":"8 Latent variables","text":"","code":""},{"path":"cfa.html","id":"cfa","chapter":"9 Confirmatory factor analysis","heading":"9 Confirmatory factor analysis","text":"","code":""},{"path":"sem.html","id":"sem","chapter":"10 Structural equation models","heading":"10 Structural equation models","text":"","code":""},{"path":"scm.html","id":"scm","chapter":"11 Structural causal models","heading":"11 Structural causal models","text":"","code":""},{"path":"appendix-rules.html","id":"appendix-rules","chapter":"A Variance/covariance rules","heading":"A Variance/covariance rules","text":"Rule 1If \\(C\\) constant, \\[\nVar\\left(C\\right) = 0\n\\]Rule 2If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} + X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]Consequence Rule 1 Rule 2:\\[\nVar\\left(X + C\\right) = Var\\left(X\\right)\n\\]Rule 3If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} - X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]Rule 4If \\(\\) number,\\[\nVar\\left(aX\\right) = ^2 Var\\left(X\\right)\n\\]Rule 5\\[\nCov(X, X) = Var(X)\n\\]Rule 6\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{2}, X_{1}\\right)\n\\]Rule 7If \\(C\\) constant, \\[\nCov\\left(X, C\\right) = 0\n\\]Rule 8\\[\nCov\\left(X_{1} + X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) + Cov\\left(X_{2}, X_{3}\\right)  \n\\]Rule 9\\[\nCov\\left(X_{1} - X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) - Cov\\left(X_{2}, X_{3}\\right)  \n\\]Consequence Rule 6, Rule 8, Rule 9:\\[\nCov\\left(X_{1}, X_{2} \\pm X_{3}\\right) = Cov\\left(X_{1}, X_{2}\\right) \\pm Cov\\left(X_{1}, X_{3}\\right)  \n\\]Rule 10If \\(\\) number,\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{1}, X_{2}\\right) =  Cov\\left(X_{1}, X_{2}\\right)  \n\\]Rule 11If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nCov\\left(X_{1}, X_{2}\\right) = 0\n\\]Rule 12For two variables \\(X_{1}\\) \\(X_{2}\\):\\[\nVar(aX_{1} + bX_{2}) =\n    ^2Var(X_{1}) + b^2Var(X_{2}) + 2abCov(X_{1}, X_{2})\n\\]three variables \\(X_{1}\\), \\(X_{2}\\), \\(X_{3}\\):\\[\\begin{align}\nVar(aX_{1} + bX_{2} + cX_{3}) &=\n    ^2Var(X_{1}) + b^2Var(X_{2}) + c^2Var(X_{3}) \\\\\n    & \\quad + 2abCov(X_{1}, X_{2}) \\\\\n    & \\quad + 2acCov(X_{1}, X_{3}) \\\\\n    & \\quad + 2bcCov(X_{2}, X_{3})\n\\end{align}\\]can extended number variables. variance appears coefficient squared pair variables gets covariance term 2 times product corresponding variable coefficients.","code":""},{"path":"appendix-lisrel.html","id":"appendix-lisrel","chapter":"B LISREL notation","heading":"B LISREL notation","text":"","code":""}]
