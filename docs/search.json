[{"path":"index.html","id":"intro","chapter":"Introduction","heading":"Introduction","text":"Welcome book structural equation modeling![BOOK WORK PROGRESS. FEEL FREE PERUSE WHATEVER CONTENT FIND , FINAL VERSION READY SOMETIME 2023.]want, can also download book PDF EPUB file. aware print versions missing richer formatting online version.","code":""},{"path":"index.html","id":"intro-history","chapter":"Introduction","heading":"Some history","text":"2016, Jonathan Sean embarked upon bold experiment, asking question, “possible teach structural equation modeling (SEM) undergraduates little statistical background?” make things even exciting, attempted special topics course lasting one month May Term Westminster College (Salt Lake City, UT).endeavor, temper expectations, course. goal produce competent practitioners subsequently go serious research using SEM techniques. quite happy , end May, undergraduates able put together simple final project required find data, posit model, fit model R, interpret output, check model fit statistics. exposure topic appreciation power satisfying enough. fact, think got little : reasonably confident students developed—point right taking course—ability read research article SEM model least idea article talking . called win!repeated experiment modifications materials pedagogy 2018. point, clear finding textbooks articles assign students challenging. great books , mostly aimed graduate students. Even ones labeled “introductory” often far typical undergraduate limited statistical training.decided write textbook fill hole literature. book follows fruit efforts.Sean granted sabbatical Spring 2020 proposed use time start writing book preparation running May Term course May, 2020. , well, know went…pandemic subsided enough us offer course person , attempted May, 2022. [CONTINUED]","code":""},{"path":"index.html","id":"intro-philosophy","chapter":"Introduction","heading":"Our philosophy","text":"mentioned , motivation writing book driven difficulty finding readings students. Perhaps begs question, one even try teaching topic “difficult” “advanced” structural equation modeling audience ? sure, traditional approaches already market seem assume lot background disposal. books claim assume less background…well, sometimes require let .1The prerequisite class Westminster College intro stats class covers pretty standard material course: hypothesis testing confidence intervals one two proportions, one two means (paired means), ANOVA, chi-squared, simple linear regression. technical sense, little material truly required understand book. said , though, prior exposure statistical ideas helpful motivating rationale building kinds models teach course.also benefit intro course introduces students R. lacking R background, first five chapters [FIX LINK DATA 220 BOOK ALSO FULLY ONLINE] suffice basic introduction R R Markdown, graphing ggplot, basic tidyverse stuff tibbles data manipulation. try hard give lots fully worked-code examples book students able copy, paste, modify slightly meet modeling needs. know make attempt language agnostic ; R one tool use.respect students, made deliberate choices way book structured.Make book free open source.Students enough trouble lives without subjected extortionate practices textbook publishers. book freely available online, ’s also published permissive open source license (MIT license) allows folks “use, copy, modify, merge, publish, distribute, sublicense, /sell” versions book desired. Furthermore, derivative book must also abide open standards. book libre gratis (, common parlance, “free speech” “free beer”).Start scratch.Explain everything beginning terms simple possible. first chapters may look like review students. Even , course, review gives students confidence tackle upcoming new material. might surprised novel ways explain seemingly familiar concepts. exposition eye toward direct application later chapters, might seem little idiosyncratic first motivated desire smooth pathways later concepts.Incorporate active learning everything.chapters structured work templates classroom experiences. intersperse conceptual explanation activities designed reinforce concepts lead students important conclusions. learning activities appear framed blue boxes [CHANGE ESTABLISH CUSTOM CALLOUT] like :Hey, kids! Stop activity !math well.One common thread see lot SEM books tendency sweep math rug. intention comes good place; mathematics can appear intimidating , therefore, may seem serve deterrent learning. sure, complex mathematical ideas SEM inaccessible audience. time—, fairness, may due Sean’s bias mathematician—truly believe mathematics, carefully explained continually reinforced, can illuminate student understanding. mathy sections may need additional instructor support students without strong math background. takes relatively straightforward algebra nail concepts books ignore. good example investing time rules manipulating variances covariances. allows students calculate “model-implied matrix” cryptically referenced textbooks. However, skip math sometimes. example, lot math behind model fit indices left unexplained. least, hope transparent choices include exclude certain mathematical details.Use “nice” data.Finding data hard, rely lot data sets textbooks R package authors make available (due attribution, course). keep things simple course, work almost exclusively cross-sectional, numerical (quantitative) data. [MODIFY END WORKING BINARY CATEGORICAL EXOGENOUS VARIABLES (CODED 0/1) POINT.]careful diagrams.Learning complex models induces sizable cognitive load. Shortcuts diagrams tend confuse students. example, error terms truly latent variables, drawn circles hidden, even advanced practitioner “knows” ’re . Variances covariances among exogenous variables always appear well. take time build consistent pictographic representation every part model. (chapter introduced archetypal diagram illustrates chapter’s content.) stick representation throughout book.careful notation.may industry standard, LISREL notation needlessly complex undergraduate students. take consistent simple approach notation represents variables using UPPERCASE names parameter values using lowercase names. Abstract variables tend called something like X exogenous Y endogenous. Real-world variables contextually meaningful names. interested reading research literature, included appendix describing LISREL notation.","code":""},{"path":"index.html","id":"intro-structure","chapter":"Introduction","heading":"Course structure","text":"use book teach 2-credit-hour course. (Even though ’s special topics course May Term, number contact hours students equivalent semester-long, 2-credit-hour course.)[ADD INFO DECIDE MUCH REASONABLE COVER. WANT BOOK USABLE 4-CREDIT-HOUR COURSE, ADDITIONAL MATERIAL CONSIDER INCLUDING?]","code":""},{"path":"index.html","id":"intro-onward","chapter":"Introduction","heading":"Onward and upward","text":"hope enjoy textbook. Please send us feedback!–Jonathan Amburgey (jamburgey@westminstercollege.edu)–Sean Raleigh (sraleigh@westminstercollege.edu)","code":""},{"path":"variables.html","id":"variables","chapter":"1 Variables and measurement","heading":"1 Variables and measurement","text":"","code":""},{"path":"variables.html","id":"variables-first-section","chapter":"1 Variables and measurement","heading":"1.1 First section","text":"[SOMEWHERE NEED MENTION “CONSTANT” VARIABLES, VARIABLES TAKE ONE VALUE.]","code":""},{"path":"variance.html","id":"variance","chapter":"2 Variance","heading":"2 Variance","text":"","code":""},{"path":"variance.html","id":"variance-mean","chapter":"2 Variance","heading":"2.1 A quick refresher on the mean","text":"us taught calculate mean variable way back elementary school: add numbers divide size group numbers. statistics context, often use “bar” indicate mean variable; words, variable called \\(X\\), mean denoted \\(\\overline{X}\\). Remembering always use \\(n\\) represent sample size, formula \\[\n\\overline{X} = \\frac{\\sum{X}}{n}\n\\](case forgot, Greek letter Sigma \\(\\Sigma\\) stands “sum” means “add values thing follows”.)small data set ’ll use throughout chapter simple example can work “hand”:3, 4, 5, 6, 6, 7, 8, 9Calculate mean set eight numbers.","code":""},{"path":"variance.html","id":"variance-calculating","chapter":"2 Variance","heading":"2.2 Calculating variance","text":"Variance quantity meant capture information spread data .Let’s build step step.first thing note spread don’t care large small numbers absolute sense. care large small relative .Look numbers earlier exercise:3, 4, 5, 6, 6, 7, 8, 9What following numbers instead?1003, 1004, 1005, 1006, 1006, 1007, 1008, 1009Explain reasonable measure “spread” groups numbers.One way measure large small number relative whole set measure distance number mean.Recall mean following numbers 6:3, 4, 5, 6, 6, 7, 8, 9Create new list eight numbers measures distance numbers mean. words, subtract 6 numbers.numbers new list negative, zero, positive. make sense? words, mean number negative, zero, positive?original set numbers called \\(X\\), ’ve just calculated new list \\(\\left(X - \\overline{X}\\right)\\). Let’s start organizing table:numbers second columns “deviations” mean.One way might measure “spread” look average deviation. , deviations represent distances mean, set large spread large deviations set small spread small deviations.Go ahead take average (mean) numbers second column .Uh, oh! calculated zero. Explain always get zero, matter set numbers start .idea “average deviation” seems like work, clearly doesn’t. fix idea?Hopefully, identified negative deviations problem canceled positive deviations. deviations positive, wouldn’t issue .two ways making numbers positive:Taking absolute valuesWe just take absolute value make values positive. statistical procures just ,2 ’re going take slightly different approach…SquaringIf square value, become positive.Taking absolute value conceptually easier, historical mathematical reasons squaring little better.3Square numbers second column table . calculate new list \\(\\left(X - \\overline{X}\\right)^{2}\\)Putting new numbers previous table:Now take average (mean) numbers third column .number got (3.5) almost call variance. ’s one annoying wrinkle.took mean last column numbers, added divided 8 since 8 numbers list. fairly technical mathematical reasons, actually don’t want divide 8. Instead, divide one less number; words, divide 7.4Re-math , divide 7 instead dividing 8.number found variance, written \\(Var(X)\\). full formula \\[\nVar(X) = \\frac{\\sum{\\left(X - \\overline{X}\\right)^{2}}}{n - 1}\n\\]one-liner, formula may look little intimidating, break step step , ’s bad.full calculation table:diagrams, variance variable indicated curved, double-headed arrow, labeled value variance, like :Using tabular approach, calculate variance following set numbers:4, 3, 7, 2, 9, 4, 6Consider following two sets numbers:1, 2, 5, 8, 91, 2, 5, 8, 91, 4, 5, 6, 91, 4, 5, 6, 9Without calculations, sets larger variance?’ve decided, calculate variance sets check answer.","code":""},{"path":"variance.html","id":"variance-r","chapter":"2 Variance","heading":"2.3 Calculating variance in R","text":"’ve done hand times make sure understand formula works, can let R work us:also easier real-world data highly engineered 😉 produce whole numbers:","code":"\nX1 <- c(3, 4, 5, 6, 6, 7, 8, 9)\nvar(X1)## [1] 4\nX2 <- c(4, 3, 7, 2, 9, 4, 6)\nvar(X2)## [1] 6\nX3 <- c(1, 2, 5, 8, 9)\nvar(X3)## [1] 12.5\nX4 <- c(1, 4, 5, 6, 9)\nvar(X4)## [1] 8.5\nPlantGrowth$weight##  [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n## [16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\nvar(PlantGrowth$weight)## [1] 0.49167"},{"path":"variance.html","id":"variance-rules","chapter":"2 Variance","heading":"2.4 Variance rules","text":"course, need able calculate variance various combinations variables. example, \\(X_{1}\\) \\(X_{2}\\) two variables, can create new variable \\(X_{1} + X_{2}\\) adding values two variables. variance \\(X_{1} + X_{2}\\)?answer , let’s establish first rule.Rule 1Suppose \\(C\\) “constant” variable, meaning always value (rather variable contain lots different numbers). ,\\[\nVar\\left(C\\right) = 0\n\\]Rule 1 true? can either reason conceptually, based understand variance supposed measure, can sample calculation. (Make table starting column contains many copies single number work calculation.)Now, back example beginning section finding variance \\(X_{1} + X_{2}\\).Rule 2If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} + X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]’re going get formal definition independence . now, suffices think intuitive definition may already head means two things independent. idea , independent, \\(X_{1}\\) \\(X_{2}\\) nothing . Knowing value one variable give information values . next chapter, ’ll say rule.’s important note Rule 2 abstract mathematical rule holds theory. actual data, however, know statistics won’t always match theoretical values. example, even true population mean 42, samples drawn population sample means close 42, likely exactly 42.5Let’s test . , define two new variables using random numbers.quick note random numbers first: ask computer give us random numbers, ’s going give us truly random numbers. algorithms designed give us numbers correct statistical properties random numbers without actually random. called pseudo-random numbers. can use fact benefit. set.seed command tells computer start generating numbers specific way. Anyone else using R (version R) gives machine “seed” generate list numbers. makes work “reproducible”: able reproduce results book machine.variable X5 normally distributed mean 1 standard deviation 2. (don’t remember standard deviation intro stats, talk next section.) next variable X6 normally distributed mean 4 standard deviation 3. independent definition X5 depend way definition X6 vice versa.sample sizes (2000) large enough get pretty close theoretically correct results .Use R calculate variances X5 X6 separately. use R add two numbers just obtained (sum two variances). Finally, use R calculate variance sum two variables.’s example help think intuitively.Suppose someone comes along offers give random amount money, number $0 $100.6 variance measure spread, stands reason variance reflects something uncertain much money transaction. average, expect $50, know actual amount money receive can vary greatly.Okay, now second person comes along offers deal, random dollar gift $0 $100.7 end transactions, much money ? average, maybe $100, uncertainty? total amount result two random gifts, even less sure close $100 might . range possible values now $0 $200.8 uncertainty greater overall.course, explains variance sum two variables larger variance either variable individually. fact variance sum two independent variables exactly sum variances shown mathematically. hopefully, intuition clear.next rule consequence first two rules, give special number\\[\nVar\\left(X + C\\right) = Var\\left(X\\right)\n\\]Can apply Rule 2 followed Rule 1 see mathematically \\(Var\\left(X + C\\right) = Var\\left(X\\right)\\)?assumes constant independent variable? Intuitively speaking, true?intuition behind statement \\(Var\\left(X + C\\right) = Var\\left(X\\right)\\)? words, can explain rule someone terms means shifting values data set constant amount?Rule 3 similar Rule 2, ’s quite counter-intuitive:Rule 3If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} - X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]common students think minus sign left translate minus sign right.9What gives?Let’s return example strangers giving money.10 first person still offers random amount $0 $100. , now, second person robber, forces give random dollar value $0 $100 (choosing, course). much money expect two events? average, $0. (first person gives , average, $50, second person takes away, average, $50.) certain amount?Imagine world wrong rule prevailed. \\(Var\\left(X_{1} - X_{2}\\right)\\) truly difference two variances. \\(Var\\left(X_{1}\\right)\\) \\(Var\\left(X_{2}\\right)\\) scenario. (Although one person giving money one taking, uncertainty dollar amount cases.) implies\n\\[\nVar\\left(X_{1}\\right) - Var\\left(X_{2}\\right) = 0\n\\]\nCan true? Zero variance means “spread” means exact certainty value. (Remember Rule 1?) 100% confident end transactions exactly $0? way!fact, amount money end ranges -$100 $100. larger range either transaction individually. uncertainty grown two random processes play, just like scenario two beneficent strangers. fact, width range possibilities scenarios: $0 $200 -$100 $100 span range $200.next rule, unfortunately, great intuitive explanation. make little sense next chapter, ’ll revisit .Rule 4If \\(\\) number,\\[\nVar\\left(aX\\right) = ^2 Var\\left(X\\right)\n\\]go back table, imagine multiplying every number first column \\(\\). Every number second column still factor \\(\\). square values, every number third column factor \\(^{2}\\). ’s gist rule anyway. , , ’s much intuition makes sense.can, least, check empirically rule works.’ll use \\(X_{5}\\) defined , normally distributed variable mean 1 standard deviation 2. variance data 4:Let’s use \\(= 3\\).R, calculate \\(Var\\left(3X_{5}\\right)\\). (Don’t forget R, can’t just type 3 X5. explicitly include multiplication sign: 3 * X5.)Now try calculating \\(3 Var\\left(X_{5}\\right)\\). ’ll see don’t get right answer.now try \\(9 Var\\left(X_{5}\\right)\\). work.’s variance rules ’ll need!","code":"\nset.seed(10101)\nX5 <- rnorm(2000, mean = 1, sd = 2)\nX6 <- rnorm(2000, mean = 4, sd = 3)\nhead(X5)## [1] -0.7535339 -0.4927789  3.7518296  1.4751639  1.2172549  3.4054426\nhead(X6)## [1] 2.297279 4.856377 6.661822 1.309892 2.270882 3.827944\nvar(X5)## [1] 4.15763"},{"path":"variance.html","id":"variance-sd","chapter":"2 Variance","heading":"2.5 Standard deviation","text":"variance nice obeys rules. one big downside ’s interpretable.example, think scenario people giving/taking money. case, values measures units dollars.\\(X\\) measured dollars, units measurement \\(\\overline{X}\\)? seems sensible, right?units \\(\\left(X - \\overline{X}\\right)\\)? Still sensible, right? (’s problem values positive negative. Negative dollars still make sense. Just think student loans.)Okay, now ’s things get weird. units \\(\\left(X - \\overline{X}\\right)^{2}\\)? longer makes sense.Variance nearly average bunch squared deviations, variable measured dollars, units variance “squared dollars”, whatever .Variances really interpretable directly. make interpretable? Well, variance “squared” units, can take square root get back natural units started .called standard deviation, \\(SD(X)\\).\\[\nSD(X) = \\sqrt{\\frac{\\sum{\\left(X - \\overline{X}\\right)^{2}}}\n{n - 1}}\n\\], said simply,\\[\nSD(X) = \\sqrt{Var(X)}\n\\]\nEquivalently,\\[\nVar(X) = SD(X)^2\n\\]Often, concise notation required, write \\(s_{X}\\) \\(SD(X)\\).\\[\ns_{X} = \\sqrt{Var(X)}\n\\]\n\\[\nVar(X) = s_{X}^2\n\\]Due interpretability, intro stats class focus far standard deviation variance. downside mathematical rules aren’t nice standard deviations. example, \n\\[\nSD\\left(X_{1} + X_{2}\\right)?\n\\]can work definition see \\[\nSD\\left(X_{1} + X_{2}\\right) = \\sqrt{\nSD\\left(X_{1}\\right)^{2} + SD\\left(X_{2}\\right)^{2}\n}\n\\]\n, eww, ’s gross.constant multiple rule works nice, though.number \\(\\), \\(SD(aX)\\)? Finish following calculation can simplify get back something involving just \\(SD(X)\\):\\[\\begin{align}\nSD(aX)  &= \\sqrt{Var(aX)} \\\\\n        &= \\quad ???\n\\end{align}\\]careful! happens \\(\\) negative number? Standard deviations (like variances) always non-negative.convenient way express fact coefficient always come positive following:\\[\nSD(aX) = \\left| \\right| SD(X)\n\\]SEM, focus almost exclusively variance switch standard deviation two reasons:need communicate something spread meaningful units.need standardize variables. (See Section 2.7 .)Although standard deviance just square root variance, worth knowing R command calculate . ’s just sd. example:can see sd right thing:","code":"\nsd(PlantGrowth$weight)## [1] 0.7011918\nsqrt(var(PlantGrowth$weight))## [1] 0.7011918"},{"path":"variance.html","id":"variance-mean-centering","chapter":"2 Variance","heading":"2.6 Mean centering data","text":"Many statistical techniques taught intro stats course focus learning means variables. Structural equation modeling little different focused explaining variability data—changes one variables predict changes variables.11A habit ’ll start forming now mean center variables. subtracting mean variable values.Let’s use \\(X_{6}\\) defined , normally distributed variable mean 4 standard deviation 3. interpret values \\(X_{6} - \\overline{X_{6}}\\)? (Remember, just second column variance tables earlier.)shift \\(X_{6}\\) values left \\(\\overline{X_{6}}\\) units, mean new list numbers?Let’s verify R. ’ll use “suffix” mc indicate mean-centered variable.answer exactly agree “theoretical” answer came lines ? (don’t already know, e-16 expression scientific notation means “times \\(10^{-16}\\). ’s really small number!)Take guess variance X6_mc. Verify guess R.good news mean centering preserves variance. mean shifted 0, variance change, statistical model build analyzes variance affecting mean-centering.","code":"\nX6_mc <- X6 - mean(X6)\nmean(X6_mc)## [1] 2.851573e-16"},{"path":"variance.html","id":"variance-standardizing","chapter":"2 Variance","heading":"2.7 Standardizing data","text":"’ve mean centered data, can go one step divide standard deviation. results something often called z-score. process converting variables original units z-scores called standardizing data.\\[\nZ = \\frac{\\left(X - \\overline{X}\\right)}{SD(X)}\n\\]happens try standardize variable constant? (Hint: think denominator fraction defining z-score.)useful standardize variables? One reason remove units measurement facilitate comparisons variables. Suppose \\(X\\) represents height inches. numerator (\\(X - \\overline{X}\\)) units inches. standard deviation \\(SD(X)\\) also units inches. divide, units go away z-score left without units, sometimes called “dimensionless quantity”.Suppose female United States 6 feet tall (72 inches). Suppose female China 5’8 tall (68 inches). absolute terms, American woman taller Chinese woman. ’re interested knowing woman taller relative respective population?mean height American woman 65” standard deviation 3.5” mean height Chinese woman 62” standard deviation 2.5”. (numbers aren’t perfectly correct, ’re probably close-ish.)Calculate z-scores women.woman taller relative population?Although z-scores don’t technically units, can think measuring many standard deviations value lies mean.z-score value equals mean?meaning negative z-score?z-score American woman 2. means height measures two standard deviations mean.real-world data, use technology . temperature measurements New York 1974. (daily highs across six-month period.)calculate mean standard deviation:average high 78 degrees Fahrenheit standard deviation 9.5 degrees Fahrenheit.just subtract mean, get mean-centered data.also divide standard deviation, get standardized variable (set z-scores). Note extra parentheses make sure get order operations right. subtract first, divide whole mean-centered quantity standard deviation.easier way R use scale command. (Sorry, output little long. Keep scrolling .)Although outputs formatted little differently, can go back check sets numbers match .mean standardized variable? know ?Let’s calculate variance standardized variable. , ’ll note mean \\(\\overline{X}\\) just number. Also, standard deviation \\(SD(X)\\) just number. make calculation easier understand, let’s just substitute letters easier work :\\(M = \\overline{X}\\)\\(S = SD(X)\\)Remember, \\(M\\) \\(S\\) constants.Now need calculate \\(Var(Z)\\). ’ll first couple steps. take , using variance rules earlier chapter, simplify expression get numerical answer. sure justify step citing rule invoked get .\\[\\begin{align}\nVar(Z) &= Var\\left(\\frac{\\left(X - \\overline{X}\\right)}{SD(X)}\\right) \\\\\n    &= Var\\left(\\frac{\\left(X - M\\right)}{S}\\right) \\\\\n    &= Var\\left(\\frac{1}{S}\\left(X - M\\right)\\right) \\\\\n    &= \\quad ???\n\\end{align}\\]may feel little uncomfortable applying Rule 3 might worry \\(X\\) \\(M\\) independent. Since \\(M\\) mean \\(X\\), seems like dependent \\(X\\). intuition independence breaks rely mathematical rules haven’t really gotten . constants independent variable.get answer 1. standardized variable always variance 1. important fact future chapters.","code":"\nairquality$Temp##   [1] 67 72 74 62 56 66 65 59 61 69 74 69 66 68 58 64 66 57 68 62 59 73 61 61 57\n##  [26] 58 57 67 81 79 76 78 74 67 84 85 79 82 87 90 87 93 92 82 80 79 77 72 65 73\n##  [51] 76 77 76 76 76 75 78 73 80 77 83 84 85 81 84 83 83 88 92 92 89 82 73 81 91\n##  [76] 80 81 82 84 87 85 74 81 82 86 85 82 86 88 86 83 81 81 81 82 86 85 87 89 90\n## [101] 90 92 86 86 82 80 79 77 79 76 78 78 77 72 75 79 81 86 88 97 94 96 94 91 92\n## [126] 93 93 87 84 80 78 75 73 81 76 77 71 71 78 67 76 68 82 64 71 81 69 63 70 77\n## [151] 75 76 68\nmean(airquality$Temp)## [1] 77.88235\nsd(airquality$Temp)## [1] 9.46527\nairquality$Temp - mean(airquality$Temp)##   [1] -10.8823529  -5.8823529  -3.8823529 -15.8823529 -21.8823529 -11.8823529\n##   [7] -12.8823529 -18.8823529 -16.8823529  -8.8823529  -3.8823529  -8.8823529\n##  [13] -11.8823529  -9.8823529 -19.8823529 -13.8823529 -11.8823529 -20.8823529\n##  [19]  -9.8823529 -15.8823529 -18.8823529  -4.8823529 -16.8823529 -16.8823529\n##  [25] -20.8823529 -19.8823529 -20.8823529 -10.8823529   3.1176471   1.1176471\n##  [31]  -1.8823529   0.1176471  -3.8823529 -10.8823529   6.1176471   7.1176471\n##  [37]   1.1176471   4.1176471   9.1176471  12.1176471   9.1176471  15.1176471\n##  [43]  14.1176471   4.1176471   2.1176471   1.1176471  -0.8823529  -5.8823529\n##  [49] -12.8823529  -4.8823529  -1.8823529  -0.8823529  -1.8823529  -1.8823529\n##  [55]  -1.8823529  -2.8823529   0.1176471  -4.8823529   2.1176471  -0.8823529\n##  [61]   5.1176471   6.1176471   7.1176471   3.1176471   6.1176471   5.1176471\n##  [67]   5.1176471  10.1176471  14.1176471  14.1176471  11.1176471   4.1176471\n##  [73]  -4.8823529   3.1176471  13.1176471   2.1176471   3.1176471   4.1176471\n##  [79]   6.1176471   9.1176471   7.1176471  -3.8823529   3.1176471   4.1176471\n##  [85]   8.1176471   7.1176471   4.1176471   8.1176471  10.1176471   8.1176471\n##  [91]   5.1176471   3.1176471   3.1176471   3.1176471   4.1176471   8.1176471\n##  [97]   7.1176471   9.1176471  11.1176471  12.1176471  12.1176471  14.1176471\n## [103]   8.1176471   8.1176471   4.1176471   2.1176471   1.1176471  -0.8823529\n## [109]   1.1176471  -1.8823529   0.1176471   0.1176471  -0.8823529  -5.8823529\n## [115]  -2.8823529   1.1176471   3.1176471   8.1176471  10.1176471  19.1176471\n## [121]  16.1176471  18.1176471  16.1176471  13.1176471  14.1176471  15.1176471\n## [127]  15.1176471   9.1176471   6.1176471   2.1176471   0.1176471  -2.8823529\n## [133]  -4.8823529   3.1176471  -1.8823529  -0.8823529  -6.8823529  -6.8823529\n## [139]   0.1176471 -10.8823529  -1.8823529  -9.8823529   4.1176471 -13.8823529\n## [145]  -6.8823529   3.1176471  -8.8823529 -14.8823529  -7.8823529  -0.8823529\n## [151]  -2.8823529  -1.8823529  -9.8823529\n(airquality$Temp - mean(airquality$Temp))/sd(airquality$Temp)##   [1] -1.14971398 -0.62146702 -0.41016823 -1.67796094 -2.31185730 -1.25536337\n##   [7] -1.36101276 -1.99490912 -1.78361034 -0.93841519 -0.41016823 -0.93841519\n##  [13] -1.25536337 -1.04406459 -2.10055851 -1.46666216 -1.25536337 -2.20620791\n##  [19] -1.04406459 -1.67796094 -1.99490912 -0.51581762 -1.78361034 -1.78361034\n##  [25] -2.20620791 -2.10055851 -2.20620791 -1.14971398  0.32937752  0.11807873\n##  [31] -0.19886945  0.01242934 -0.41016823 -1.14971398  0.64632570  0.75197509\n##  [37]  0.11807873  0.43502691  0.96327387  1.28022205  0.96327387  1.59717023\n##  [43]  1.49152084  0.43502691  0.22372813  0.11807873 -0.09322005 -0.62146702\n##  [49] -1.36101276 -0.51581762 -0.19886945 -0.09322005 -0.19886945 -0.19886945\n##  [55] -0.19886945 -0.30451884  0.01242934 -0.51581762  0.22372813 -0.09322005\n##  [61]  0.54067630  0.64632570  0.75197509  0.32937752  0.64632570  0.54067630\n##  [67]  0.54067630  1.06892327  1.49152084  1.49152084  1.17457266  0.43502691\n##  [73] -0.51581762  0.32937752  1.38587145  0.22372813  0.32937752  0.43502691\n##  [79]  0.64632570  0.96327387  0.75197509 -0.41016823  0.32937752  0.43502691\n##  [85]  0.85762448  0.75197509  0.43502691  0.85762448  1.06892327  0.85762448\n##  [91]  0.54067630  0.32937752  0.32937752  0.32937752  0.43502691  0.85762448\n##  [97]  0.75197509  0.96327387  1.17457266  1.28022205  1.28022205  1.49152084\n## [103]  0.85762448  0.85762448  0.43502691  0.22372813  0.11807873 -0.09322005\n## [109]  0.11807873 -0.19886945  0.01242934  0.01242934 -0.09322005 -0.62146702\n## [115] -0.30451884  0.11807873  0.32937752  0.85762448  1.06892327  2.01976780\n## [121]  1.70281962  1.91411841  1.70281962  1.38587145  1.49152084  1.59717023\n## [127]  1.59717023  0.96327387  0.64632570  0.22372813  0.01242934 -0.30451884\n## [133] -0.51581762  0.32937752 -0.19886945 -0.09322005 -0.72711641 -0.72711641\n## [139]  0.01242934 -1.14971398 -0.19886945 -1.04406459  0.43502691 -1.46666216\n## [145] -0.72711641  0.32937752 -0.93841519 -1.57231155 -0.83276580 -0.09322005\n## [151] -0.30451884 -0.19886945 -1.04406459\nscale(airquality$Temp)##               [,1]\n##   [1,] -1.14971398\n##   [2,] -0.62146702\n##   [3,] -0.41016823\n##   [4,] -1.67796094\n##   [5,] -2.31185730\n##   [6,] -1.25536337\n##   [7,] -1.36101276\n##   [8,] -1.99490912\n##   [9,] -1.78361034\n##  [10,] -0.93841519\n##  [11,] -0.41016823\n##  [12,] -0.93841519\n##  [13,] -1.25536337\n##  [14,] -1.04406459\n##  [15,] -2.10055851\n##  [16,] -1.46666216\n##  [17,] -1.25536337\n##  [18,] -2.20620791\n##  [19,] -1.04406459\n##  [20,] -1.67796094\n##  [21,] -1.99490912\n##  [22,] -0.51581762\n##  [23,] -1.78361034\n##  [24,] -1.78361034\n##  [25,] -2.20620791\n##  [26,] -2.10055851\n##  [27,] -2.20620791\n##  [28,] -1.14971398\n##  [29,]  0.32937752\n##  [30,]  0.11807873\n##  [31,] -0.19886945\n##  [32,]  0.01242934\n##  [33,] -0.41016823\n##  [34,] -1.14971398\n##  [35,]  0.64632570\n##  [36,]  0.75197509\n##  [37,]  0.11807873\n##  [38,]  0.43502691\n##  [39,]  0.96327387\n##  [40,]  1.28022205\n##  [41,]  0.96327387\n##  [42,]  1.59717023\n##  [43,]  1.49152084\n##  [44,]  0.43502691\n##  [45,]  0.22372813\n##  [46,]  0.11807873\n##  [47,] -0.09322005\n##  [48,] -0.62146702\n##  [49,] -1.36101276\n##  [50,] -0.51581762\n##  [51,] -0.19886945\n##  [52,] -0.09322005\n##  [53,] -0.19886945\n##  [54,] -0.19886945\n##  [55,] -0.19886945\n##  [56,] -0.30451884\n##  [57,]  0.01242934\n##  [58,] -0.51581762\n##  [59,]  0.22372813\n##  [60,] -0.09322005\n##  [61,]  0.54067630\n##  [62,]  0.64632570\n##  [63,]  0.75197509\n##  [64,]  0.32937752\n##  [65,]  0.64632570\n##  [66,]  0.54067630\n##  [67,]  0.54067630\n##  [68,]  1.06892327\n##  [69,]  1.49152084\n##  [70,]  1.49152084\n##  [71,]  1.17457266\n##  [72,]  0.43502691\n##  [73,] -0.51581762\n##  [74,]  0.32937752\n##  [75,]  1.38587145\n##  [76,]  0.22372813\n##  [77,]  0.32937752\n##  [78,]  0.43502691\n##  [79,]  0.64632570\n##  [80,]  0.96327387\n##  [81,]  0.75197509\n##  [82,] -0.41016823\n##  [83,]  0.32937752\n##  [84,]  0.43502691\n##  [85,]  0.85762448\n##  [86,]  0.75197509\n##  [87,]  0.43502691\n##  [88,]  0.85762448\n##  [89,]  1.06892327\n##  [90,]  0.85762448\n##  [91,]  0.54067630\n##  [92,]  0.32937752\n##  [93,]  0.32937752\n##  [94,]  0.32937752\n##  [95,]  0.43502691\n##  [96,]  0.85762448\n##  [97,]  0.75197509\n##  [98,]  0.96327387\n##  [99,]  1.17457266\n## [100,]  1.28022205\n## [101,]  1.28022205\n## [102,]  1.49152084\n## [103,]  0.85762448\n## [104,]  0.85762448\n## [105,]  0.43502691\n## [106,]  0.22372813\n## [107,]  0.11807873\n## [108,] -0.09322005\n## [109,]  0.11807873\n## [110,] -0.19886945\n## [111,]  0.01242934\n## [112,]  0.01242934\n## [113,] -0.09322005\n## [114,] -0.62146702\n## [115,] -0.30451884\n## [116,]  0.11807873\n## [117,]  0.32937752\n## [118,]  0.85762448\n## [119,]  1.06892327\n## [120,]  2.01976780\n## [121,]  1.70281962\n## [122,]  1.91411841\n## [123,]  1.70281962\n## [124,]  1.38587145\n## [125,]  1.49152084\n## [126,]  1.59717023\n## [127,]  1.59717023\n## [128,]  0.96327387\n## [129,]  0.64632570\n## [130,]  0.22372813\n## [131,]  0.01242934\n## [132,] -0.30451884\n## [133,] -0.51581762\n## [134,]  0.32937752\n## [135,] -0.19886945\n## [136,] -0.09322005\n## [137,] -0.72711641\n## [138,] -0.72711641\n## [139,]  0.01242934\n## [140,] -1.14971398\n## [141,] -0.19886945\n## [142,] -1.04406459\n## [143,]  0.43502691\n## [144,] -1.46666216\n## [145,] -0.72711641\n## [146,]  0.32937752\n## [147,] -0.93841519\n## [148,] -1.57231155\n## [149,] -0.83276580\n## [150,] -0.09322005\n## [151,] -0.30451884\n## [152,] -0.19886945\n## [153,] -1.04406459\n## attr(,\"scaled:center\")\n## [1] 77.88235\n## attr(,\"scaled:scale\")\n## [1] 9.46527"},{"path":"covariance.html","id":"covariance","chapter":"3 Covariance","heading":"3 Covariance","text":"","code":""},{"path":"covariance.html","id":"covariance-calculating","chapter":"3 Covariance","heading":"3.1 Calculating covariance","text":"last chapter variance, measures spread single variable. Now extend idea pairs variables.say two variables “co-vary” spread one variable related spread another variable. relationship represents association two variables.’ll call two variables \\(X_{1}\\) \\(X_{2}\\). keep things simple, let’s assume already mean centered variables.\\(X_{1}\\) \\(X_{2}\\) already mean centered, \\(\\overline{X_{1}}\\) \\(\\overline{X_{2}}\\)?last chapter variance, ’ll build calculation covariance step--step using table keep track intermediate quantities need.two variables (\\(n = 7\\)) mean centered:Check mean columns truly zero.Something interesting happens look product \\(X_{1}X_{2}\\).\\(X_{1}\\) \\(X_{2}\\) lie means, positive numbers. Therefore, product positive.\\(X_{1}\\) \\(X_{2}\\) lie means? know values individually know product?chart , products listed new column:Now add products across seven data pairs:\\(X_{1}\\) \\(X_{2}\\) tend similar values (positive negative), product usually positive. ’s true every pair values table ; products negative. majority positive. Therefore, sum products positive.’re almost . Just like wanted average squared deviation calculate variance, want average products third column . just like case variance, ’s quite average calculate. Instead dividing \\(n\\), divide \\(n - 1\\) exactly esoteric reason. example, 7 data points (words, 7 rows data), divide 6.Putting together:diagrams, covariance two variables indicated curved, double-headed arrow pointing boxes labeled value covariance, like :Note still include variances individual variables. still important us. just one new type arrow now.Verify variances diagram correct example. can hand want, using R fine .final formula covariance, written \\(Cov\\left(X_{1}, X_{2}\\right)\\). works pairs variables, even aren’t mean centered. terms \\(\\left(X_{1} - \\overline{X_1}\\right)\\) \\(\\left(X_{2} - \\overline{X_2}\\right)\\) mean centering:\\[\nCov\\left(X_{1}, X_{2}\\right) = \\frac{\\sum \\left(X_{1} - \\overline{X_{1}}\\right)\\left(X_{2} - \\overline{X_{2}}\\right)}{n - 1}\n\\]Suppose \\(X_{1}\\) tends mean \\(X_{2}\\) mean \\(X_{1}\\) tends mean \\(X_{2}\\) mean. product \\(\\left(X_{1} - \\overline{X_{1}}\\right)\\left(X_{2} - \\overline{X_{2}}\\right)\\) usually ? Therefore, sum products likely ?general variables (necessarily mean centered), table actually look like :Calculate covariance hand making table like one . (variables mean centered, ’ll calculate mean variable order fill third fourth columns.)\\(X_{3}\\): 8, 10, 16, 7, 4, 3\\(X_{4}\\): 6, 5, 4, 9, 11, 7Explain intuitively covariance negative two variables.calculating variance, order data points matter. ?calculating covariance, order data points matter. ?keep pairs together, rearrange rows table. affect covariance?","code":""},{"path":"covariance.html","id":"covariance-r","chapter":"3 Covariance","heading":"3.2 Calculating covariance in R","text":"’ve done hand times make sure understand formula works, can let R work us:’s real world data. addition temperature (’ve already seen), can use wind speed see association :","code":"\nX1 <- c(-1,-2, 2, -3, 4, -1, 1)\nX2 <- c(-2, 2, -2, -1, 2, -2, 3)\ncov(X1, X2)## [1] 1.666667\nX3 <- c(8, 10, 16, 7, 4, 3)\nX4 <- c(6, 5, 4, 9, 11, 7)\ncov(X3, X4)## [1] -9.2\ncov(airquality$Temp, airquality$Wind)## [1] -15.27214"},{"path":"covariance.html","id":"covariance-rules","chapter":"3 Covariance","heading":"3.3 Covariance rules","text":"’ll think variance covariance rules one big list. left Rule 4, now ’ll introduce Rule 5.Rule 5\\[\nCov(X, X) = Var(X)\n\\]words, Rule 5 states covariance variable just thing variance variable. quite remarkable! means variance really just special case covariance.Explain Rule 5 true. (Hint: think calculate \\(Cov(X, X)\\) using either formula table—!)Rule 6\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{2}, X_{1}\\right)\n\\]words, say covariance symmetric.Explain Rule 6 true. (, think formula table—!)next four rules analogous similar rules variance (Rule 1, Rule 2, Rule 3, Rule 4).Rule 7Suppose \\(C\\) “constant” variable, meaning always value (rather variable contain lots different numbers). ,\\[\nCov\\left(X, C\\right) = 0\n\\]always, try explain rule. Give intuitive explanation rule “” true. think computationally, thinking either formula table—!Rule 8\\[\nCov\\left(X_{1} + X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) + Cov\\left(X_{2}, X_{3}\\right)  \n\\]appreciate longer restriction relationships among variables involved. Rule 2 worked two variables independent. hand, Rule 8 works combination variables, matter relation.Even satisfying next rule:Rule 9\\[\nCov\\left(X_{1} - X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) - Cov\\left(X_{2}, X_{3}\\right)  \n\\]Yay! minus sign behaves sensibly now! course, since covariances can positive negative (unlike variances always positive!) can safely subtract two without worry. rule, like Rule 8, depend \\(X_{1}\\) \\(X_{2}\\) independent. can two variables.versions rules addition subtraction side, just minor variations Rule 8 Rule 9, ’re worth mentioning separate rule. Remember covariance symmetric, can always swap things left right comma.\\[\nCov\\left(X_{1}, X_{2} \\pm X_{3}\\right) = Cov\\left(X_{1}, X_{2}\\right) \\pm Cov\\left(X_{1}, X_{3}\\right)  \n\\]Rule 10If \\(\\) number,\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{1}, X_{2}\\right) =  Cov\\left(X_{1}, X_{2}\\right)  \n\\]rule also sensible. Instead Rule 4 takes number \\(\\) pulls \\(^{2}\\), Rule 10 just pulls single factor \\(\\) (either slot).Just couple rules. talking independence conjunction Rule 8 Rule 9. leads directly interesting super-important rule:Rule 11If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nCov\\left(X_{1}, X_{2}\\right) = 0\n\\]Rule 11 true, intuitively?’s interesting note rule works one way. words, know two variables independent, can conclude covariance zero. However, know covariance zero, doesn’t necessarily mean two variables independent. ’ll see example later chapter.Finally, one rule rule :Rule 12For two variables \\(X_{1}\\) \\(X_{2}\\):\\[\nVar(aX_{1} + bX_{2}) =\n    ^2Var(X_{1}) + b^2Var(X_{2}) + 2abCov(X_{1}, X_{2})\n\\]brings practically everything know together one rule!Proving Rule 12 give us good practice type manipulation ’ll need future chapters. goes. first steps, name rule ’re invoking. , ’ll pick thread follow last steps .\\[\\begin{align}\nVar(aX_{1} + bX_{2}) &= Cov(aX_{1} + bX_{2}, aX_{1} + bX_{2}) \\\\\n    &= Cov(aX_{1} + bX_{2}, aX_{1}) + Cov(aX_{1} + bX_{2}, bX_{2}) \\\\\n    &=  Cov(aX_{1}, aX_{1}) +\n        Cov(bX_{2}, aX_{1}) + \\\\\n    &   \\qquad Cov(aX_{1}, bX_{2}) +\n        Cov(bX_{2}, bX_{2}) \\\\\n    &= \\quad ???\n\\end{align}\\]’ll need rules calculations future chapters. Rather search Chapter 2 chapter, ’ve gathered rules one convenient place Appendix .","code":""},{"path":"covariance.html","id":"covariance-correlation","chapter":"3 Covariance","heading":"3.4 Correlation","text":"pros cons calculating covariance similar variance. mathematics much nicer covariance, lose interpretability.Let’s suppose \\(X_{1}\\) measures salary dollars \\(X_{2}\\) measures years education. expect association variables, calculate covariance. unit measurement resulting number?solution problem simple variance. Since variance squared units, take square root. Covariance weird product units, something clever.Following activity , let’s suppose covariance units “dollar-years”. divide number expressed dollars, get rid units ’re left years. seems unsatisfying; covariance express something variables went . Likewise, makes sense divide number expressed years leave us just dollars.solution dilemma accept aren’t going able keep units meaningful way. Therefore, want something standardized, meaning units.\\(X_{1}\\) expressed dollars, can think statistic measures spread also units dollars?Likewise, \\(X_{2}\\) measured years, statistic measures spread also units years?previous activity gives us idea. divide covariance standard deviation \\(X_{1}\\) standard deviation \\(X_{2}\\)?\\[\n\\frac{Cov(X_{1},X_{2})}{SD(X_{1}) SD(X_{2})}\n\\]Sometimes ’s written like :\\[\n\\frac{Cov(X_{1},X_{2})}{\\sqrt{Var(X_{1})} \\sqrt{Var(X_{2})}}\n\\]\n’s thing, right?quantity units. call correlation \\(X_{1}\\) \\(X_{2}\\). ’ll either write\n\\[\nCorr(X_{1}, X_{2})\n\\]\n, need concise,\n\\[\nr_{X_{1}X_{2}}\n\\]Yes, correlation coefficient learned intro stats class, although wasn’t likely presented quite way.12One great thing correlation units, serves sort “universal” measure two variables co-vary. best part nice intuitive meaning precisely factors pieces covariance spread two variables individually. words, fact \\(X_{1}\\) \\(X_{2}\\) variability actually complicates notion covariance. individual variances “corrupt” interpretation covariance. excising , ’s left correlation “pure” part covariance expresses relationship association \\(X_{1}\\) \\(X_{2}\\).","code":""},{"path":"covariance.html","id":"covariance-standardized","chapter":"3 Covariance","heading":"3.5 Covariance with standardized data","text":"last chapter, showed variance standardized variable 1. covariance two standardized variables?Let’s standardize \\(X_{1}\\) \\(X_{2}\\). make math little easier, ’ll use similar notation used end last chapter.\\(M_{1} = \\overline{X_{1}}\\)\\(S_{1} = SD(X_{1})\\)\\(M_{2} = \\overline{X_{2}}\\)\\(S_{2} = SD(X_{2})\\)’ll write z-scores way amenable mathematical manipulation (like ):\\[\nZ_{1} = \\frac{1}{S_{1}}\\left(X_{1} - M_{1}\\right)\n\\]\\[\nZ_{2} = \\frac{1}{S_{2}}\\left(X_{2} - M_{2}\\right)\n\\]looks little intimidating, apply rules, works :\\[\\begin{align}\nCov(Z_{1}, Z_{2}) &= Cov\\left( \\frac{1}{S_{1}}\\left(X_{1} - M_{1}\\right), \\frac{1}{S_{2}}\\left(X_{2} - M_{2}\\right) \\right) \\\\\n    &= \\quad ???\n\\end{align}\\]Work . Take time. Apply rules carefully. know ’re aiming , get\\[\nCov(Z_{1}, Z_{2}) = \\frac{Cov\\left( X_{1}, X_{2} \\right)}{S_{1} S_{2}}\n\\]Okay, now remember \\(S_{1}\\) just convenient substitute \\(SD(X_{1})\\) \\(S_{2}\\) just substitute \\(SD(X_{2})\\). Wait, answer look familiar?cool! Correlation simply covariance two variables ’ve standardized.also reinforces earlier comment interpreting covariance removing extraneous influence spread individual variables. Standardizing variables makes spread variables 1, covariance now pure representation just association .probably remember intro stats correlation takes values -1 1. fact obvious formula . fraction\n\\[\n\\frac{Cov(X_{1},X_{2})}{SD(X_{1}) SD(X_{2})}\n\\]\nbounded -1 1?Let’s go back standardized variable keep things simple. correlation just covariance two standardized variables:\\[\nCorr(X_{1}, X_{2}) = Cov(Z_{1}, Z_{2})\n\\]Use rules calculate :\\[\nVar(Z_{1} + Z_{2})\n\\]Remember \\(Z_{1}\\) \\(Z_{2}\\) necessarily independent. (fact, hope . Otherwise, care correlation? zero!) need Rule 12, Rule 2. Keep manipulating get\n\\[\n2 + 2Corr(X_{1}, X_{2})\n\\]Since variances always non-negative, now know \\[\n0 \\leq 2 + 2Corr(X_{1}, X_{2})\n\\]\nSolve inequality \\(Corr(X_{1}, X_{2})\\).Now follow exact steps \n\\[\nVar(Z_{1} - Z_{2})\n\\]little change answer, one small change. , solve resulting inequality. (Don’t forget key rule working inequalities multiplying dividing negative number changes direction inequality.)fact state without proof:Correlations interpretable strength linear associations.? Basically, boils fact “perfect” correlation 1 -1 achievable data points lie perfectly straight line. Therefore, thinking correlation lying 0 1 (0 -1) sensible judging close points lying straight line. ’ll see examples next section plot data.calculation correlation R, use cor command:Use R confirm number covariance divided product standard deviations.","code":"\ncor(airquality$Temp, airquality$Wind)## [1] -0.4579879"},{"path":"covariance.html","id":"covariance-visualizing","chapter":"3 Covariance","heading":"3.6 Visualizing correlation","text":"Covariance hard interpret, ’re visualizing data want understand association might exist two variables, correlation much better statistic calculate. Let’s see correlation relates graph two variables.getting graphing, need load packages. tidyverse whole set commonly used packages allow us work data frames (“tibbles” cool kids calling ) make graphs. sure load package typing following R going :fact, , ’ll start chapter loading necessary libraries R ’ll need.standard graph two numerical variables scatterplot. Let’s start straight line relationship. First, define two variables. ’ll use shortcuts make lives little easier. seq command just generates sequence numbers.can establish linear relationship just declaring one formula:put variables graph, helps make columns single tibble.graph:Now correlation:1, expected.perfectly straight line negative slope?’ll throw new variable tibble already (convenience). explain syntax , %>% symbol called “pipe” tells R pass linear_data tibble next row process . processing dictated bind_cols command tells R “bind new column” tibble. part says X7 = X7 may little confusing. says add new column X7, also still call X7., expected.happens plot random data? runif command just chooses random numbers uniformly 0 1.13 use set.seed make work reproducible. get set random numbers machine use seed use .guess correlation \\(X_{8}\\) \\(X_{9}\\)?Now calculate using R? get exact answer guessed? , ?data follows perfect mathematical relationship straight line? example, part parabola.Now correlation:large correlation, exactly 1, even though points follow precise mathematical relationship. relationship linear.’s fascinating example. , ’ll want parabola goes .looking answer, guess correlation \\(X_{5}\\) \\(X_{11}\\)?Now calculate correlation R., ’s perfect mathematical relationship two variables. definitely associated. correlation 0?Recall earlier promise discuss Rule 11. two variables independent, covariance zero, , therefore, correlation also zero. However, rule doesn’t work way around. claim knowing covariance/correlation zero imply (necessarily) two variables independent. promised example phenomenon. \\(X_{5}\\) \\(X_{11}\\) zero correlation. yet, \\(X_{5}\\) \\(X_{11}\\) definitely independent.important enough fancy box:see correlation two variables zero near zero, careful conclude variables independent.zero near-zero correlation indicates lack linear association two variables. may nonlinear associations. ’s ’s always good idea graph data.Real data , course, much messier ’s just possible perfect correlations two variables measured real world. (find perfect correlation two columns data, chances either recorded column twice, second column simple transformation first column, like multiplying every value number something like .)plot temperature (degrees Fahrenheit) wind speed (mph) New York air quality data set.Just looking scatterplot (without calculating anything), correlation two variables positive negative? Try guessing exact value correlation.Now calculate exact value correlation see close .want practice looking scatterplots guessing correlation, try online game:Guess CorrelationTurn sound! whole class plays time, classroom sound like arcade. Compete classmates see can get high score.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.7     ✔ dplyr   1.0.9\n## ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nX5 <- seq(1, 9)\nX5## [1] 1 2 3 4 5 6 7 8 9\nX6 <- 3 + 0.5 * X5\nX6## [1] 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5\nlinear_data <- tibble(X5, X6)\nlinear_data## # A tibble: 9 × 2\n##      X5    X6\n##   <int> <dbl>\n## 1     1   3.5\n## 2     2   4  \n## 3     3   4.5\n## 4     4   5  \n## 5     5   5.5\n## 6     6   6  \n## 7     7   6.5\n## 8     8   7  \n## 9     9   7.5\nggplot(linear_data, aes(y = X6, x = X5)) +\n    geom_point()\ncor(X5, X6)## [1] 1\nX7 <- 5 - 0.2 * X5\nX7## [1] 4.8 4.6 4.4 4.2 4.0 3.8 3.6 3.4 3.2\nlinear_data <- linear_data %>%\n    bind_cols(X7 = X7)\nlinear_data## # A tibble: 9 × 3\n##      X5    X6    X7\n##   <int> <dbl> <dbl>\n## 1     1   3.5   4.8\n## 2     2   4     4.6\n## 3     3   4.5   4.4\n## 4     4   5     4.2\n## 5     5   5.5   4  \n## 6     6   6     3.8\n## 7     7   6.5   3.6\n## 8     8   7     3.4\n## 9     9   7.5   3.2\nggplot(linear_data, aes(y = X7, x = X5)) +\n    geom_point()\ncor(X5, X7)## [1] -1\nset.seed(1234)\nX8 <- runif(20)\nX9 <- runif(20)\nX8##  [1] 0.113703411 0.622299405 0.609274733 0.623379442 0.860915384 0.640310605\n##  [7] 0.009495756 0.232550506 0.666083758 0.514251141 0.693591292 0.544974836\n## [13] 0.282733584 0.923433484 0.292315840 0.837295628 0.286223285 0.266820780\n## [19] 0.186722790 0.232225911\nX9##  [1] 0.31661245 0.30269337 0.15904600 0.03999592 0.21879954 0.81059855\n##  [7] 0.52569755 0.91465817 0.83134505 0.04577026 0.45609148 0.26518667\n## [13] 0.30467220 0.50730687 0.18109621 0.75967064 0.20124804 0.25880982\n## [19] 0.99215042 0.80735234\nrandom_data <- tibble(X8, X9)\nrandom_data## # A tibble: 20 × 2\n##         X8     X9\n##      <dbl>  <dbl>\n##  1 0.114   0.317 \n##  2 0.622   0.303 \n##  3 0.609   0.159 \n##  4 0.623   0.0400\n##  5 0.861   0.219 \n##  6 0.640   0.811 \n##  7 0.00950 0.526 \n##  8 0.233   0.915 \n##  9 0.666   0.831 \n## 10 0.514   0.0458\n## 11 0.694   0.456 \n## 12 0.545   0.265 \n## 13 0.283   0.305 \n## 14 0.923   0.507 \n## 15 0.292   0.181 \n## 16 0.837   0.760 \n## 17 0.286   0.201 \n## 18 0.267   0.259 \n## 19 0.187   0.992 \n## 20 0.232   0.807\nggplot(random_data, aes(y = X9, x = X8)) +\n    geom_point()\nX10 <- 0.1 * X5^2\nX10## [1] 0.1 0.4 0.9 1.6 2.5 3.6 4.9 6.4 8.1\nnonlinear_data <- tibble(X5, X10)\nnonlinear_data## # A tibble: 9 × 2\n##      X5   X10\n##   <int> <dbl>\n## 1     1   0.1\n## 2     2   0.4\n## 3     3   0.9\n## 4     4   1.6\n## 5     5   2.5\n## 6     6   3.6\n## 7     7   4.9\n## 8     8   6.4\n## 9     9   8.1\nggplot(nonlinear_data, aes(y = X10, x = X5)) +\n    geom_point()\ncor(X5, X10)## [1] 0.975281\nX11 <- 0.5 * (X5 - 5)^2\nX11## [1] 8.0 4.5 2.0 0.5 0.0 0.5 2.0 4.5 8.0\nnonlinear_data <- nonlinear_data %>%\n    bind_cols(X11 = X11)\nnonlinear_data## # A tibble: 9 × 3\n##      X5   X10   X11\n##   <int> <dbl> <dbl>\n## 1     1   0.1   8  \n## 2     2   0.4   4.5\n## 3     3   0.9   2  \n## 4     4   1.6   0.5\n## 5     5   2.5   0  \n## 6     6   3.6   0.5\n## 7     7   4.9   2  \n## 8     8   6.4   4.5\n## 9     9   8.1   8\nggplot(nonlinear_data, aes(y = X11, x = X5)) +\n    geom_point()\nggplot(airquality, aes(y = Temp, x = Wind)) +\n    geom_point()"},{"path":"simple.html","id":"simple","chapter":"4 Simple regression","heading":"4 Simple regression","text":"","code":""},{"path":"simple.html","id":"preliminaries","chapter":"4 Simple regression","heading":"Preliminaries","text":"need load packages use chapter. tidyverse package sorts utilities working tibbles (data frames). broom package used calculate store residuals model. also first introduction lavaan package used throughout rest book.","code":"\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(lavaan)## This is lavaan 0.6-11\n## lavaan is FREE software! Please report any bugs."},{"path":"simple.html","id":"simple-advice","chapter":"4 Simple regression","heading":"4.1 Some friendly advice","text":"Even seen regression reading book, sure read study chapter next chapter thoroughly. nothing else, need comfortable notation terminology established . also take special care motivate justify calculations taken granted treatments regression. framework important move mediation path analysis following chapters. comfortable content chapter, won’t much “new” say multiple regression, mediation, path analysis generally.","code":""},{"path":"simple.html","id":"simple-prediction","chapter":"4 Simple regression","heading":"4.2 Prediction","text":"One important tasks statistics prediction. Given data, can predict value something important population interest?Suppose gathered data anxiety among Utah high school students. various instruments available measuring anxiety, say administered Beck Anxiety Inventory. instrument assigns score 0 63, lower numbers indicating less anxiety higher numbers indicating .take care make sure sample close simple random sample possible ’s representative population (high school students state Utah). sample data, can calculate summary statistics. example, might find mean anxiety score Utah high school students 7.1 standard deviation 3.9.random Utah high school student walks door. don’t know anything . Can say anything anxiety? best guess score might Beck Anxiety Inventory?can lot better another variable can measure. example, let’s suppose data records anxiety, also minutes smart phone usage per day.theory, information smart phone usage potentially help us make better predictions anxiety?suspect association anxiety smart phone usage positive negative? (can Google question check empirical evidence guess.)Now imagine another random Utah high school student walks door. time, tell smart phone usage average (sitting mean). best prediction anxiety score? (Give exact value.)told student walked door higher average smart phone usage? prediction anxiety score? (can’t give exact value , give qualitatively sensible answer.)told student walked door lower average smart phone usage? prediction anxiety score? (, just give qualitatively sensible answer.)","code":""},{"path":"simple.html","id":"simple-terminology","chapter":"4 Simple regression","heading":"4.3 Regression terminology","text":"one variable suspect may help us predict another variable, one way study using simple regression model.related , somewhat different , covariance. Covariance symmetric, expresses idea two variables mutually related. “directionality” relationship. way contrast, simple regression model asserts one variables “predictor” “response”. words, start values properties predictor variable try deduce can values properties response variable.Keep mind “directionality” “causality”. ’s possible one variable causes another, needs data collection process (often carefully controlled experiment) clear scientific rationale justifies causal relationship variables can start thinking inferring causality. purposes much book, directionality just means wish establish predictive relationship wherein start properties one variable try predict properties another variable. often “sensible” order based research questions asked hypotheses posed.many different terminological conventions statistics, aware “predictor” variables also called—often depending discipline context—features, covariates, controls, regressors, inputs, explanatory variables, independent variables. fact, context structural equation modeling, use term “exogenous” refer variables play role. (term much precise definition ’ll discuss future chapters.) “response” variables might called outcomes, outputs, targets, criteria, predicted variables, explained variables, dependent variables, among others. book, often use term “endogenous” (, specific way yet explained). data collection process clear scientific rationale justifies causal relationship variables, might able refer variables either “cause” “effect”.Keep mind ’s scientific question want ask determines predictor/response relationship. different researcher different hypothesis might use two variables roles reversed.anxiety/smart-phone example , variable predictor response, least according way stated scenario?","code":""},{"path":"simple.html","id":"simple-model","chapter":"4 Simple regression","heading":"4.4 The simple regression model","text":"figure top chapter, now decorated letters (number):goal section explain .variable names \\(X\\) \\(Y\\). \\(X\\) exogenous variable \\(Y\\) endogenous variable. example, \\(X\\) might smart phone usage \\(Y\\) might anxiety score example . section, ’re going concrete calculations using example last chapter wind speed temperature airquality data set. last chapter, simply calculated (symmetric) correlation wind speed temperature. , consider wind speed exogenous temperature endogenous. words, goal use wind speed predictor temperature.letter \\(v\\) requires explanation. variance variable \\(X\\), already know .parameter \\(b\\) supposed measure something predictive relationship \\(X\\) \\(Y\\). attached arrow drawn little thicker arrows diagram. say moment.really weird, new part circle right. “error” term.“error” ? illustrate, let’s plot wind speed temperature. plotting analyzing variables, going mean-center put tibble.Note exogenous variable (wind speed) x-axis endogenous variable (temperature) y-axis.can see negative reasonably linear association variables, let’s add line best fit data.line passes right origin \\((0, 0)\\). ?slope line \\(-1.23\\). ’ll see calculate slope bit. slope mean?Look help file airquality data set. (Either use Help tab RStudio type ?airquality Console.)units measurement \\(X\\)? units measurement \\(Y\\)? Since slope “rise run”, units slope?, idea every additional mile per hour wind speed, predict temperature goes 1.23 degrees Fahrenheit.following sentence incorrect?every additional mile per hour wind speed, temperature goes 1.23 degrees Fahrenheit.point line model makes predictions. long \\(X\\) \\(Y\\) mean-centered, equation line \\[\n\\hat{Y} = bX\n\\]new piece notation : \\(\\hat{Y}\\). symbol represents predicted value \\(Y\\) according model. get actual value \\(Y\\) piece model actual values \\(Y\\) differ model real-world data doesn’t lie perfect straight line. moment.According information , can estimate value \\(b\\) \\(-1.23\\):\\[\n\\hat{Y} = -1.23X\n\\]proportional effect. , long \\(X\\) \\(Y\\) mean-centered, knowing value \\(X\\) allows us predict value \\(Y\\) multiplying \\(b\\).predictions almost always wrong. given day, given increase 1 mile per hour wind wind speed, rarely happen temperature drop exactly 1.23 degrees. ’s just sort “average” time. average, ’s slight temperature change associated 1 mph change wind speed, number -1.23 best estimate average change across whole data set. need especially clear increase wind speed necessary cause drop temperature. mean, might partially true, can’t prove observational data. sorts reasons explain increase wind speed drop temperature (like cold front moving ).Since predictions average effects specific guarantees, every prediction make wrong amount. (get extremely lucky, even , ’s difficult imagine situation prediction precisely correct , say, 10 decimal places something like .) Therefore, error prediction. new equation—accounting error—\\[\nY = bX + E\n\\]\n\n\\[\nY = -1.23X + E\n\\]Now use \\(Y\\) instead \\(\\hat{Y}\\). include error, can recover exact value \\(Y\\), longer just prediction straight-line model. Remember :write regression equation endogenous variable includes incoming arrows, including error term, use \\(Y\\).write regression equation endogenous variable includes incoming arrows, excluding error term, use \\(\\hat{Y}\\).Error funny word negative connotation. sounds like made mistake. Well, model make mistakes. Every model prediction technically wrong. kind mistake results arithmetic wrong anything like . ’s simply “natural” error results messiness real world impossibility predicting anything certainty. reason, often prefer term “residual”. ’s “left ” made prediction. ’s extra change temperature, example, accounted model wind speed alone.residuals evident plot . residuals, every data point lie perfect straight line. data points either little line. vertical distances data line residuals errors. example two residuals plotted red.Points line negative residuals points line positive residuals.residuals appear observed measured variables data. consequence variety unmeasured factors determine temperature aside wind speed. unmeasured variable appears model called latent variable. discuss latent variables far greater detail Chapter 8. now, just know latent variables indicated circles diagram. ’s circle letter \\(E\\) inside.equation\\[\nY = bX + E\n\\]\ncan also written \n\\[\nY = bX + 1 \\cdot E\n\\]“1” represented diagram?letters \\(v\\), \\(b\\), \\(e\\) called free parameters free vary depending data. “1” called fixed parameter. attached arrow, ’s technically parameter model, parameter need calculate. “fixed” value 1 error term model represented \\(+E\\) fixed coefficient 1. general, throughout book, word “parameter” used without qualification, can assume talking free parameters, ones need calculate.Arrows represent coefficients regression relationships drawn little thicker arrows diagram. convention , knowledge, unique book. absolutely necessary, helpful later arrows floating distinguish regression relationship kinds relationships (like error terms covariances, example).thing diagram hasn’t explained yet \\(e\\).\\(e\\) appear diagram? Given appears, represent mathematically?know curved arrows represent variance. mean measure variance variable can’t observe?scatterplot look like error variance small. error variance large?variability size residuals. small (points close line) large (points far line). spread residuals can estimated data, just like variance calculation.14 turns 70.8. ’ll see calculate .Let’s put everything together diagram.First, need variance \\(X\\) (wind speed). Calculate R. get 12.4.matter calculate variance variable Wind original airquality data, variance \\(X\\) variable airqualilty_mc? ?’s helpful see \\(X\\) \\(Y\\) generic prototypes simple regression model, applied problems now ’ll refer variables using contextually meaningful names. final diagram looks like :error variable \\(E\\) exogenous endogenous?One final note diagram: may noticed \\(Y\\) variable (\\(\\textit{TEMP}\\)) variance term attached. double-headed arrow box. ? point trying understand variance \\(Y\\) using elements model. words, \\(Y\\) variance, variance partially predicted \\(X\\). rest variance predicted \\(X\\) swept error term \\(E\\). variance \\(Y\\) accounted contribution \\(X\\) \\(E\\) combined.","code":"\nX <- airquality$Wind - mean(airquality$Wind)\nY <- airquality$Temp - mean(airquality$Temp)\nairquality_mc <- tibble(X, Y)\nairquality_mc## # A tibble: 153 × 2\n##        X      Y\n##    <dbl>  <dbl>\n##  1 -2.56 -10.9 \n##  2 -1.96  -5.88\n##  3  2.64  -3.88\n##  4  1.54 -15.9 \n##  5  4.34 -21.9 \n##  6  4.94 -11.9 \n##  7 -1.36 -12.9 \n##  8  3.84 -18.9 \n##  9 10.1  -16.9 \n## 10 -1.36  -8.88\n## # … with 143 more rows\nggplot(airquality_mc, aes(y = Y, x = X)) +\n    geom_point()\nggplot(airquality_mc, aes(y = Y, x = X)) +\n    geom_point() +\n    geom_smooth(method = lm, se = FALSE)## `geom_smooth()` using formula 'y ~ x'\nggplot(airquality_mc, aes(y = Y, x = X)) +\n    geom_point() +\n    geom_smooth(method = lm, se = FALSE) +\n    annotate(\"segment\",\n             x = -2.56, y = 3.15,\n             xend = -2.56, yend = 3.15 - 14.03,\n             color = \"red\", size = 1.5) +\n    annotate(\"segment\",\n             x = 4.94, y = -6.08,\n             xend = 4.94, yend = -6.08 + 9.20,\n             color = \"red\", size = 1.5)## `geom_smooth()` using formula 'y ~ x'"},{"path":"simple.html","id":"simple-assumptions","chapter":"4 Simple regression","heading":"4.5 Simple regression assumptions","text":"calculations need , ability interpret results, depend certain assumptions met.look regression assumptions, might find huge list requirements. requirements relate calculating statistics like P-values regression parameters. now, content simply know makes sense interpret parameters model ., really need five assumptions:data come “good” sample.values exogenous variable measured without error.relationship \\(Y\\) \\(X\\) approximately linear.residuals independent \\(X\\) values.influential outliers.Let’s address one time:mean “good” sample? simple random sample gold standard, ’s usually possible obtain one real world. make sampling process random possible ensure resulting sample representative population ’re trying study possible.always strive measure things precisely. measuring physical phenomena precise scientific instruments, can usually minimize -called “measurement error”. measurements lot messier. might ask person series survey questions today ask questions tomorrow get somewhat different answers. might record data consists “educated guess” estimates things difficult pin precisely. Whenever exogenous variable unreliable, can introduce bias model. (Curiously, measurement error endogenous variable doesn’t matter quite much. may introduce variability estimates, bias values parameters regression model.)can check linearity scatterplot. Just make sure pattern dots doesn’t strong curvature .patterns residuals . randomly scattered around best-fit line average size residuals change radically one side graph .15 can check plotting residuals, can’t done model fit. (residuals don’t exist value \\(\\hat{Y}\\) compare \\(Y\\).)Check scatterplot outliers. serious ones, assess make sure data entry mistakes. correspond valid data, just throw away.16 Often, solution run analysis including (temporarily) excluding outliers make sure presence doesn’t radically alter parameter estimates.","code":""},{"path":"simple.html","id":"simple-calculating","chapter":"4 Simple regression","heading":"4.6 Calculating regression parameters","text":"Now ’ll show one way calculate parameters (numbers) diagram. isn’t way . fact, approach used intro stats classes. approach helpful illustrate way calculations future chapters.Let’s go back diagram without numbers:letter \\(v\\) easy ’s just variance \\(X\\):\\[\nVar(X) = v\n\\]can estimate directly data. (already R wind speed.) completing activities , also calculate \\(b\\) \\(e\\).get parameters, set equations.\nfirst observation need make , important arrows diagram, ’s just important arrows .arrows directly connecting \\(X\\) \\(E\\)? might imply relationship \\(X\\) \\(E\\)? regression assumption related question?imply value \\(Cov(E, X)\\)?little careful line reasoning . Even direct paths \\(X\\) \\(E\\), indirect paths \\(X\\) \\(E\\)? indirect path might source kind association \\(X\\) \\(E\\).possible path goes \\(Y\\) looks like\n\\[\nX \\boldsymbol{\\rightarrow} Y \\leftarrow E\n\\]\nreasons won’t explain (explained Chapter 7), type path imply kind association \\(X\\) \\(E\\). Therefore, model imply \\(X\\) \\(E\\) independent, , therefore, \\(Cov(E, X) = 0\\).Next, \\(Y\\) combination \\(X\\) \\(E\\), ’ve already seen can write\\[\nY = bX + E\n\\]Therefore, can calculate \\(Var(Y)\\) according formula using established rules. (convenient list one place located Appendix .)Keep simplifying following much possible:\\[\\begin{align}\nVar(Y)  &= Var(bX + E) \\\\\n        &= \\quad ???\n\\end{align}\\]end \\[\nb^{2}v + e\n\\]Don’t forget \\(Var(X) = v\\) \\(Var(E) = e\\) diagram!also need use information covariance \\(Y\\) \\(X\\). Keep simplifying calculation :\\[\\begin{align}\nCov(Y, X)  &= Cov(bX + E, X) \\\\\n        &= \\quad ???\n\\end{align}\\]end \\[\nbv\n\\]Use R calculate \\(Var(Y)\\) \\(Cov(Y, X)\\) airquality data. (’ve already computed \\(Var(X)\\). 12.4.)get 89.6 -15.3, respectively.Now can set equations need solve various letters want. three equations established:\\[\\begin{align}\n12.4 &= v \\\\\n89.6 &= b^2v + e \\\\\n-15.3 &= bv\n\\end{align}\\]Time little algebra. know \\(v\\). Using value, solve \\(b\\) first (using last equation). , using values \\(b\\) \\(v\\), solve \\(e\\) second equation.Check values got ones earlier diagram (possibility little rounding error).Now let’s go , time, full generality:\\[\\begin{align}\nVar(X) &= v \\\\\nVar(Y) &= b^2v + e \\\\\nCov(Y, X) &= bv\n\\end{align}\\]Therefore,\\[\nv = Var(X)\n\\]\\[\nb = \\frac{Cov(Y, X)}{Var(X)}\n\\]\\[\ne = Var(Y) - \\left( \\frac{Cov(Y, X)}{Var(X)} \\right)^2 Var(X)\n\\]","code":""},{"path":"simple.html","id":"simple-mim","chapter":"4 Simple regression","heading":"4.7 The model-implied matrix","text":"convenient future chapters collect numbers need array terms called sample covariance matrix. (Sometimes called variance-covariance matrix.) idea take covariance possible pairs observed variables arrange follows:\\[\n\\begin{bmatrix}\nCov(X, X)    &    Cov(X, Y) \\\\\nCov(Y, X)    &    Cov(Y, Y) \\\\\n\\end{bmatrix}\n\\]immediate simplifications make.Since \\(Cov(X, Y) = Cov(Y, X)\\), point writing twice. just use dot (\\(\\bullet\\)) replace \\(Cov(X, Y)\\).can replace upper-left lower-right entries (entries -called “diagonal” matrix) variances.final sample covariance matrix:\\[\n\\begin{bmatrix}\nVar(X)       &    \\bullet \\\\\nCov(Y, X)    &    Var(Y)    \\\\\n\\end{bmatrix}\n\\]Alternatively, also write :\\[\n\\begin{bmatrix}\nVar(X)       &    Cov(X, Y) \\\\\n\\bullet      &    Var(Y)    \\\\\n\\end{bmatrix}\n\\]former called lower-triangular form, latter upper-triangular form. contain information, really doesn’t matter one use.data , can calculate numbers quantities.another important matrix called model-implied matrix. Given model, covariance matrix look like? calculations , know model-implied matrix \\[\n\\begin{bmatrix}\nv   &    \\bullet    \\\\\nbv  &    b^{2}v + e \\\\\n\\end{bmatrix}\n\\]letters \\(b\\), \\(v\\), \\(e\\) unknowns. Okay, \\(v\\) unknown. ’s unknown sense parameter model, don’t work hard find . point model parameters estimated equating covariance matrix (calculated data) model-implied matrix trying solve unknown parameters.new math section. matrices just convenient ways organize work ’ve already done. parameter estimation structural equation modeling essentially setting two matrices (sample covariance matrix model-implied matrix) equal solving:\\[\n\\begin{bmatrix}\nVar(X)       &    \\bullet   \\\\\nCov(Y, X)    &    Var(Y)    \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nv       &    \\bullet      \\\\\nbv      &    b^{2}v + e        \\\\\n\\end{bmatrix}\n\\]Don’t forget matrix left—sample covariance matrix—consists numbers calculate data. matrix right—model-implied matrix—contains letters, unknown parameters ’re trying find.","code":""},{"path":"simple.html","id":"simple-coefficients-correlation","chapter":"4 Simple regression","heading":"4.8 Coefficients in terms of correlation","text":"formulas derived fine far go. allow take quantities calculated data (variances covariances observed variables) translate estimates model parameters.formula slope parameter \\(b\\) pretty simple intuitive content. ’s covariance \\(Y\\) \\(X\\), dividing variance \\(X\\) make sure right units.\\[\nb = \\frac{Cov(Y, X)}{Var(X)}\n\\]Another way look formula rearrange things bit follows:formula \\(b\\) equivalent \\[\nb = \\frac{Cov(Y, X) \\sqrt{Var (Y)}}{Var(X)\\sqrt{Var(Y)}}\n\\]?Now write like :\\[\nb = \\frac{Cov(Y, X) \\sqrt{Var (Y)}}{\\sqrt{Var(X)}\\sqrt{Var(X)}\\sqrt{Var(Y)}}\n\\]happened ?Finally, write like :\\[\nb = \\left(\\frac{Cov(Y, X)}{\\sqrt{Var(X)} \\sqrt{Var(Y)}}\\right) \\left(\\frac{\\sqrt{Var(Y)}}{\\sqrt{Var(X)}}\\right)\n\\]Explain simplifies \\[\nb = Corr(Y, X)\\left(\\frac{SD(Y)}{SD(X)}\\right)\n\\]often formula taught intro stats classes. concise notation:\\[\nb = r_{YX}\\left(\\frac{s_{Y}}{s_{X}}\\right)\n\\]intuition \\(b\\) basically just correlation \\(Y\\) \\(X\\), account scales units \\(Y\\) \\(X\\).standard deviation \\(Y\\) numerator standard deviation \\(X\\) denominator? Think units \\(b\\) must .formula error variance \\(e\\) little gross. similar trickery, though, can simplify formula quite bit.starting point:\\[\ne = Var(Y) - \\left( \\frac{Cov(Y, X)}{Var(X)} \\right)^2 Var(X)\n\\]Explain right-hand side can rewritten \\[\nVar(Y) - \\frac{Cov(Y, X)^{2}}{Var(X)}\n\\]Explain next step valid:\\[\nVar(Y) - \\frac{Cov(Y, X)^{2} Var(Y)}{Var(X) Var(Y)}\n\\]next one?\\[\nVar(Y) \\left( 1 - \\frac{Cov(Y, X)^{2}}{Var(X) Var(Y)} \\right)\n\\]thing? words, new fraction right look familiar way?hope recognize fraction right just correlation coefficient squared. whole equation can now written \\[\ne = Var(Y) \\left( 1 - r_{YX}^2 \\right)\n\\]nice consequence last equation. term parentheses \\(\\left( 1 - r_{YX}^2 \\right)\\) number 0 1, right? Since multiplying variance \\(Y\\), can think term parentheses proportion. variance \\(Y\\) explained model one two ways. thick arrow coming left uses \\(X\\) predict variance \\(Y\\). rest variance \\(Y\\) left error term \\(e\\). Therefore, \\(\\left( 1 - r_{YX}^2 \\right)\\) proportion variance \\(Y\\) left error.true, must also case \\(r_{YX}^2\\) proportion variance \\(Y\\) explained \\(X\\). Calculating one minus proportion gives complementary proportion. example, \\(\\left( 1 - r_{YX}^2 \\right) = 0.3\\), 30% variance \\(Y\\) left error. implies 70% variance \\(Y\\) explained \\(X\\). \\(1 - 0.3 = 0.7\\).authors write \\(R^{2}\\) instead \\(r^{2}\\) reason. Rearranging equation , replacing \\(r_{YX}^{2}\\) \\(R^{2}\\), writing \\(e\\) \\(Var(E)\\) looks like\\[\nR^{2} = 1 - \\frac{Var(E)}{Var(Y)}\n\\]words, can think error variance proportion total variance \\(Y\\), \\(R^2\\) complementary proportion. Therefore, \\(R^{2}\\) proportion variance accounted model.","code":""},{"path":"simple.html","id":"simple-standardized","chapter":"4 Simple regression","heading":"4.9 Regression with standardized variables","text":"Recall convert variables z-scores, variances 1 covariances become correlation coefficients. words, covariance matrix becomes correlation matrix looks like :\\[\n\\begin{bmatrix}\n1       &    \\bullet \\\\\nr_{YX}  &    1       \\\\\n\\end{bmatrix}\n\\]model-implied matrix change. Solving parameters , , except can now replace \\(Var(X)\\) \\(Var(Y)\\) 1, \\(Cov(Y, X)\\) \\(r_{YX}\\).\\[\n\\begin{bmatrix}\n1       &    \\bullet  \\\\\nr_{YX}  &    1        \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nv    &    \\bullet         \\\\\nbv   &    b^{2}v + e \\\\\n\\end{bmatrix}\n\\]\\[\\begin{align}\n1 &= v \\\\\nr_{YX} &= bv \\\\\n1 &= b^2v + e\n\\end{align}\\]Therefore,\\[\nv = 1\n\\]\\[\nb = r_{YX}\n\\]\\[\ne = 1 - r_{YX}^{2}\n\\]variables standardized, slope regression just correlation! error variance just proportion 0 1 complementary \\(r_{YX}^{2}\\) (aka, \\(R^{2}\\), variance explained model). two variances, \\(e\\) \\(R^{2}\\) now add 1.’ll use scale command create standardized variables temperature wind speed put new tibble.Modify ggplot code earlier chapter create scatterplot new standardized variables along best-fit line. slope line? (Hint: calculate correlation coefficient two standardized variables.)","code":"\nX_std <- scale(airquality$Wind)\nY_std <- scale(airquality$Temp)\nairquality_std <- tibble(X_std, Y_std)\nairquality_std## # A tibble: 153 × 2\n##    X_std[,1] Y_std[,1]\n##        <dbl>     <dbl>\n##  1    -0.726    -1.15 \n##  2    -0.556    -0.621\n##  3     0.750    -0.410\n##  4     0.438    -1.68 \n##  5     1.23     -2.31 \n##  6     1.40     -1.26 \n##  7    -0.385    -1.36 \n##  8     1.09     -1.99 \n##  9     2.88     -1.78 \n## 10    -0.385    -0.938\n## # … with 143 more rows"},{"path":"simple.html","id":"simple-r","chapter":"4 Simple regression","heading":"4.10 Simple regression in R","text":"","code":""},{"path":"simple.html","id":"simple-r-lm","chapter":"4 Simple regression","heading":"4.10.1 Using lm","text":"straightforward way run regression R use lm command. stands “linear model”. uses special symbol, tilde ~, express relationship endogenous variable exogenous variable. endogenous (response) variable always goes left, tilde. exogenous (predictor) variable goes right, tilde. Finally, data argument tell lm find variables model.two numbers slope \\(b\\)?haven’t talked intercept yet, according output, ? (Hint: ’s literally \\(1.117 \\times 10^{-14}\\). number really mean?)Run lm command, time using standardized variables airquality_std tibble. value slope surprise . Explain .Don’t forget: parameters make sense interpret unless regression assumptions met. looked scatterplot already determined approximately linear. haven’t checked residuals.residuals can obtained easily model using augment command broom package following way:many columns ’re going discuss , residuals model stored column called .resid. also standardized residuals stored column std.resid. look exactly plot except scale axes, doesn’t much matter use.residuals now stored new tibble called YX_aug, sure use following ggplot command original data. ’ll put residuals y-axis. Since ’re interested checking residuals independent \\(X\\) variable, put , unsurprisingly, x-axis. reference line \\(y = 0\\) helps us see residuals centered.orWhat see graphs ? good bad? indicates graphs residuals independent \\(X\\)?","code":"\nYX_lm <- lm(Y ~ X, data = airquality_mc)\nYX_lm## \n## Call:\n## lm(formula = Y ~ X, data = airquality_mc)\n## \n## Coefficients:\n## (Intercept)            X  \n##   1.117e-14   -1.230e+00\nYX_aug <- augment(YX_lm)\nYX_aug## # A tibble: 153 × 8\n##         Y     X .fitted  .resid    .hat .sigma   .cooksd .std.resid\n##     <dbl> <dbl>   <dbl>   <dbl>   <dbl>  <dbl>     <dbl>      <dbl>\n##  1 -10.9  -2.56    3.15 -14.0   0.0100    8.39 0.0141       -1.67  \n##  2  -5.88 -1.96    2.41  -8.29  0.00857   8.44 0.00420      -0.986 \n##  3  -3.88  2.64   -3.25  -0.631 0.0102    8.47 0.0000292    -0.0751\n##  4 -15.9   1.54   -1.90 -14.0   0.00780   8.39 0.0109       -1.66  \n##  5 -21.9   4.34   -5.34 -16.5   0.0165    8.36 0.0328       -1.98  \n##  6 -11.9   4.94   -6.08  -5.80  0.0195    8.46 0.00478      -0.694 \n##  7 -12.9  -1.36    1.67 -14.6   0.00751   8.39 0.0113       -1.73  \n##  8 -18.9   3.84   -4.73 -14.2   0.0144    8.39 0.0208       -1.69  \n##  9 -16.9  10.1   -12.5   -4.40  0.0611    8.46 0.00942      -0.538 \n## 10  -8.88 -1.36    1.67 -10.6   0.00751   8.43 0.00596      -1.25  \n## # … with 143 more rows\nggplot(YX_aug, aes(y = .resid, x = X)) +\n    geom_point() +\n    geom_hline(yintercept = 0, color = \"blue\")\nggplot(YX_aug, aes(y = .std.resid, x = X)) +\n    geom_point() +\n    geom_hline(yintercept = 0, color = \"blue\")"},{"path":"simple.html","id":"simple-r-lavaan","chapter":"4 Simple regression","heading":"4.10.2 Using lavaan","text":"also introduce briefly lavaan package. ’s totally overkill simple regression, getting used syntax now make easier continue build confidence using tool use.lavaan model built similar way lm using tilde ~ notation. One big difference model needs specified inside quotation marks first assigned name like :pass model text sem function lavaan:model now stored TEMP_WIND_fit. One way learn model use parameterEstimates function.lot output , ’re going talk now. Focus est column.recognize three estimates. Explain numbers represent.particular, pay close attention second line. hasty, may think variance \\(Y\\), correct.can also produce standardized estimates., explain three numbers. (now listed column called est.std “standardized estimates”.)Verify second line actually error variance. (Hint: remember \\(1 - r_{YX}^{2}\\).) know ’s standardized variance \\(Y\\)? (words, actually know standardized variance \\(Y\\)?)One downside using lavaan doesn’t store residuals, way checking regression assumption. complex models future chapters lavaan (comparable package) choice, residual independence assumption just : assumption. must substantive reason believe ’s true specify model.","code":"\nTEMP_WIND_model <- \"Y ~ X\"\nTEMP_WIND_fit <- sem(TEMP_WIND_model, data = airquality_mc)\nparameterEstimates(TEMP_WIND_fit)##   lhs op rhs    est    se      z pvalue ci.lower ci.upper\n## 1   Y  ~   X -1.230 0.193 -6.373      0   -1.609   -0.852\n## 2   Y ~~   Y 70.337 8.042  8.746      0   54.575   86.098\n## 3   X ~~   X 12.330 0.000     NA     NA   12.330   12.330\nstandardizedSolution(TEMP_WIND_fit)##   lhs op rhs est.std    se      z pvalue ci.lower ci.upper\n## 1   Y  ~   X  -0.458 0.060 -7.577      0   -0.576   -0.340\n## 2   Y ~~   Y   0.790 0.055 14.273      0    0.682    0.899\n## 3   X ~~   X   1.000 0.000     NA     NA    1.000    1.000"},{"path":"simple.html","id":"simple-intercepts","chapter":"4 Simple regression","heading":"4.11 What about intercepts?","text":"familiar regression another course, may wondering intercepts went. mean-centered /standardized data, intercepts. regression line always passes \\((0, 0)\\) mean-centered standardized data.[PUT REFERENCE DECIDE COVER MEAN STRUCTURE FUTURE CHAPTER.]","code":""},{"path":"multiple.html","id":"multiple","chapter":"5 Multiple regression","heading":"5 Multiple regression","text":"","code":""},{"path":"multiple.html","id":"preliminaries-1","chapter":"5 Multiple regression","heading":"Preliminaries","text":"load tidyverse package work tibbles, broom package calculate residuals, lavaan.","code":"\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(lavaan)"},{"path":"multiple.html","id":"multiple-model","chapter":"5 Multiple regression","heading":"5.1 The multiple regression model","text":"chapter extension ideas established last chapter. Multiple regression like simple regression, exogenous variables. still one endogenous variable. Although archetype illustrated beginning chapter three predictor variables, start two predictor variables keep things simple. understand happens two variables, ’s fairly straightforward generalize knowledge three predictors. logic .multiple regression model two predictors paths given parameter labels:many free parameters appear model?many fixed parameters appear model?equation describing relationship among variables can written either\\[\n\\hat{Y} = b_{1}X_{1} + b_{2}X_{2}\n\\]\\[\nY = b_{1}X_{1} + b_{2}X_{2} + E\n\\]use \\(\\hat{Y}\\) first equation \\(Y\\) second equation?Although ’ll work details two predictors, multiple regression model \\(k\\) predictors look like\\[\n\\hat{Y} = b_{1}X_{1} + b_{2}X_{2} + \\dots + b_{k}X_{k}\n\\]\\[\nY = b_{1}X_{1} + b_{2}X_{2}  + \\dots + b_{k}X_{k} + E\n\\]","code":""},{"path":"multiple.html","id":"multiple-assumptions","chapter":"5 Multiple regression","heading":"5.2 Multiple regression assumptions","text":"Fortunately, assumptions multiple regression basically simple regression minor modifications one addition:data come “good” sample.exogenous variables measured without error.relationship \\(X_{1}, \\dots, X_{k}\\), \\(Y\\) approximately linear.residuals independent \\(X_{1}, \\dots, X_{k}\\) values.influential outliers.exogenous variables highly correlated one another.discuss briefly:Nothing changed . Good analysis starts good data collection practices.Nothing changed . ’s good idea try measure variables little error possible, particular, measurement errors exogenous variables can bias parameter estimates.\\(Y\\) \\(X\\), regression model line. \\(Y\\) \\(X_{1}\\) \\(X_{2}\\), regression model plane (2-dimensional plane sitting 3-dimensional space) little challenging graph. predictors, regression model lives even higher dimensions ’s impossible visualize. check condition, best can usually check scatterplots \\(Y\\) \\(X_{}\\) individually approximately linear.fit model, can check residuals. Rather plotting residuals \\(X_{}\\) separately, can employ trick ’ll explain later chapter.Nothing changes .new condition. two predictors variables highly correlated , induces condition called multicollinearity.illustrate multicollinearity problem, think two-variable case:\\[\n\\hat{Y} = b_{1}X_{1} + b_{2}X_{2}\n\\]\ngeneral, able compute values \\(b_{1}\\) \\(b_{2}\\) best fit model data.now suppose \\(X_{2}\\) just multiple \\(X_{1}\\), say \\(X_{2} = 2X_{1}\\). Now equation looks like\\[\\begin{align}\n\\hat{Y} &= b_{1}X_{1} + b_{2}X_{2} \\\\\n        &= b_{1}X_{1} + b_{2}(2 X_{1}) \\\\\n        &= (b_{1} + 2b_{2})X_{1}\n\\end{align}\\]even though “looked like” two distinct predictors variables, just simple regression disguise. Okay, now let’s suppose try calculate slope simple regression. Say ’s 10. values \\(b_{1}\\) \\(b_{2}\\)? words, values \\(b_{1}\\) \\(b_{2}\\) solve following equation?\\[\nb_{1} + 2b_{2} = 10\n\\]Explain impossible pin unique values \\(b_{1}\\) \\(b_{2}\\) make equation true.choose large, negative value \\(b_{1}\\), imply value \\(b_{2}\\)?choose large, positive value \\(b_{1}\\), imply value \\(b_{2}\\)?Multicollinearity works lot like . Even variables exact multiples , sets highly correlated variables result equations large range possible values consistent data. Even dangerously, fitting algorithm may estimate values coefficients, numbers likely meaningless. completely different set numbers may also perfectly consistent data.clear, ’s problem covariance among predictors. expect . problem arises two predictors highly correlated .","code":""},{"path":"multiple.html","id":"multiple-calculating","chapter":"5 Multiple regression","heading":"5.3 Calculating regression parameters","text":"nothing new , calculations start get little messy. Everything follows two predictors . calculations three predictors. gets hand pretty quickly.First, let’s remember ’re trying . data, can calculate sample covariance matrix. variances covariances among observed variables:\\[\n\\begin{bmatrix}\nVar(X_{1})          &   \\bullet         &   \\bullet \\\\\nCov(X_{2}, X_{1})   &   Var(X_{2})      &   \\bullet \\\\\nCov(Y, X_{1})       &   Cov(Y, X_{2})   &   Var(Y)\n\\end{bmatrix}\n\\]Remember entries just numbers calculate directly data.get started model-implied matrix, let’s extend Rule 12 little.three variables \\(X_{1}\\), \\(X_{2}\\), \\(X_{3}\\):\\[\\begin{align}\nVar(aX_{1} + bX_{2} + cX_{3}) &=\n    ^2Var(X_{1}) + b^2Var(X_{2}) + c^2Var(X_{3}) \\\\\n    & \\quad + 2abCov(X_{1}, X_{2}) \\\\\n    & \\quad + 2acCov(X_{1}, X_{3}) \\\\\n    & \\quad + 2bcCov(X_{2}, X_{3})\n\\end{align}\\]can extended number variables. variance appears coefficient squared pair variables gets covariance term 2 times product corresponding variable coefficients. (’s hard describe words, ’s still trouble ’s worth writing formal mathematical notation. Hopefully can see pattern coefficients generalizes.)Now can compute, example, \\(Var(Y)\\):\\[\\begin{align}\nVar(Y)  &= Var(b_{1}X_{1} + b_{2}X_{2} + E) \\\\\n    &= b_{1}^{2} Var(X_{1}) + b_{2}^{2} Var(X_{2}) + Var(E) \\\\\n    & \\quad + 2b_{1}b_{2} Cov(X_{1}, X_{2}) \\\\\n    & \\quad + 2b_{1} Cov(X_{1}, E) \\\\\n    & \\quad + 2b_{2} Cov(X_{2}, E)\n\\end{align}\\]happens last two lines ? ?Therefore,\\[\nVar(Y) = b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e\n\\]Rule 8 Rule 9 extend similar way sums three terms. ’s even easier: just split covariance many pieces terms split.turn.Calculate \\(Cov(Y, X_{1})\\). get\\[\nb_{1} v_{1} + b_{2} c_{12}\n\\]\nCalculate \\(Cov(Y, X_{2})\\). get\\[\nb_{2} v_{2} + b_{1} c_{12}\n\\]turns computation need write model-implied matrix.first three entries easy just parameters \\(v_{1}\\), \\(c_{12}\\), \\(v_{2}\\). last column contains entries just calculated .Therefore, model-implied matrix \\[\n\\begin{bmatrix}\nv_{1}   &   \\bullet &  \\bullet  \\\\\nc_{12}  &   v_{2}   &  \\bullet \\\\\nb_{1} v_{1} + b_{2} c_{12}  &  b_{2} v_{2} + b_{1} c_{12}  &   b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e\n\\end{bmatrix}\n\\]\nset expressions equal numbers sample covariance matrix, theory solve unknown parameters model-implied matrix . Three basically already done since can just read \\(v_{1}\\), \\(c_{12}\\), \\(v_{2}\\). solving \\(b_{1}\\), \\(b_{2}\\), \\(e\\) joke! even , resulting expressions particularly enlightening. quite happy turning computational details computer.","code":""},{"path":"multiple.html","id":"multiple-interpreting","chapter":"5 Multiple regression","heading":"5.4 Interpreting the coefficients","text":"Without explicit mathematical expressions parameters, ’s bit challenging explain interpretation. now, ’ll take faith following true:multiple regression model, \\(b_{}\\) represents slope linear association \\(Y\\) \\(X_{}\\) holding value predictors constant.mean?Let’s work concrete example. Suppose think college GPA can predicted using high school GPA along number hours per week spent studying college. model might look like:high school GPA hours per week studying correlated (likely ), influence , influence danger “corrupting” estimates path coefficients. example, \\(b_{2}\\) positive, suggest hours spent studying associated predicted increases college GPA. know ’s really due studying? Maybe students well high school just “smarter”.17 Sure, also put hours studying, maybe doesn’t matter. Maybe students just well college even didn’t study whole lot. case, coefficient \\(b_{2}\\) positive just set students (happen study , even though doesn’t matter) also ones high college GPAs.’s important control variables. means need temporarily fix value variables make comparison fair. example, look students 3.0 high school. Among students, variability number hours study college. variability associated variability college GPA, know hours spent studying least partly associated change. (lots factors , swept error variance.) high school GPA can’t predict fixed 3.0, ’re comparing apples apples. Students got 2.0 high school may poorly overall, relative increase GPA due studying (least everything linear, assumed).parameter \\(b_{2}\\) estimated 0.13, suggests additional hour study time per week predicts increase 0.13 points college GPA, holding high school GPA constant. means increase 0.13 predicted within groups students high school GPA.parameter \\(b_{1}\\) estimated 1.2, suggests college GPA predicted increase 1.2 points every point increase high school GPA. coefficient can interpreted holding hours per week studying constant. means estimate makes sense interpret within groups students put number hours studying. takes hours studying explanation accounts changes high school GPA associated changes college GPA.","code":""},{"path":"multiple.html","id":"multiple-standardized","chapter":"5 Multiple regression","heading":"5.5 Regression with standardized variables","text":"Things get little easier (although completely trivial) standardized variables.First, notational simplification. correlations variables—according convention—called \\(r_{X_{2}X_{1}}\\), \\(r_{YX_{1}}\\), \\(r_{YX_{2}}\\). little hard look complex expressions, replace \\(r_{21}\\), \\(r_{Y1}\\), \\(r_{Y2}\\). (Don’t confused \\(r_{21}\\) vs \\(r_{12}\\) \\(c_{21}\\) vs \\(c_{12}\\). Since covariance correlation symmetric, order subscripts matter.)Let’s look sample covariance matrix model-implied matrix standardized variables:\\[\n\\begin{bmatrix}\n1       &   \\bullet &   \\bullet    \\\\\nr_{21}  &   1       &   \\bullet    \\\\\nr_{Y1}  &   r_{Y2}  &   1\n\\end{bmatrix} =\n\\begin{bmatrix}\nv_{1}   &    \\bullet  &   \\bullet \\\\\nc_{12}  &    v_{2}   &    \\bullet \\\\\nb_{1} v_{1} + b_{2} c_{12} &    b_{2} v_{2} + b_{1} c_{12} &   b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e\n\\end{bmatrix}\n\\]\nentry lower-left corner yields\\[\nr_{Y1} = b_{1} v_{1} + b_{2} c_{12}\n\\]\nsimplifies \n\\[\nr_{Y1} = b_{1} + b_{2} r_{21}\n\\]\nnext entry right yields\\[\nr_{Y2} = b_{2} v_{2} + b_{1} c_{12}\n\\]\nsimplifies \n\\[\nr_{Y2} = b_{2} + b_{1} r_{21}\n\\]two equations can solved two unknown parameters \\(b_{1}\\) \\(b_{2}\\).feeling brave? algebra skills sharp? Totally optional, see can derive final answers :\\[\nb_{1} = \\frac{r_{Y1} - r_{Y2}r_{21}}{1 - r_{21}^{2}}\n\\]\\[\nb_{2} = \\frac{r_{Y2} - r_{Y1}r_{21}}{1 - r_{21}^{2}}\n\\]still pretty gross, intuitive content . Look numerator fraction \\(b_{1}\\). Essentially, just \\(r_{Y1}\\) extra stuff. simple regression, expect slope \\(b_{1}\\) simply correlation \\(X_{1}\\) \\(Y\\). multiple regression, also control contribution model coming \\(X_{2}\\). ? subtracting contribution, turns \\(r_{Y2}r_{21}\\). latter term appear way ? need control effect \\(X_{2}\\) \\(X_{2}\\) providing “information” regression model \\(X_{1}\\). Therefore, don’t need subtract \\(r_{Y2}\\) control \\(X_{2}\\), just fraction \\(r_{Y2}\\). fraction? \\(r_{21}\\)! just need part \\(X_{2}\\) common \\(X_{1}\\). don’t want “double-count” contribution model common \\(X_{2}\\) \\(X_{1}\\).’s another way think . \\(X_{1}\\) \\(X_{2}\\) independent? Calculate \\(b_{1}\\) \\(b_{2}\\) formulas much easier case. (Don’t overthink . \\(r_{21}\\) case?)\\(X_{1}\\) \\(X_{2}\\) independent, offer unique contribution predicting \\(Y\\) model. contribution just correlation \\(Y\\) (\\(r_{Y1}\\) \\(r_{Y2}\\), respectively). overlap. \\(X_{1}\\) \\(X_{2}\\) correlated, “influence” counted twice. subtract influence \\(b_{1}\\) \\(b_{2}\\) measuring “pure” contribution \\(X_{1}\\) \\(X_{2}\\), controlling one.\\(1 - r_{21}^{2}\\) denominator? ’s less good intuitive explanation . ’s —mathematically speaking—. rescales slope coefficients make everything work way .final equation one \\(Var(Y)\\) lower-right corner matrix. says\\[\n1 = b_{1}^{2} v_{1} + b_{2}^{2} v_{2} + 2b_{1}b_{2} c_{12} + e\n\\]\nsimplifies \n\\[\n1 = b_{1}^{2} + b_{2}^{2} + 2b_{1}b_{2} r_{21} + e\n\\]\nRearranging solve \\(e\\),\n\\[\ne = 1 - \\left(b_{1}^{2} + b_{2}^{2} + 2b_{1}b_{2} r_{21}\\right)\n\\]\nenlightening way replace \\(b_{1}\\) \\(b_{2}\\) earlier fractions. can leave \\(e\\) like .Since standardized variance \\(Y\\) 1, stuff inside parentheses represents variance accounted model. (subtracted 1, , left \\(e\\), error variance.) analogous \\(R^{2}\\) term described last chapter.makes conceptual sense . pieces \\(\\left(b_{1}^{2} + b_{2}^{2} + 2b_{1}b_{2} r_{21}\\right)\\) correspond various pieces model. first two relate direct effects \\(X_{1}\\) \\(X_{2}\\) third piece relates “indirect” effect shared .","code":""},{"path":"multiple.html","id":"multiple-r","chapter":"5 Multiple regression","heading":"5.6 Multiple regression in R","text":"Let’s fit multiple regression model data music. data sample 10,000 songs Million Song Dataset, collection metrics audio million contemporary popular music tracks.data set downloaded CORGIS Dataset Project information variables data set can found .endogenous variable interest us measure song’s popularity, called song.hotttnesss (scale 0 1).18 many possible exogenous predictors, let’s focus three:artist.hotttnesss\npopularity artist (scale 0 1).\npopularity artist (scale 0 1).song.loudness\nclear website exactly, appears kind average dBFS (decibels relative full scale). Numbers close zero actually loud recordings reasonably go increasingly negative numbers represent softer volumes.\nclear website exactly, appears kind average dBFS (decibels relative full scale). Numbers close zero actually loud recordings reasonably go increasingly negative numbers represent softer volumes.song.tempo\nmeasured beats per minute (BPM).\nmeasured beats per minute (BPM).Let’s plot song.hotttnesss three proposed predictors test linearity assumption, starting artist.hotttnesss:Uh, ’ve got issues deal . Since song.hotttness supposed 0 1, can guess -1 values likely coded represent “missing” data. Even values 0 don’t seem valid given big gap row zeros rest cluster actual data. artist.hotttness variable also seems zeros disconnected rest data. may genuine outliers, ’s likely artists data collected.’re suspecting issues, let’s also check song.loudness song.tempo.song.loudness distribution looks reasonable. ’s definitely skewed left, strict requirements predictor variables particular type distribution.possible song tempo 0 BPM?make little cleaner, following code select variables ’re interested. filter values want keep (discarding ones represent missing/invalid data). ’ll put new tibble called music_clean.reduced number rows 4,157, still huge sample size.Let’s check scatterplots , now music_clean data.doesn’t appear much association loudness tempo. doesn’t violate assumptions. (violation assumptions decidedly non-linear association, just near-zero association.) Given graphs, expect model tell us song popularity maybe somewhat associated artist popularity, much loudness tempo.","code":"\nmusic <- read_csv(\"https://raw.githubusercontent.com/VectorPosse/sem_book/main/data/music.csv\")## Rows: 10000 Columns: 35\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (4): artist.id, artist.name, artist.terms, song.id\n## dbl (31): artist.familiarity, artist.hotttnesss, artist.latitude, artist.loc...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nmusic## # A tibble: 10,000 × 35\n##    artist.familiarity artist.hotttnes… artist.id artist.latitude artist.location\n##                 <dbl>            <dbl> <chr>               <dbl>           <dbl>\n##  1              0.582            0.402 ARD7TVE1…             0                 0\n##  2              0.631            0.417 ARMJAGH1…            35.1               0\n##  3              0.487            0.343 ARKRRTF1…             0                 0\n##  4              0.630            0.454 AR7G5I41…             0                 0\n##  5              0.651            0.402 ARXR32B1…             0                 0\n##  6              0.535            0.385 ARKFYS91…             0                 0\n##  7              0.556            0.262 ARD0S291…             0                 0\n##  8              0.801            0.606 AR10USD1…             0                 0\n##  9              0.427            0.332 AR8ZCNI1…             0                 0\n## 10              0.551            0.423 ARNTLGG1…             0                 0\n## # … with 9,990 more rows, and 30 more variables: artist.longitude <dbl>,\n## #   artist.name <chr>, artist.similar <dbl>, artist.terms <chr>,\n## #   artist.terms_freq <dbl>, release.id <dbl>, release.name <dbl>,\n## #   song.artist_mbtags <dbl>, song.artist_mbtags_count <dbl>,\n## #   song.bars_confidence <dbl>, song.bars_start <dbl>,\n## #   song.beats_confidence <dbl>, song.beats_start <dbl>, song.duration <dbl>,\n## #   song.end_of_fade_in <dbl>, song.hotttnesss <dbl>, song.id <chr>, …\nggplot(music, aes(y = song.hotttnesss,\n                  x = artist.hotttnesss)) +\n    geom_point()\nggplot(music, aes(y = song.hotttnesss,\n                  x = song.loudness)) +\n    geom_point()\nggplot(music, aes(y = song.hotttnesss,\n                  x = song.tempo)) +\n    geom_point()\nmusic_clean <- music %>%\n    select(song.hotttnesss, artist.hotttnesss,\n           song.loudness, song.tempo) %>%\n    filter(song.hotttnesss > 0,\n           artist.hotttnesss > 0,\n           song.tempo > 0)\nmusic_clean## # A tibble: 4,157 × 4\n##    song.hotttnesss artist.hotttnesss song.loudness song.tempo\n##              <dbl>             <dbl>         <dbl>      <dbl>\n##  1           0.602             0.402        -11.2        92.2\n##  2           0.605             0.402         -4.50      130. \n##  3           0.266             0.332        -13.5        86.6\n##  4           0.266             0.352         -7.54      118. \n##  5           0.405             0.448         -8.58      120. \n##  6           0.335             0.331        -16.1       128. \n##  7           0.684             0.513         -5.27      150. \n##  8           0.314             0.378         -8.05      112. \n##  9           0.667             0.542         -4.26      167. \n## 10           0.495             0.306        -12.3       138. \n## # … with 4,147 more rows\nggplot(music_clean, aes(y = song.hotttnesss,\n                        x = artist.hotttnesss)) +\n    geom_point()\nggplot(music_clean, aes(y = song.hotttnesss,\n                        x = song.loudness)) +\n    geom_point()\nggplot(music_clean, aes(y = song.hotttnesss,\n                        x = song.tempo)) +\n    geom_point()"},{"path":"multiple.html","id":"multiple-r-lm","chapter":"5 Multiple regression","heading":"5.6.1 Using lm","text":"lm model specification minor extension learned simple regression. Just use plus signs right side tilde ~ add predictors. sure use music_clean music!didn’t go trouble mean-centering data time, intercept longer 0. attempt interpret intercept anyway. three coefficients \\(b_{1}\\), \\(b_{2}\\) \\(b_{3}\\), path coefficients model. interpreted follows:\\(b_{1}\\):\nSong popularity predicted increase 0.7 points every point increase artist popularity.\nSong popularity predicted increase 0.7 points every point increase artist popularity.mathematically true, ’s kind nonsensical report using numbers magnitude. scales go 0 1, increase 1 point measuring difference artist 0 popularity (lowest possible value popularity) artist 1 popularity (highest possible value popularity).better way report scale everything factor 10:Song popularity predicted increase 0.07 points every 0.1 increase artist popularity.Song popularity predicted increase 0.07 points every 0.1 increase artist popularity.\\(b_{2}\\):\nSong popularity predicted increase 0.004 points every increase 1 dB loudness.\n\\(b_{2}\\):Song popularity predicted increase 0.004 points every increase 1 dB loudness.increase 1 dB much, , can scale result make meaningful. time ’ll multiply factor 10:Song popularity predicted increase 0.04 points every increase 10 dB loudness.Song popularity predicted increase 0.04 points every increase 10 dB loudness.\\(b_{3}\\):\nSong popularity predicted increase 0.0002 points every increase 1 BPM tempo.\n\\(b_{3}\\):Song popularity predicted increase 0.0002 points every increase 1 BPM tempo.Restate interpretation \\(b_{3}\\) scale makes sense. ’re familiar BPM, Google get sense reasonable jump tempo might .Now model fit, can use broom capture residuals.graph now three predictor variables? graph residuals three predictors separately, ’s efficient method.Calculate\\[\nCov(E, \\hat{Y})\n\\]\nsubstituting\\[\n\\hat{Y} = b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}\n\\]assume \\(E\\) independent predictors, value \\(Cov(E, \\hat{Y})\\)?course, \\(Cov(E, \\hat{Y}) = 0\\), necessarily imply \\(Cov(E, X_{})\\) must zero. even zero, doesn’t imply independence. \\(Cov(E, \\hat{Y}) \\neq 0\\), know least one \\(Cov(E, X_{})\\) also must non-zero. Therefore, can plot residuals fitted values serve disqualifying condition. problem plot residuals fitted values serves evidence problem model.One nice features augment output also column called .fitted stores \\(\\hat{Y}\\) values.(standardized) residuals graphed fitted values:weirdness residuals ideal. doesn’t prevent us fitting model, state results cautiously knowing variance toward left half graph compressed relative right side graph. Therefore, error variance “acting” model way across combinations predictor variables.Go back look original scatterplots data (music_clean see can figure residuals cut funny like lower left quadrant.)","code":"\nSONG_lm <- lm(song.hotttnesss ~ artist.hotttnesss +\n                  song.loudness +\n                  song.tempo,\n              data = music_clean)\nSONG_lm## \n## Call:\n## lm(formula = song.hotttnesss ~ artist.hotttnesss + song.loudness + \n##     song.tempo, data = music_clean)\n## \n## Coefficients:\n##       (Intercept)  artist.hotttnesss      song.loudness         song.tempo  \n##         0.1636446          0.7003692          0.0036969          0.0002057\nSONG_aug <- augment(SONG_lm)\nSONG_aug## # A tibble: 4,157 × 10\n##    song.hotttnesss artist.hotttnesss song.loudness song.tempo .fitted  .resid\n##              <dbl>             <dbl>         <dbl>      <dbl>   <dbl>   <dbl>\n##  1           0.602             0.402        -11.2        92.2   0.423  0.179 \n##  2           0.605             0.402         -4.50      130.    0.455  0.149 \n##  3           0.266             0.332        -13.5        86.6   0.364 -0.0984\n##  4           0.266             0.352         -7.54      118.    0.406 -0.140 \n##  5           0.405             0.448         -8.58      120.    0.470 -0.0652\n##  6           0.335             0.331        -16.1       128.    0.362 -0.0273\n##  7           0.684             0.513         -5.27      150.    0.535  0.150 \n##  8           0.314             0.378         -8.05      112.    0.422 -0.108 \n##  9           0.667             0.542         -4.26      167.    0.562  0.105 \n## 10           0.495             0.306        -12.3       138.    0.361  0.134 \n## # … with 4,147 more rows, and 4 more variables: .hat <dbl>, .sigma <dbl>,\n## #   .cooksd <dbl>, .std.resid <dbl>\nggplot(SONG_aug, aes(y = .std.resid, x = .fitted)) +\n    geom_point() +\n    geom_hline(yintercept = 0, color = \"red\")"},{"path":"multiple.html","id":"multiple-r-lavaan","chapter":"5 Multiple regression","heading":"5.6.2 Using lavaan","text":"Model specification lavaan happens separate step model quotes:model fit sem function.unstandardized parameter estimates:Focus estimate column (est).recognize values lines 1 3?line 4 mean? (Hint: ’s variance song.hotttnesss even though notation makes look like .)’s going lines 5 10?standardized parameter estimates:Focus estimates (est.std).easier compare values lines 1 though 3 output unstandardized table? (Hint: think units measurement lack thereof.)value line 4 tell ? (Hint: ’s closer 1 0.)lines 5, 8, 10 equal 1?interpret lines 6, 7, 9?final model variables labeled unstandardized parameter estimates identified:thing, standardized parameter estimates:","code":"\nSONG_model <- \"song.hotttnesss ~ artist.hotttnesss +\n    song.loudness + \n    song.tempo\" \nSONG_fit <- sem(SONG_model, data = music_clean)\nparameterEstimates(SONG_fit)##                  lhs op               rhs      est    se      z pvalue ci.lower\n## 1    song.hotttnesss  ~ artist.hotttnesss    0.700 0.021 33.427  0.000    0.659\n## 2    song.hotttnesss  ~     song.loudness    0.004 0.000  7.987  0.000    0.003\n## 3    song.hotttnesss  ~        song.tempo    0.000 0.000  3.096  0.002    0.000\n## 4    song.hotttnesss ~~   song.hotttnesss    0.021 0.000 45.591  0.000    0.020\n## 5  artist.hotttnesss ~~ artist.hotttnesss    0.012 0.000     NA     NA    0.012\n## 6  artist.hotttnesss ~~     song.loudness    0.112 0.000     NA     NA    0.112\n## 7  artist.hotttnesss ~~        song.tempo    0.091 0.000     NA     NA    0.091\n## 8      song.loudness ~~     song.loudness   25.426 0.000     NA     NA   25.426\n## 9      song.loudness ~~        song.tempo   27.118 0.000     NA     NA   27.118\n## 10        song.tempo ~~        song.tempo 1185.061 0.000     NA     NA 1185.061\n##    ci.upper\n## 1     0.741\n## 2     0.005\n## 3     0.000\n## 4     0.022\n## 5     0.012\n## 6     0.112\n## 7     0.091\n## 8    25.426\n## 9    27.118\n## 10 1185.061\nstandardizedSolution(SONG_fit)##                  lhs op               rhs est.std    se      z pvalue ci.lower\n## 1    song.hotttnesss  ~ artist.hotttnesss   0.459 0.012 39.345  0.000    0.436\n## 2    song.hotttnesss  ~     song.loudness   0.111 0.014  8.051  0.000    0.084\n## 3    song.hotttnesss  ~        song.tempo   0.042 0.014  3.100  0.002    0.016\n## 4    song.hotttnesss ~~   song.hotttnesss   0.752 0.011 69.186  0.000    0.731\n## 5  artist.hotttnesss ~~ artist.hotttnesss   1.000 0.000     NA     NA    1.000\n## 6  artist.hotttnesss ~~     song.loudness   0.202 0.000     NA     NA    0.202\n## 7  artist.hotttnesss ~~        song.tempo   0.024 0.000     NA     NA    0.024\n## 8      song.loudness ~~     song.loudness   1.000 0.000     NA     NA    1.000\n## 9      song.loudness ~~        song.tempo   0.156 0.000     NA     NA    0.156\n## 10        song.tempo ~~        song.tempo   1.000 0.000     NA     NA    1.000\n##    ci.upper\n## 1     0.482\n## 2     0.138\n## 3     0.069\n## 4     0.773\n## 5     1.000\n## 6     0.202\n## 7     0.024\n## 8     1.000\n## 9     0.156\n## 10    1.000"},{"path":"mediation.html","id":"mediation","chapter":"6 Mediation","heading":"6 Mediation","text":"","code":""},{"path":"mediation.html","id":"preliminaries-2","chapter":"6 Mediation","heading":"Preliminaries","text":"load tidyverse package work tibbles lavaan.","code":"\nlibrary(tidyverse)\nlibrary(lavaan)"},{"path":"mediation.html","id":"mediation-paths","chapter":"6 Mediation","heading":"6.1 Mediators, confounders, and colliders","text":"start , let’s look possible paths connect three variables two arrows. (moment, ’ll leave variances, covariances, error terms.)second model just copy first model reversed, can disregard . three models genuinely distinct models somewhat different consequences relationship among three variables:first model represents “mediator”.third model represents “confounder”.fourth model represents “collider”.first part chapter make distinctions clear.","code":""},{"path":"mediation.html","id":"mediation-exogenous-endogenous","chapter":"6 Mediation","heading":"6.2 Exogenous and endogenous variables","text":"Look first model . ’s clear variable left exogenous variable right endogenous. middle variable called mediator. exogenous endogenous?give specific definition two terms:exogenous variable one unidirectional arrows (counting double-headed arrows) entering model diagram. unidirectional arrows leaving .endogenous variable one least one unidirectional arrow entering (, counting double-headed arrows). may unidirectional arrows entering /leaving.prefix exo- means “outside”. whatever variability exogenous variable must come “outside” model. unidirectional arrows coming , nothing model account variance, , matter, covariance exogenous variables.prefix exo- means “outside”. whatever variability exogenous variable must come “outside” model. unidirectional arrows coming , nothing model account variance, , matter, covariance exogenous variables.prefix endo- means “within”. variability endogenous variables accounted variables (including error terms) inside model. fact might arrows leaving endogenous variables irrelevant definition. ’s arrows coming .prefix endo- means “within”. variability endogenous variables accounted variables (including error terms) inside model. fact might arrows leaving endogenous variables irrelevant definition. ’s arrows coming .According definition , mediator exogenous endogenous?Really Important Rules (RIR™) using exogenous endogenous variables models. come three pairs:Rule 1:\nEvery exogenous variable model requires double-headed arrow pointing , representing variance.\nendogenous variable double-headed arrow pointing .\nEvery exogenous variable model requires double-headed arrow pointing , representing variance.endogenous variable double-headed arrow pointing .Rule 2:\nEvery pair exogenous variables model—except error terms—requires double-headed arrow joining , representing covariance.\npair variables model (exogenous endogenous, endogenous) double-headed arrow joining .\nEvery pair exogenous variables model—except error terms—requires double-headed arrow joining , representing covariance.pair variables model (exogenous endogenous, endogenous) double-headed arrow joining .Rule 3:\nEvery endogenous variable model requires error term.\nexogenous variable model error term.\nEvery endogenous variable model requires error term.exogenous variable model error term.six rules important justifications. Don’t just memorize rules blindly. Understand imperative.Rule 1:\nExogenous variables vary, source variance model. (’s makes exogenous.) Therefore, represent variance “manually” model indicating double-headed arrow.\nhand, variance endogenous variables accounted variables model already, doesn’t need separate parameter representing variance.\nExogenous variables vary, source variance model. (’s makes exogenous.) Therefore, represent variance “manually” model indicating double-headed arrow.hand, variance endogenous variables accounted variables model already, doesn’t need separate parameter representing variance.Rule 2:\nPairs exogenous variables co-vary. source covariance model, represent “manually” indicating double-headed arrow. Error terms exception rule. ’s possible error terms can co-vary, usually isn’t sensible models. future chapter [LINK] cover error terms can correlated, never default assumption model.\nCovariances types variables (exogenous endogenous, endogenous endogenous) consequences arrows diagram create direct indirect paths among variables, covariance separately drawn double-headed arrow.\nPairs exogenous variables co-vary. source covariance model, represent “manually” indicating double-headed arrow. Error terms exception rule. ’s possible error terms can co-vary, usually isn’t sensible models. future chapter [LINK] cover error terms can correlated, never default assumption model.Covariances types variables (exogenous endogenous, endogenous endogenous) consequences arrows diagram create direct indirect paths among variables, covariance separately drawn double-headed arrow.Rule 3:\nmodel supposed account variance endogenous variables incoming arrows, never able explain 100% variance just using variables model. always residuals, residuals represented “manually” model using error terms.\nExogenous variables assumed measured without error. assumption always realistic real world, don’t much choice. definition, variance exogenous variable isn’t accounted anything else model, error terms just don’t make sense .\nmodel supposed account variance endogenous variables incoming arrows, never able explain 100% variance just using variables model. always residuals, residuals represented “manually” model using error terms.Exogenous variables assumed measured without error. assumption always realistic real world, don’t much choice. definition, variance exogenous variable isn’t accounted anything else model, error terms just don’t make sense .","code":""},{"path":"mediation.html","id":"mediation-mediators","chapter":"6 Mediation","heading":"6.3 Mediators","text":"Let’s revisit first model (four shown earlier):Draw model piece paper. Following rules , draw variances, covariances, error terms present diagram. (Don’t worry labeling anything letters yet. Just draw arrows circles.)need establish conventions naming things.need name variables. model real-world data, ’ll use contextually meaningful names, abstract models draw, need consistent way labeling .\nExogenous variables called \\(X_{}\\) (using numbers subscripts).\nEndogenous variables called \\(Y_{}\\) (also using numbers subscripts).\nError terms called \\(E_{}\\) subscript matching one endogenous variable ’s attached.\nExogenous variables called \\(X_{}\\) (using numbers subscripts).Endogenous variables called \\(Y_{}\\) (also using numbers subscripts).Error terms called \\(E_{}\\) subscript matching one endogenous variable ’s attached.need label parameters along various paths model:\nVariances called \\(v_{}\\) subscript match exogenous variables \\(X_{}\\) ’re attached.\nError variances called \\(e_{}\\) subscript match error terms \\(E_{}\\) ’re attached.\nCovariances called \\(c_{ij}\\) connecting exogenous variables \\(X_{}\\) \\(X_{j}\\). (Since covariance symmetric, also called \\(c_{ji}\\).)\nUnidirectional arrows error terms corresponding endogenous variables always fixed parameters labeled “1”.\nThick unidirectional arrows exogenous variable \\(X_{}\\) endogenous variable \\(Y_{j}\\) called \\(b_{ji}\\). Note order subscripts: always start subscript target variable end subscript predictor.\nThick unidirectional arrows endogenous variable \\(Y_{}\\) another endogenous variable \\(Y_{j}\\) called \\(a_{ji}\\).\nVariances called \\(v_{}\\) subscript match exogenous variables \\(X_{}\\) ’re attached.Error variances called \\(e_{}\\) subscript match error terms \\(E_{}\\) ’re attached.Covariances called \\(c_{ij}\\) connecting exogenous variables \\(X_{}\\) \\(X_{j}\\). (Since covariance symmetric, also called \\(c_{ji}\\).)Unidirectional arrows error terms corresponding endogenous variables always fixed parameters labeled “1”.Thick unidirectional arrows exogenous variable \\(X_{}\\) endogenous variable \\(Y_{j}\\) called \\(b_{ji}\\). Note order subscripts: always start subscript target variable end subscript predictor.Thick unidirectional arrows endogenous variable \\(Y_{}\\) another endogenous variable \\(Y_{j}\\) called \\(a_{ji}\\).need naming convention thick arrows two exogenous variables?Let’s now include extra bits model required aforementioned rules: variance term exogenous variable \\(X_{1}\\) error terms two endogenous variables \\(Y_{1}\\) \\(Y_{2}\\) along labels everything.include covariances model ?","code":""},{"path":"mediation.html","id":"mediation-confounders","chapter":"6 Mediation","heading":"6.4 Confounders","text":"","code":""},{"path":"mediation.html","id":"mediation-colliders","chapter":"6 Mediation","heading":"6.5 Colliders","text":"","code":""},{"path":"mediation.html","id":"mediation-simple","chapter":"6 Mediation","heading":"6.6 The simple mediation model","text":"","code":""},{"path":"path.html","id":"path","chapter":"7 Path analysis","heading":"7 Path analysis","text":"","code":""},{"path":"latent.html","id":"latent","chapter":"8 Latent variables","heading":"8 Latent variables","text":"","code":""},{"path":"cfa.html","id":"cfa","chapter":"9 Confirmatory factor analysis","heading":"9 Confirmatory factor analysis","text":"","code":""},{"path":"sem.html","id":"sem","chapter":"10 Structural equation models","heading":"10 Structural equation models","text":"","code":""},{"path":"scm.html","id":"scm","chapter":"11 Structural causal models","heading":"11 Structural causal models","text":"","code":""},{"path":"appendix-rules.html","id":"appendix-rules","chapter":"A Variance/covariance rules","heading":"A Variance/covariance rules","text":"Rule 1If \\(C\\) constant, \\[\nVar\\left(C\\right) = 0\n\\]Rule 2If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} + X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]Consequence Rule 1 Rule 2:\\[\nVar\\left(X + C\\right) = Var\\left(X\\right)\n\\]Rule 3If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nVar\\left(X_{1} - X_{2}\\right) =\nVar\\left(X_{1}\\right) + Var\\left(X_{2}\\right)\n\\]Rule 4If \\(\\) number,\\[\nVar\\left(aX\\right) = ^2 Var\\left(X\\right)\n\\]\nRelated rule corresponding one standard deviations:\\[\nSD\\left(aX\\right) = \\left| \\right| SD\\left(X\\right)\n\\]Rule 5\\[\nCov(X, X) = Var(X)\n\\]Rule 6\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{2}, X_{1}\\right)\n\\]Rule 7If \\(C\\) constant, \\[\nCov\\left(X, C\\right) = 0\n\\]Rule 8\\[\nCov\\left(X_{1} + X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) + Cov\\left(X_{2}, X_{3}\\right)  \n\\]Rule 9\\[\nCov\\left(X_{1} - X_{2}, X_{3}\\right) = Cov\\left(X_{1}, X_{3}\\right) - Cov\\left(X_{2}, X_{3}\\right)  \n\\]Consequence Rule 6, Rule 8, Rule 9:\\[\nCov\\left(X_{1}, X_{2} \\pm X_{3}\\right) = Cov\\left(X_{1}, X_{2}\\right) \\pm Cov\\left(X_{1}, X_{3}\\right)  \n\\]Rule 10If \\(\\) number,\\[\nCov\\left(X_{1}, X_{2}\\right) = Cov\\left(X_{1}, X_{2}\\right) =  Cov\\left(X_{1}, X_{2}\\right)  \n\\]Rule 11If \\(X_{1}\\) \\(X_{2}\\) independent, \\[\nCov\\left(X_{1}, X_{2}\\right) = 0\n\\]Rule 12For two variables \\(X_{1}\\) \\(X_{2}\\):\\[\nVar(aX_{1} + bX_{2}) =\n    ^2Var(X_{1}) + b^2Var(X_{2}) + 2abCov(X_{1}, X_{2})\n\\]three variables \\(X_{1}\\), \\(X_{2}\\), \\(X_{3}\\):\\[\\begin{align}\nVar(aX_{1} + bX_{2} + cX_{3}) &=\n    ^2Var(X_{1}) + b^2Var(X_{2}) + c^2Var(X_{3}) \\\\\n    & \\quad + 2abCov(X_{1}, X_{2}) \\\\\n    & \\quad + 2acCov(X_{1}, X_{3}) \\\\\n    & \\quad + 2bcCov(X_{2}, X_{3})\n\\end{align}\\]can extended number variables. variance appears coefficient squared pair variables gets covariance term 2 times product corresponding variable coefficients.","code":""},{"path":"appendix-lisrel.html","id":"appendix-lisrel","chapter":"B LISREL notation","heading":"B LISREL notation","text":"","code":""}]
