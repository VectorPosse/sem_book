% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Demystifying Structural Equation Modeling},
  pdfauthor={Jonathan Amburgey and Sean Raleigh, Westminster College (Salt Lake City, UT)},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Demystifying Structural Equation Modeling}
\author{Jonathan Amburgey and Sean Raleigh, Westminster College (Salt Lake City, UT)}
\date{2022-05-10}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{intro}{%
\chapter*{Introduction}\label{intro}}
\addcontentsline{toc}{chapter}{Introduction}

Welcome to our book on structural equation modeling!

If you want, you can also download this book as a PDF or EPUB file. Be aware that the print versions are missing some of the richer formatting of the online version.

\hypertarget{intro-history}{%
\section*{Some history}\label{intro-history}}
\addcontentsline{toc}{section}{Some history}

In 2016, Jonathan and Sean embarked upon a bold experiment, asking the question, ``Is is possible to teach structural equation modeling (SEM) to undergraduates with little statistical background?'' To make things even more exciting, we attempted to do so in a special topics course lasting only one month during our May Term at Westminster College (Salt Lake City, UT).

In such an endeavor, we had to temper our expectations, of course. The goal was not to produce competent practitioners who would subsequently go on to do serious research using SEM techniques. We were quite happy that, at the end of May, we had undergraduates who were able to put together a simple final project that required them to find some data, posit a model, fit the model in R, interpret the output, and check a few model fit statistics. Some exposure to the topic and some appreciation of its power were satisfying enough. In fact, we think we got a little more out of it than that: we are reasonably confident that most of our students had developed---at that point right after taking the course---the ability to read a research article with an SEM model and have at least some idea what the article was talking about. We called it a win!

We repeated the experiment with some modifications to our materials and pedagogy in 2018. By that point, it was clear that finding textbooks and articles to assign to students was challenging. There are some great books out there, but they are mostly aimed at graduate students. Even the ones labeled ``introductory'' were often far from that for the typical undergraduate with limited statistical training.

We decided that we could write our own textbook that would fill this hole in the literature. The book that follows is the fruit of our efforts.

Sean was granted sabbatical in Spring 2020 and proposed to use that time to start writing the book in preparation for running the May Term course again in May, 2020. And, well, we all know how that went\ldots{}

Once the pandemic subsided enough for us to offer the course in person again, we attempted it again in May, 2022. {[}TO BE CONTINUED{]}

\hypertarget{intro-philosophy}{%
\section*{Our philosophy}\label{intro-philosophy}}
\addcontentsline{toc}{section}{Our philosophy}

As we mentioned before, our motivation for writing the book was driven by the difficulty we had finding readings for the students. Perhaps that begs the question, should one even try teaching a topic as ``difficult'' or ``advanced'' as structural equation modeling to the audience we had? To be sure, the traditional approaches already on the market seem to assume a lot more background than we had at our disposal. And the books that claim to assume less background\ldots well, sometimes they require more than they let on.

The prerequisite for our class is an intro stats class that covers pretty standard material for such a course: hypothesis testing and confidence intervals for one and two proportions, one and two means (and paired means), ANOVA, chi-squared, and simple linear regression. We also benefit in that our intro course introduces students to R. (For those lacking R background, the first four modules \href{https://github.com/VectorPosse/Intro_Stats}{here} {[}FIX THIS LINK ONCE THE DATA 220 BOOK IS ALSO FULLY ONLINE{]} should suffice as a basic introduction to R and R Markdown sufficient for success in this course.)

To respect our students, we made some very deliberate choices about the way our book would be structured.

\begin{itemize}
\tightlist
\item
  \emph{Make the book free and open source.}
\end{itemize}

Students have enough trouble in their lives and shouldn't be exposed to the extortionate practices of most textbook publishers. Not only is this book freely available online, it's also published under a permissive open source license (the \href{https://opensource.org/licenses/MIT}{MIT license}) that allows folks to ``use, copy, modify, merge, publish, distribute, sublicense, and/or sell'' their own versions of the book as desired. Furthermore, any derivative of the book must also abide by the same open standards. So our book is both \emph{libre} and \emph{gratis} (or, in more common parlance, ``free as in speech'' and ``free as in beer'').

\begin{itemize}
\tightlist
\item
  \emph{Start from scratch.}
\end{itemize}

Explain everything from the beginning in terms that are as simple as possible. Some of the first few chapters may look like review for students. Even if it is, of course, that review gives students confidence to tackle upcoming new material. But you might be surprised at some of the novel ways we explain seemingly familiar concepts. All the exposition has an eye toward direct application in later chapters, so what might seem a little idiosyncratic at first is motivated by a desire to smooth the pathways into later concepts.

\begin{itemize}
\tightlist
\item
  \emph{Incorporate active learning into everything.}
\end{itemize}

The chapters are structured to work as templates for classroom experiences. They intersperse conceptual explanation with activities designed to reinforce those concepts and lead students to important conclusions. These learning activities will appear framed in blue boxes {[}CHANGE THIS IF WE ESTABLISH A CUSTOM CALLOUT{]} like this:

Hey, kids! Stop and do this activity here!

\begin{itemize}
\tightlist
\item
  \emph{Do the math and do it well.}
\end{itemize}

One common thread we see in a lot of SEM books is a tendency to sweep most of the math under the rug. The intention comes from a good place; mathematics can appear intimidating and, therefore, may seem to serve as a deterrent to learning. To be sure, there are some complex mathematical ideas in SEM that are inaccessible to our audience. At the same time---and, in fairness, this may be due to Sean's bias as a mathematician---we truly believe that the mathematics, carefully explained, can illuminate student understanding. The more mathy sections may need additional instructor support for students without a strong math background. But all it takes is some relatively straightforward algebra to nail down some concepts that most books ignore. A good example of this is investing time in the rules for manipulating variances and covariances. This allows students to calculate the ``model-implied matrix'' that is only cryptically referenced in most textbooks. However, we do skip the math sometimes. For example, a lot of the math behind model fit indices is left unexplained. At the very least, we hope to be transparent about our choices to include or exclude certain mathematical details.

\begin{itemize}
\tightlist
\item
  \emph{Use ``nice'' data.}
\end{itemize}

Finding data is hard, so we rely a lot on data sets that other textbooks and R package authors make available (with due attribution, of course). To keep things simple for this course, we work almost exclusively with numerical (quantitative) data. {[}MODIFY THIS IF WE END UP WORKING WITH BINARY CATEGORICAL EXOGENOUS VARIABLES (CODED 0/1) AT SOME POINT.{]}

\begin{itemize}
\tightlist
\item
  \emph{Be careful about diagrams.}
\end{itemize}

Learning about complex models induces a sizable cognitive load. Shortcuts in diagrams tend to confuse students. For example, if error terms are truly latent variables, they should be drawn as circles or ellipses and not hidden, even if an advanced practitioner ``knows'' they're there. Variances and covariances among exogenous variables should always appear as well. We take the time to build up a consistent pictographic representation of every part of a model. (Each chapter is introduced with an archetypal diagram that illustrates that chapter's content.) Then we stick to that representation throughout the book.

\begin{itemize}
\tightlist
\item
  \emph{Be careful about notation.}
\end{itemize}

While it may be the industry standard, LISREL notation is needlessly complex for undergraduate students. We take a consistent and simple approach to notation that represents all variables using UPPERCASE names and all path values using lowercase names. Abstract variables tend to be called something like X when exogenous and Y when endogenous. Real-world variables have contextually meaningful names. For those interested in reading the research literature, we have included an appendix describing LISREL notation.

\hypertarget{intro-structure}{%
\section*{Course structure}\label{intro-structure}}
\addcontentsline{toc}{section}{Course structure}

We use this book to teach a 2-credit-hour course. (Even though it's a special topics course in our May Term, the number of contact hours for students is equivalent to a semester-long, 2-credit-hour course.)

{[}ADD INFO HERE AS WE DECIDE HOW MUCH IS REASONABLE TO COVER. IF WE WANT THE BOOK TO BE USABLE IN A 4-CREDIT-HOUR COURSE, WHAT ADDITIONAL MATERIAL SHOULD WE CONSIDER INCLUDING?{]}

\hypertarget{intro-onward}{%
\section*{Onward and upward}\label{intro-onward}}
\addcontentsline{toc}{section}{Onward and upward}

We hope you enjoy our textbook. Please send us your feedback!

--Jonathan Amburgey (\href{mailto:jamburgey@westminstercollege.edu}{\nolinkurl{jamburgey@westminstercollege.edu}})

--Sean Raleigh (\href{mailto:sraleigh@westminstercollege.edu}{\nolinkurl{sraleigh@westminstercollege.edu}})

\hypertarget{variables}{%
\chapter{Variables and measurement}\label{variables}}

\begin{center}\includegraphics{graphics/variable} \end{center}

\hypertarget{variables-first-section}{%
\section{First section}\label{variables-first-section}}

{[}SOMEWHERE NEED TO MENTION ``CONSTANT'' VARIABLES, OR VARIABLES THAT TAKE ONLY ONE VALUE.{]}

\hypertarget{variance}{%
\chapter{Variance}\label{variance}}

\begin{center}\includegraphics{graphics/variance} \end{center}

\hypertarget{variance-mean}{%
\section{A quick refresher on the mean}\label{variance-mean}}

Most of us were taught how to calculate the mean of a variable way back in elementary school: add up all the numbers and divide by the size of the group of numbers. In a statistics context, we often use a ``bar'' to indicate the mean of a variable; in other words, if a variable is called \(X\), the mean is denoted \(\overline{X}\). Remembering that we always use \(n\) to represent the sample size, the formula is

\[
\overline{X} = \frac{\sum{X}}{n}
\]

(In case you forgot, the Greek letter Sigma \(\Sigma\) stands for ``sum'' and means ``add up all values of the thing that follows''.)

Here is a small data set we'll use throughout this chapter as a simple example we can work ``by hand'':

3, 4, 5, 6, 6, 7, 8, 9

Calculate the mean of this set of eight numbers.

\hypertarget{variance-calculating}{%
\section{Calculating variance}\label{variance-calculating}}

Variance is a quantity meant to capture information about how spread out data is.

Let's build it up step by step.

The first thing to note about spread is that we don't care how large or small the numbers are in any absolute sense. We only care how large or small they are \emph{relative to each other}.

Look at the numbers from the earlier exercise:

3, 4, 5, 6, 6, 7, 8, 9

What if we had the following numbers instead?

1003, 1004, 1005, 1006, 1006, 1007, 1008, 1009

Explain why any reasonable measure of ``spread'' should be the same for both groups of numbers.

One way to measure how large or small a number is relative to the whole set is to measure the distance of each number to the mean.

Recall that the mean of the following numbers is 6:

3, 4, 5, 6, 6, 7, 8, 9

Create a new list of eight numbers that measures the distance between each of the above numbers and the mean. In other words, subtract 6 from each of the above numbers.

Some of the numbers in your new list should be negative, some should be zero, and some should be positive. Why does that make sense? In other words, what does it mean when a number is negative, zero, or positive?

If the original set of numbers is called \(X\), then what you've just calculated is a new list \(\left(X - \overline{X}\right)\). Let's start organizing this into a table:

\begin{longtable}[]{@{}rr@{}}
\toprule
\(X\) & \(\left(X - \overline{X}\right)\) \\
\midrule
\endhead
3 & -3 \\
4 & -2 \\
5 & -1 \\
6 & 0 \\
6 & 0 \\
7 & 1 \\
8 & 2 \\
9 & 3 \\
\bottomrule
\end{longtable}

The numbers in the second columns are ``deviations'' from the mean.

One way you might measure ``spread'' is to look at the average deviation. After all, if the deviations represent the distances to the mean, a set with large spread will have large deviations and a set with small spread will have small deviations.

Go ahead and take the average (mean) of the numbers in the second column above.

Uh, oh! You should have calculated zero. Explain why you will always get zero, no matter what set of numbers you start with.

The idea of the ``average deviation'' seems like it should work, but it clearly doesn't. How do we fix the idea?

Hopefully, you identified that having negative deviations was a problem because they canceled out the positive deviations. But if all the deviations were positive, that wouldn't be an issue any more.

There are two ways of making numbers positive:

\begin{itemize}
\tightlist
\item
  Taking absolute values
\end{itemize}

We could just take the absolute value and make all the values positive. There are some statistical procures that do just that,\footnote{This leads to the ``mean absolute deviation'' or MAD.} but we're going to take a slightly different approach\ldots{}

\begin{itemize}
\tightlist
\item
  Squaring
\end{itemize}

If we square each value, they all become positive.

Taking the absolute value is conceptually easier, but there are some historical and mathematical reasons why squaring is a little better.\footnote{If you know calculus, you might think why the square function is much better behaved than the absolute value function.}

Square each of the numbers from the second column of the table above. This will calculate a new list \(\left(X - \overline{X}\right)^{2}\)

Putting the new numbers into our previous table:

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3000}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4000}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3000}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedleft
\(X\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\left(X - \overline{X}\right)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\left(X - \overline{X}\right)^{2}\)
\end{minipage} \\
\midrule
\endhead
3 & -3 & 9 \\
4 & -2 & 4 \\
5 & -1 & 1 \\
6 & 0 & 0 \\
6 & 0 & 0 \\
7 & 1 & 1 \\
8 & 2 & 4 \\
9 & 3 & 9 \\
\bottomrule
\end{longtable}

Now take the average (mean) of the numbers in the third column above.

The number you got (should be 3.5) is \emph{almost} what we call the variance. There's only one more annoying wrinkle.

When you took the mean of the last column of numbers, you added them all up and divided by 8 since there are 8 numbers in the list. But for some fairly technical mathematical reasons, we actually don't want to divide by 8. Instead, we divide by one less than that number; in other words, we divide by 7.\footnote{For more information on that, search the internet for ``sample variance unbiased''}

Re-do the math above, but divide by 7 instead of dividing by 8.

The number you found is the \emph{variance}, written as \(Var(X)\). The full formula is

\[
Var(X) = \frac{\sum{\left(X - \overline{X}\right)^{2}}}{n - 1}
\]

As a one-liner, the formula may look a little intimidating, but when you break it down step by step as we did above, it's not so bad.

Here is the full calculation in the table:

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3000}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4000}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3000}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedleft
\(X\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\left(X - \overline{X}\right)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\left(X - \overline{X}\right)^{2}\)
\end{minipage} \\
\midrule
\endhead
3 & -3 & 9 \\
4 & -2 & 4 \\
5 & -1 & 1 \\
6 & 0 & 0 \\
6 & 0 & 0 \\
7 & 1 & 1 \\
8 & 2 & 4 \\
9 & 3 & 9 \\
& & Sum: 28 \\
& & Variance: 28/7 = \(\boxed{4}\) \\
\bottomrule
\end{longtable}

In our diagrams, the variance of a variable is indicated by a curved, double-headed arrow, labeled with the value of the variance, like this:

\begin{center}\includegraphics{graphics/variance_labeled} \end{center}

Using the tabular approach, calculate the variance of the following set of numbers:

4, 3, 7, 2, 9, 4, 6

Consider the following two sets of numbers:

\begin{enumerate}
\def\labelenumi{\Alph{enumi})}
\item
  1, 2, 5, 8, 9
\item
  1, 4, 5, 6, 9
\end{enumerate}

Without doing any calculations, which of the sets has the larger variance?

Once you've decided, then calculate the variance for both sets and check your answer.

\hypertarget{variance-r}{%
\section{Calculating variance in R}\label{variance-r}}

Once we've done it by hand a few times to make sure we understand how the formula works, from here on out we can let R do the work for us:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{)}
\FunctionTok{var}\NormalTok{(X1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\FunctionTok{var}\NormalTok{(X2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{)}
\FunctionTok{var}\NormalTok{(X3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X4 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{9}\NormalTok{)}
\FunctionTok{var}\NormalTok{(X4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8.5
\end{verbatim}

This is also easier for real-world data that is not highly engineered ðŸ˜‰ to produce whole numbers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PlantGrowth}\SpecialCharTok{$}\NormalTok{weight}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87
## [16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var}\NormalTok{(PlantGrowth}\SpecialCharTok{$}\NormalTok{weight)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.49167
\end{verbatim}

\hypertarget{variance-rules}{%
\section{Variance rules}\label{variance-rules}}

In this course, we will need to be able to calculate the variance of various combinations of variables. For example, if \(X_{1}\) and \(X_{2}\) are two variables, we can create a new variable \(X_{1} + X_{2}\) by adding up the values of the two variables. What is the variance of \(X_{1} + X_{2}\)?

But before we answer that, let's establish the first rule.

\begin{itemize}
\tightlist
\item
  \textbf{Rule 1}
\end{itemize}

Suppose that \(C\) is a ``constant'' variable, meaning that it always has the same value (rather than being a variable that could contain lots of different numbers). Then,

\[
Var\left(C\right) = 0
\]

Why is \protect\hyperlink{Rule1}{\textbf{Rule 1}} true? You can either reason through this conceptually, based on how you understand what variance is supposed to measure, or you can do a sample calculation. (Make a table starting with a column that contains many copies of only a single number and work through the calculation.)

Now, back to the example at the beginning of the section of finding the variance of \(X_{1} + X_{2}\).

\begin{itemize}
\tightlist
\item
  \textbf{Rule 2}
\end{itemize}

If \(X_{1}\) and \(X_{2}\) are independent, then

\[
Var\left(X_{1} + X_{2}\right) =
Var\left(X_{1}\right) + Var\left(X_{2}\right)
\]

We're not going to get into a formal definition of \emph{independence} here. For now, it suffices to think of the intuitive definition you may already have in your head of what it means for two things to be independent. The idea is that, to be independent, \(X_{1}\) and \(X_{2}\) should have nothing to do with each other. Knowing the values of one should not give you any information about values of the other. In the next chapter {[}LINK{]}, we'll say more about this rule.

It's important to note that \protect\hyperlink{Rule2}{\textbf{Rule 2}} is an abstract mathematical rule that holds \emph{in theory}. When we have actual data, however, we know that statistics won't always match their theoretical values. For example, even if a true population mean is 42, samples drawn from that population will have sample means that are \emph{close} to 42, but likely not exactly 42.\footnote{The exact distribution of sample means around a true population value is something you probably learned about in an intro stats course. Sample means follow a Student t distribution.}

Let's test this out. Below are two new variables that are defined using random numbers. The first one is normally distributed with mean 1 and standard deviation 2. (If you don't remember standard deviation from intro stats, we talk about it in the next section.) The second one is normally distributed with mean 4 and standard deviation 3. {[}WHERE DOES SEED INFO GO?{]} These are independent because the definition of \(X_{5}\) does not depend on any way on the definition of \(X_{6}\) and vice versa.

The sample sizes (2000) are large enough that we should get pretty close to the theoretically correct results here.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{10101}\NormalTok{)}
\NormalTok{X5 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)}
\NormalTok{X6 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{4}\NormalTok{, }\AttributeTok{sd =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(X5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.7535339 -0.4927789  3.7518296  1.4751639  1.2172549  3.4054426
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(X6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.297279 4.856377 6.661822 1.309892 2.270882 3.827944
\end{verbatim}

Use R to calculate the variance of \(X_{5}\) and \(X_{6}\) separately. Then use R to add the two numbers you just obtained (the sum of the two variances). Finally, use R to calculate the variance of the sum of the two variables.

Here's an example to help think about this intuitively.

Suppose someone comes along and offers to give you a random amount of money, some number between \$0 and \$100.\footnote{To be more concrete, the values are uniformly distributed, meaning that any number between 0 and 100 is equally likely.} If the variance is a measure of spread, then it stands to reason that variance reflects something about how uncertain you are about how much money you will have after this transaction. On average, you expect about \$50, but you know that the actual amount of money you will receive can vary greatly.

Okay, now a second person comes along and offers you the same deal, a random dollar gift between \$0 and \$100.\footnote{Apparently you live in a town with very generous strangers.} At the end of both transactions, how much money will you have? On average, maybe about \$100, but what about your uncertainty? Because the total amount is the result of two random gifts, you are even less sure how close to \$100 you might be. The range of possible values is now \$0 to \$200.\footnote{To be clear, though, the probabilities are no longer uniform between 0 and 200. To get near 0, you would have to be unlucky twice, and to get near 200 you would have to get lucky twice.} Your uncertainty is greater overall.

Of course, all this explains is why the variance of the sum of two variables is larger than the variance of either variable individually. The fact that the variance of the sum of two independent variables is \emph{exactly} the sum of the variances has to be shown mathematically. But hopefully, the intuition is clear.

The next rule is a consequence of the first two rules, so we will not give it a special number

\[
Var\left(X + C\right) = Var\left(X\right)
\]

Can you apply \protect\hyperlink{Rule2}{\textbf{Rule 2}} followed by \protect\hyperlink{Rule1}{\textbf{Rule 1}} to see mathematically why \(Var\left(X + C\right) = Var\left(X\right)\)?

What is the intuition behind the statement \(Var\left(X + C\right) = Var\left(X\right)\)? In other words, can you explain the rule to someone in terms of what it means about shifting the values of a data set up or down by a constant amount?

\protect\hyperlink{Rule3}{\textbf{Rule 3}} is similar to \protect\hyperlink{Rule2}{\textbf{Rule 2}}, but it's quite counter-intuitive:

\begin{itemize}
\tightlist
\item
  \textbf{Rule 3}
\end{itemize}

If \(X_{1}\) and \(X_{2}\) are independent, then

\[
Var\left(X_{1} - X_{2}\right) =
Var\left(X_{1}\right) + Var\left(X_{2}\right)
\]

It is very common for students to think that a minus sign on the left would translate into a minus sign on the right.\footnote{This results from many years of developing a Pavlovian response to anything that looks like the distributive law from algebra.}

What gives?

Let's return to our example of strangers giving you money.\footnote{Actually, that sounds a little creepy when put like that.} The first person still offers you a random amount between \$0 and \$100. But, now, the second person is a robber, and forces you to give them a random dollar value between \$0 and \$100 (of their choosing, of course). How much money do you expect to have after these two events? On average, \$0. (The first person gives you, on average, \$50, and the second person takes away, on average, \$50.) But how certain are you about that amount?

Imagine a world in which the wrong rule prevailed. What if \(Var\left(X_{1} - X_{2}\right)\) were truly the difference of the two variances. But \(Var\left(X_{1}\right)\) and \(Var\left(X_{2}\right)\) are the same in this scenario. (Although one person is giving money and one is taking, our uncertainty about the dollar amount is the same in both cases.) And this implies
\[
Var\left(X_{1}\right) - Var\left(X_{2}\right) = 0
\]
Can this be true? Zero variance means ``no spread'' which means exact certainty of the value. (Remember \protect\hyperlink{Rule1}{\textbf{Rule 1}}?) Are you 100\% confident that you will end both transactions with exactly \$0? No way!

In fact, the amount of money you end up with ranges from -\$100 up to \$100. This is a larger range than in either transaction individually. Our uncertainty has grown because there are two random processes in play, just like in the scenario with two beneficent strangers. In fact, the width of the range of possibilities is the same in both scenarios: \$0 to \$200 and -\$100 to \$100 both span a range of \$200.

The next rule, unfortunately, does not have a great intuitive explanation. It will make a little more sense in the next chapter {[}LINK{]}, and we'll revisit it then.

\begin{itemize}
\tightlist
\item
  \textbf{Rule 4}
\end{itemize}

If \(a\) is any number,

\[
Var\left(aX\right) = a^2 Var\left(X\right)
\]

If you go back to the table, imagine multiplying every number in the first column by \(a\). Every number in the second column will still have a factor of \(a\). But when you square those values, every number in the third column will have a factor of \(a^{2}\). That's the gist of the rule anyway. But, again, there's not much intuition about why that makes sense.

We can, at least, check empirically that the rule works.

We'll use \(X_{5}\) as we defined it above, a normally distributed variable with mean 1 and standard deviation 2. The variance of the data is about 4:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var}\NormalTok{(X5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.15763
\end{verbatim}

Let's use \(a = 3\).

In R, calculate \(Var\left(3X_{5}\right)\). (Don't forget that in R, you can't just type \texttt{3\ X5}. You have to explicitly include the multiplication sign: \texttt{3\ *\ X5}.)

Now try calculating \(3 Var\left(X_{5}\right)\). You'll see that you don't get the right answer.

But now try \(9 Var\left(X_{5}\right)\). That should work.

And that's all the variance rules we'll need!

\hypertarget{variance-sd}{%
\section{Standard deviation}\label{variance-sd}}

The variance is nice because it obeys all the above rules. The one big downside is that it's not very interpretable.

For example, think of the scenario with people giving/taking money. In that case, the values were measures in units of dollars.

If \(X\) is measured in dollars, what are the units of measurement of \(\overline{X}\)? That seems sensible, right?

What are the units of \(\left(X - \overline{X}\right)\)? Still sensible, right? (It's not a problem that some of these values will be positive and other negative. Negative dollars still make sense. Just think about your student loans.)

Okay, now here's where things get weird. What are the units of \(\left(X - \overline{X}\right)^{2}\)? This no longer makes sense.

Variance is \emph{nearly} the average of a bunch of squared deviations, so for a variable measured in dollars, the units of variance would be ``squared dollars'', whatever that is.

Variances are not really interpretable directly. How do we make them more interpretable? Well, if variance has ``squared'' units, we can take the square root to get back to the natural units we started with.

And this is called the standard deviation, \(SD(X)\).

\[
SD(X) = \sqrt{\frac{\sum{\left(X - \overline{X}\right)^{2}}}
{n - 1}}
\]

Or, said more simply,

\[
SD(X) = \sqrt{Var(X)}
\]
Equivalently,

\[
Var(X) = SD(X)^2
\]

Due to its interpretability, an intro stats class will focus far more on the standard deviation than on the variance. The downside is that the mathematical rules aren't so nice for standard deviations. For example, what is
\[
SD\left(X_{1} + X_{2}\right)?
\]

You can work through the definition to see that

\[
SD\left(X_{1} + X_{2}\right) = \sqrt{
SD\left(X_{1}\right)^{2} + SD\left(X_{2}\right)^{2}
}
\]
But, eww, that's gross.

For SEM, we will focus almost exclusively on variance and switch to standard deviation for only two reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We need to communicate something about spread in meaningful units.
\item
  We need to standardize variables. (See Section \ref{variance-standardizing} below.)
\end{enumerate}

\hypertarget{variance-mean-centering}{%
\section{Mean centering data}\label{variance-mean-centering}}

Many of the statistical techniques taught in an intro stats course focus on learning about the means of variables. Structural equation modeling is a little different in that it is more focused on the explaining the variability of data---how changes in one or more variables predict changes in other variables.\footnote{There are tools in SEM for working with means as well. WILL WE COVER THIS IN A FUTURE CHAPTER?}

A habit we'll start forming now is to mean center all our variables. We do this by subtracting the mean of a variable from all its values.

Let's use \(X_{6}\) as we defined it before, a normally distributed variable with mean 4 and standard deviation 3. How do we interpret the values of \(X_{6} - \overline{X_{6}}\)? (Remember, this is just the second column in our variance tables earlier.)

If we shift all the \(X_{6}\) values to the left by \(\overline{X_{6}}\) units, what is the mean of the new list of numbers?

Let's verify this in R. We'll use the ``suffix'' \texttt{mc} to indicate a mean-centered variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X6\_mc }\OtherTok{\textless{}{-}}\NormalTok{ X6 }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(X6)}
\FunctionTok{mean}\NormalTok{(X6\_mc)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.851573e-16
\end{verbatim}

Why does this answer not exactly agree with the ``theoretical'' answer you came up with in a few lines above? (If you don't already know, the \texttt{e-16} in the expression above is scientific notation and means ``times \(10^{-16}\). That's a really small number!)

Take a guess about the variance of \texttt{X6\_mc}. Verify your guess in R.

So the good news is that \textbf{mean centering preserves the variance}. While the mean will be shifted to be 0, the variance does not change, so any statistical model we build that analyzes the variance will not be affecting by mean-centering.

\hypertarget{variance-standardizing}{%
\section{Standardizing data}\label{variance-standardizing}}

After we've mean centered the data, we can go one step further and divide by the standard deviation. This results in something often called a \emph{z-score}. The process of converting variables from their original units to z-scores is called \emph{standardizing} the data.

\[
Z = \frac{\left(X - \overline{X}\right)}{SD(X)}
\]

Why is this useful? One reason is that it remove the units of measurement to facilitate comparisons between variables. Suppose \(X\) represents height in inches. The numerator (\(X - \overline{X}\)) has units of inches. The standard deviation \(SD(X)\) also has units of inches. So when you divide, the units go away and the z-score is left without units, sometimes called a ``dimensionless quantity''.

Suppose a female in the United States is 6 feet tall (72 inches). Suppose a female in China is 5'8 tall (68 inches). In absolute terms, the American woman is taller than the Chinese woman. But what if we're interested in knowing which woman is taller \emph{relative} to their respective population?

The mean height for an American woman is 65'' with a standard deviation of 3.5'' The mean height for a Chinese woman is 62'' with a standard deviation of 2.5''. (These numbers aren't perfectly correct, but they're probably close-ish.)

Calculate the z-scores for both these women.

Which woman is taller relative to their population?

Although z-scores don't technically have units, we can think of them as measuring how many standard deviations a value lies above or below the mean.

What is the z-score for a value that equals the mean?

What is the meaning of a negative z-score?

The z-score for the American woman was 2. This means that her height measures two standard deviations above the mean.

For real-world data, we will use technology to do this. Here are some temperature measurements from New York in 1974. (These are daily highs across a six-month period.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airquality}\SpecialCharTok{$}\NormalTok{Temp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] 67 72 74 62 56 66 65 59 61 69 74 69 66 68 58 64 66 57 68 62 59 73 61 61 57
##  [26] 58 57 67 81 79 76 78 74 67 84 85 79 82 87 90 87 93 92 82 80 79 77 72 65 73
##  [51] 76 77 76 76 76 75 78 73 80 77 83 84 85 81 84 83 83 88 92 92 89 82 73 81 91
##  [76] 80 81 82 84 87 85 74 81 82 86 85 82 86 88 86 83 81 81 81 82 86 85 87 89 90
## [101] 90 92 86 86 82 80 79 77 79 76 78 78 77 72 75 79 81 86 88 97 94 96 94 91 92
## [126] 93 93 87 84 80 78 75 73 81 76 77 71 71 78 67 76 68 82 64 71 81 69 63 70 77
## [151] 75 76 68
\end{verbatim}

We calculate the mean and standard deviation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 77.88235
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9.46527
\end{verbatim}

This is an average high of about 78 degrees Fahrenheit with a standard deviation of about 9.5 degrees Fahrenheit.

If we just subtract the mean, we get mean-centered data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airquality}\SpecialCharTok{$}\NormalTok{Temp }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] -10.8823529  -5.8823529  -3.8823529 -15.8823529 -21.8823529 -11.8823529
##   [7] -12.8823529 -18.8823529 -16.8823529  -8.8823529  -3.8823529  -8.8823529
##  [13] -11.8823529  -9.8823529 -19.8823529 -13.8823529 -11.8823529 -20.8823529
##  [19]  -9.8823529 -15.8823529 -18.8823529  -4.8823529 -16.8823529 -16.8823529
##  [25] -20.8823529 -19.8823529 -20.8823529 -10.8823529   3.1176471   1.1176471
##  [31]  -1.8823529   0.1176471  -3.8823529 -10.8823529   6.1176471   7.1176471
##  [37]   1.1176471   4.1176471   9.1176471  12.1176471   9.1176471  15.1176471
##  [43]  14.1176471   4.1176471   2.1176471   1.1176471  -0.8823529  -5.8823529
##  [49] -12.8823529  -4.8823529  -1.8823529  -0.8823529  -1.8823529  -1.8823529
##  [55]  -1.8823529  -2.8823529   0.1176471  -4.8823529   2.1176471  -0.8823529
##  [61]   5.1176471   6.1176471   7.1176471   3.1176471   6.1176471   5.1176471
##  [67]   5.1176471  10.1176471  14.1176471  14.1176471  11.1176471   4.1176471
##  [73]  -4.8823529   3.1176471  13.1176471   2.1176471   3.1176471   4.1176471
##  [79]   6.1176471   9.1176471   7.1176471  -3.8823529   3.1176471   4.1176471
##  [85]   8.1176471   7.1176471   4.1176471   8.1176471  10.1176471   8.1176471
##  [91]   5.1176471   3.1176471   3.1176471   3.1176471   4.1176471   8.1176471
##  [97]   7.1176471   9.1176471  11.1176471  12.1176471  12.1176471  14.1176471
## [103]   8.1176471   8.1176471   4.1176471   2.1176471   1.1176471  -0.8823529
## [109]   1.1176471  -1.8823529   0.1176471   0.1176471  -0.8823529  -5.8823529
## [115]  -2.8823529   1.1176471   3.1176471   8.1176471  10.1176471  19.1176471
## [121]  16.1176471  18.1176471  16.1176471  13.1176471  14.1176471  15.1176471
## [127]  15.1176471   9.1176471   6.1176471   2.1176471   0.1176471  -2.8823529
## [133]  -4.8823529   3.1176471  -1.8823529  -0.8823529  -6.8823529  -6.8823529
## [139]   0.1176471 -10.8823529  -1.8823529  -9.8823529   4.1176471 -13.8823529
## [145]  -6.8823529   3.1176471  -8.8823529 -14.8823529  -7.8823529  -0.8823529
## [151]  -2.8823529  -1.8823529  -9.8823529
\end{verbatim}

But if we also divide by the standard deviation, we get a standardized variable (or a set of z-scores). Note the extra parentheses to make sure we get the order of operations right. We have to subtract first, but then divide that whole mean-centered quantity by the standard deviation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp))}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] -1.14971398 -0.62146702 -0.41016823 -1.67796094 -2.31185730 -1.25536337
##   [7] -1.36101276 -1.99490912 -1.78361034 -0.93841519 -0.41016823 -0.93841519
##  [13] -1.25536337 -1.04406459 -2.10055851 -1.46666216 -1.25536337 -2.20620791
##  [19] -1.04406459 -1.67796094 -1.99490912 -0.51581762 -1.78361034 -1.78361034
##  [25] -2.20620791 -2.10055851 -2.20620791 -1.14971398  0.32937752  0.11807873
##  [31] -0.19886945  0.01242934 -0.41016823 -1.14971398  0.64632570  0.75197509
##  [37]  0.11807873  0.43502691  0.96327387  1.28022205  0.96327387  1.59717023
##  [43]  1.49152084  0.43502691  0.22372813  0.11807873 -0.09322005 -0.62146702
##  [49] -1.36101276 -0.51581762 -0.19886945 -0.09322005 -0.19886945 -0.19886945
##  [55] -0.19886945 -0.30451884  0.01242934 -0.51581762  0.22372813 -0.09322005
##  [61]  0.54067630  0.64632570  0.75197509  0.32937752  0.64632570  0.54067630
##  [67]  0.54067630  1.06892327  1.49152084  1.49152084  1.17457266  0.43502691
##  [73] -0.51581762  0.32937752  1.38587145  0.22372813  0.32937752  0.43502691
##  [79]  0.64632570  0.96327387  0.75197509 -0.41016823  0.32937752  0.43502691
##  [85]  0.85762448  0.75197509  0.43502691  0.85762448  1.06892327  0.85762448
##  [91]  0.54067630  0.32937752  0.32937752  0.32937752  0.43502691  0.85762448
##  [97]  0.75197509  0.96327387  1.17457266  1.28022205  1.28022205  1.49152084
## [103]  0.85762448  0.85762448  0.43502691  0.22372813  0.11807873 -0.09322005
## [109]  0.11807873 -0.19886945  0.01242934  0.01242934 -0.09322005 -0.62146702
## [115] -0.30451884  0.11807873  0.32937752  0.85762448  1.06892327  2.01976780
## [121]  1.70281962  1.91411841  1.70281962  1.38587145  1.49152084  1.59717023
## [127]  1.59717023  0.96327387  0.64632570  0.22372813  0.01242934 -0.30451884
## [133] -0.51581762  0.32937752 -0.19886945 -0.09322005 -0.72711641 -0.72711641
## [139]  0.01242934 -1.14971398 -0.19886945 -1.04406459  0.43502691 -1.46666216
## [145] -0.72711641  0.32937752 -0.93841519 -1.57231155 -0.83276580 -0.09322005
## [151] -0.30451884 -0.19886945 -1.04406459
\end{verbatim}

The easier way to do this in R is to use the \texttt{scale} command. (Sorry, the output is a little long. Keep scrolling below.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{scale}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               [,1]
##   [1,] -1.14971398
##   [2,] -0.62146702
##   [3,] -0.41016823
##   [4,] -1.67796094
##   [5,] -2.31185730
##   [6,] -1.25536337
##   [7,] -1.36101276
##   [8,] -1.99490912
##   [9,] -1.78361034
##  [10,] -0.93841519
##  [11,] -0.41016823
##  [12,] -0.93841519
##  [13,] -1.25536337
##  [14,] -1.04406459
##  [15,] -2.10055851
##  [16,] -1.46666216
##  [17,] -1.25536337
##  [18,] -2.20620791
##  [19,] -1.04406459
##  [20,] -1.67796094
##  [21,] -1.99490912
##  [22,] -0.51581762
##  [23,] -1.78361034
##  [24,] -1.78361034
##  [25,] -2.20620791
##  [26,] -2.10055851
##  [27,] -2.20620791
##  [28,] -1.14971398
##  [29,]  0.32937752
##  [30,]  0.11807873
##  [31,] -0.19886945
##  [32,]  0.01242934
##  [33,] -0.41016823
##  [34,] -1.14971398
##  [35,]  0.64632570
##  [36,]  0.75197509
##  [37,]  0.11807873
##  [38,]  0.43502691
##  [39,]  0.96327387
##  [40,]  1.28022205
##  [41,]  0.96327387
##  [42,]  1.59717023
##  [43,]  1.49152084
##  [44,]  0.43502691
##  [45,]  0.22372813
##  [46,]  0.11807873
##  [47,] -0.09322005
##  [48,] -0.62146702
##  [49,] -1.36101276
##  [50,] -0.51581762
##  [51,] -0.19886945
##  [52,] -0.09322005
##  [53,] -0.19886945
##  [54,] -0.19886945
##  [55,] -0.19886945
##  [56,] -0.30451884
##  [57,]  0.01242934
##  [58,] -0.51581762
##  [59,]  0.22372813
##  [60,] -0.09322005
##  [61,]  0.54067630
##  [62,]  0.64632570
##  [63,]  0.75197509
##  [64,]  0.32937752
##  [65,]  0.64632570
##  [66,]  0.54067630
##  [67,]  0.54067630
##  [68,]  1.06892327
##  [69,]  1.49152084
##  [70,]  1.49152084
##  [71,]  1.17457266
##  [72,]  0.43502691
##  [73,] -0.51581762
##  [74,]  0.32937752
##  [75,]  1.38587145
##  [76,]  0.22372813
##  [77,]  0.32937752
##  [78,]  0.43502691
##  [79,]  0.64632570
##  [80,]  0.96327387
##  [81,]  0.75197509
##  [82,] -0.41016823
##  [83,]  0.32937752
##  [84,]  0.43502691
##  [85,]  0.85762448
##  [86,]  0.75197509
##  [87,]  0.43502691
##  [88,]  0.85762448
##  [89,]  1.06892327
##  [90,]  0.85762448
##  [91,]  0.54067630
##  [92,]  0.32937752
##  [93,]  0.32937752
##  [94,]  0.32937752
##  [95,]  0.43502691
##  [96,]  0.85762448
##  [97,]  0.75197509
##  [98,]  0.96327387
##  [99,]  1.17457266
## [100,]  1.28022205
## [101,]  1.28022205
## [102,]  1.49152084
## [103,]  0.85762448
## [104,]  0.85762448
## [105,]  0.43502691
## [106,]  0.22372813
## [107,]  0.11807873
## [108,] -0.09322005
## [109,]  0.11807873
## [110,] -0.19886945
## [111,]  0.01242934
## [112,]  0.01242934
## [113,] -0.09322005
## [114,] -0.62146702
## [115,] -0.30451884
## [116,]  0.11807873
## [117,]  0.32937752
## [118,]  0.85762448
## [119,]  1.06892327
## [120,]  2.01976780
## [121,]  1.70281962
## [122,]  1.91411841
## [123,]  1.70281962
## [124,]  1.38587145
## [125,]  1.49152084
## [126,]  1.59717023
## [127,]  1.59717023
## [128,]  0.96327387
## [129,]  0.64632570
## [130,]  0.22372813
## [131,]  0.01242934
## [132,] -0.30451884
## [133,] -0.51581762
## [134,]  0.32937752
## [135,] -0.19886945
## [136,] -0.09322005
## [137,] -0.72711641
## [138,] -0.72711641
## [139,]  0.01242934
## [140,] -1.14971398
## [141,] -0.19886945
## [142,] -1.04406459
## [143,]  0.43502691
## [144,] -1.46666216
## [145,] -0.72711641
## [146,]  0.32937752
## [147,] -0.93841519
## [148,] -1.57231155
## [149,] -0.83276580
## [150,] -0.09322005
## [151,] -0.30451884
## [152,] -0.19886945
## [153,] -1.04406459
## attr(,"scaled:center")
## [1] 77.88235
## attr(,"scaled:scale")
## [1] 9.46527
\end{verbatim}

Although the outputs are formatted a little differently, you can go back and check that these sets of numbers match each other.

What is the mean of a standardized variable? How do you know this?

Let's calculate the variance of a standardized variable. To do so, I'll note that the mean \(\overline{X}\) is just a number. Also, the standard deviation \(SD(X)\) is just a number. To make the calculation easier to understand, let's just substitute letters that are easier to work with:

\(M = \overline{X}\)

\(S = SD(X)\).

Remember, \(M\) and \(S\) are \emph{constants}.

Now we need to calculate \(Var(Z)\). I'll do the first couple of steps. Then you take over and, using the variance rules from earlier in the chapter, simplify the expression until you get to a numerical answer. Be sure to justify each step by citing the rule you invoked to get there.

\begin{align}
Var(Z) &= Var\left(\frac{\left(X - \overline{X}\right)}{SD(X)}\right) \\
    &= Var\left(\frac{\left(X - M\right)}{S}\right) \\
    &= Var\left(\frac{1}{S}\left(X - M\right)\right) \\
    &= \quad ???
\end{align}

You should get the answer 1. A standardized variable always has variance 1. This will be an important fact in future chapters.

\hypertarget{covariance}{%
\chapter{Covariance}\label{covariance}}

\begin{center}\includegraphics{graphics/covariance} \end{center}

\hypertarget{covariance-calculating}{%
\section{Calculating covariance}\label{covariance-calculating}}

The last chapter was about variance, which measures the spread of a single variable. Now we extend this idea to pairs of variables.

We say that two variables ``co-vary'' when the spread of one variable is related to the spread of another variable. This relationship represents an \emph{association} between the two variables.

We'll call our two variables \(X_{1}\) and \(X_{2}\). To keep things simple, let's assume that we have already mean centered our variables.

If \(X_{1}\) and \(X_{2}\) are already mean centered, then what are \(\overline{X_{1}}\) and \(\overline{X_{2}}\)?

As we did in the last chapter with variance, we'll build up the calculation of covariance step-by-step using a table to keep track of intermediate quantities we need.

Here are two variables (with \(n = 7\)) that have been mean centered:

\begin{longtable}[]{@{}rr@{}}
\toprule
\(X_{1}\) & \(X_{2}\) \\
\midrule
\endhead
-1 & -2 \\
-2 & 2 \\
2 & -2 \\
-3 & -1 \\
4 & 2 \\
-1 & -2 \\
1 & 3 \\
\bottomrule
\end{longtable}

Check that the mean of both columns is truly zero.

Something interesting happens when we look at the product \(X_{1}X_{2}\).

If \(X_{1}\) and \(X_{2}\) both lie above their means, they are both positive numbers. Therefore, their product is positive.

What if both \(X_{1}\) and \(X_{2}\) lie below their means? What do we know about their values individually and what do we know about their product?

Here is the chart again, but with the products listed in a new column:

\begin{longtable}[]{@{}rrr@{}}
\toprule
\(X_{1}\) & \(X_{2}\) & \(X_{1}X_{2}\) \\
\midrule
\endhead
-1 & -2 & 2 \\
-2 & 2 & -4 \\
2 & -2 & -4 \\
-3 & -1 & 3 \\
4 & 2 & 8 \\
-1 & -2 & 2 \\
1 & 3 & 3 \\
\bottomrule
\end{longtable}

Now we add up the products across all seven data pairs:

\begin{longtable}[]{@{}rrr@{}}
\toprule
\(X_{1}\) & \(X_{2}\) & \(X_{1}X_{2}\) \\
\midrule
\endhead
-1 & -2 & 2 \\
-2 & 2 & -4 \\
2 & -2 & -4 \\
-3 & -1 & 3 \\
4 & 2 & 8 \\
-1 & -2 & 2 \\
1 & 3 & 3 \\
& & Sum: 10 \\
\bottomrule
\end{longtable}

So when \(X_{1}\) and \(X_{2}\) tend to have similar values (both positive or both negative), their product is usually positive. It's not true of every pair of values in the table above; some products are negative. But the majority are positive. Therefore, the sum of all such products will be positive.

We're almost there. Just like we wanted the average squared deviation to calculate the variance, here we want the average of the products from the third column above. And just like in the case of variance, it's not \emph{quite} the average we calculate. Instead of dividing by \(n\), we divide by \(n - 1\) for exactly the same esoteric reason. In our example, there are 7 data points (in other words, 7 rows of data), so we divide by 6.

Putting this all together:

\begin{longtable}[]{@{}rrr@{}}
\toprule
\(X_{1}\) & \(X_{2}\) & \(X_{1}X_{2}\) \\
\midrule
\endhead
-1 & -2 & 2 \\
-2 & 2 & -4 \\
2 & -2 & -4 \\
-3 & -1 & 3 \\
4 & 2 & 8 \\
-1 & -2 & 2 \\
1 & 3 & 3 \\
& & Sum: 10 \\
& & Covariance: 10/6 = \(\boxed{1.67}\) \\
\bottomrule
\end{longtable}

In our diagrams, the covariance of two variables is indicated by a curved, double-headed arrow pointing at both boxes and labeled with the value of the covariance, like this:

\begin{center}\includegraphics{graphics/covariance_labeled} \end{center}

Note that we still include the variances of each of the individual variables. They are still important to us. We just have one new type of arrow now.

Verify that the variances in the diagram are correct for our example. You can do it by hand if you want, but using R is fine too.

Here is the final formula for covariance, written as \(Cov\left(X_{1}, X_{2}\right)\). This works for all pairs of variables, even if they aren't mean centered. The terms \(\left(X_{1} - \overline{X_1}\right)\) and \(\left(X_{2} - \overline{X_2}\right)\) do the mean centering:

\[
Cov\left(X_{1}, X_{2}\right) = \frac{\sum \left(X_{1} - \overline{X_{1}}\right)\left(X_{2} - \overline{X_{2}}\right)}{n - 1}
\]

Suppose \(X_{1}\) tends to be above its mean when \(X_{2}\) is below its mean and \(X_{1}\) tends to be below its mean when \(X_{2}\) is above its mean. What will the product \(\left(X_{1} - \overline{X_{1}}\right)\left(X_{2} - \overline{X_{2}}\right)\) usually be? Therefore, what will the sum of all such products likely be?

For general variables (not necessarily mean centered), the table will actually look like this:

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedleft
\(X_{1}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(X_{2}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\left(X_{1} - \overline{X_{1}}\right)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\left(X_{2} - \overline{X_{2}}\right)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\left(X_{1} - \overline{X_{1}}\right)\left(X_{2} - \overline{X_{2}}\right)\)
\end{minipage} \\
\midrule
\endhead
17 & 23 & -2 & 11 & -22 \\
25 & 15 & 6 & 3 & 18 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} \\
\bottomrule
\end{longtable}

Calculate the covariance by hand by making a table like the one above. (These variables are \emph{not} mean centered, so you'll have to calculate the mean of each variable in order to fill out the third and fourth columns.)

\(X_{3}\): 8, 10, 16, 7, 4, 3

\(X_{4}\): 6, 5, 4, 9, 11, 7

Explain intuitively why the covariance is negative for these two variables.

When calculating variance, the order of the data points does not matter. Why?

When calculating covariance, the order of the data points \emph{does} matter. Why?

What if you keep pairs together, but rearrange the rows of the table. How does that affect the covariance?

\hypertarget{covariance-r}{%
\section{Calculating covariance in R}\label{covariance-r}}

Once we've done it by hand a few times to make sure we understand how the formula works, from here on out we can let R do the work for us:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{X2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\FunctionTok{cov}\NormalTok{(X1, X2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.666667
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{X4 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{7}\NormalTok{)}
\FunctionTok{cov}\NormalTok{(X3, X4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -9.2
\end{verbatim}

And here's some real world data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cov}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp, airquality}\SpecialCharTok{$}\NormalTok{Wind)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -15.27214
\end{verbatim}

\hypertarget{covariance-rules}{%
\section{Covariance rules}\label{covariance-rules}}

We'll think of the variance and covariance rules as one big list. We left off on \href{./variance.html\#Rule4}{\textbf{Rule 4}}, so now we'll introduce \protect\hyperlink{Rule5}{\textbf{Rule 5}}.

\begin{itemize}
\tightlist
\item
  \textbf{Rule 5}
\end{itemize}

\[
Cov(X, X) = Var(X)
\]

In words, \protect\hyperlink{Rule5}{\textbf{Rule 5}} states that the covariance of a variable \emph{with itself} is just the same thing as the variance of that variable. This is quite remarkable! It means that variance is really just a special case of covariance.

Explain why \protect\hyperlink{Rule5}{\textbf{Rule 5}} is true. (Hint: think about how you would calculate \(Cov(X, X)\) using either the formula or the table---or both!)

\begin{itemize}
\tightlist
\item
  \textbf{Rule 6}
\end{itemize}

\[
Cov\left(X_{1}, X_{2}\right) = Cov\left(X_{2}, X_{1}\right)
\]

In words, we would say that covariance is \emph{symmetric}.

Explain why \protect\hyperlink{Rule6}{\textbf{Rule 6}} is true. (Again, think about the formula or the table---or both!)

The next four rules are analogous to similar rules for variance (\href{./variance.html\#Rule1}{\textbf{Rule 1}}, \href{./variance.html\#Rule2}{\textbf{Rule 2}}, \href{./variance.html\#Rule3}{\textbf{Rule 3}}, and \href{./variance.html\#Rule4}{\textbf{Rule 4}}).

\begin{itemize}
\tightlist
\item
  \textbf{Rule 7}
\end{itemize}

Suppose that \(C\) is a ``constant'' variable, meaning that it always has the same value (rather than being a variable that could contain lots of different numbers). Then,

\[
Cov\left(X, C\right) = 0
\]

As always, try to explain this rule. Give an intuitive explanation of why this rule ``should'' be true. Then think about it computationally, thinking of either the formula or the table---or both!

\begin{itemize}
\tightlist
\item
  \textbf{Rule 8}
\end{itemize}

\[
Cov\left(X_{1} + X_{2}, X_{3}\right) = Cov\left(X_{1}, X_{3}\right) + Cov\left(X_{2}, X_{3}\right)  
\]

What you should appreciate here is that there is no longer any restriction on the relationships among the variables involved. \href{./variance.html\#Rule2}{\textbf{Rule 2}} only worked when the two variables were independent. On the other hand, \protect\hyperlink{Rule8}{\textbf{Rule 8}} works for any combination of variables, no matter their relation.

Even more satisfying is this next rule:

\begin{itemize}
\tightlist
\item
  \textbf{Rule 9}
\end{itemize}

\[
Cov\left(X_{1} - X_{2}, X_{3}\right) = Cov\left(X_{1}, X_{3}\right) - Cov\left(X_{2}, X_{3}\right)  
\]

Yay! The minus sign behaves sensibly now! Of course, since covariances can be positive or negative (unlike variances which are always positive!) we can more safely subtract two of them without worry. And this rule, like \protect\hyperlink{Rule8}{\textbf{Rule 8}}, does not depend on \(X_{1}\) and \(X_{2}\) being independent. They can be any two variables.

There are versions of these rules with the addition or subtraction on the other side, but these are just minor variations of \protect\hyperlink{Rule8}{\textbf{Rule 8}} and \protect\hyperlink{Rule9}{\textbf{Rule 9}}, so they're not worth mentioning as a separate rule. Remember that covariance is symmetric, so you can always swap things on the left and right of the comma.

\[
Cov\left(X_{1}, X_{2} \pm X_{3}\right) = Cov\left(X_{1}, X_{2}\right) \pm Cov\left(X_{1}, X_{3}\right)  
\]

\begin{itemize}
\tightlist
\item
  \textbf{Rule 10}
\end{itemize}

If \(a\) is any number,

\[
Cov\left(a X_{1}, X_{2}\right) = a Cov\left(X_{1}, X_{2}\right) =  Cov\left(X_{1}, a X_{2}\right)  
\]

This rule is also very sensible. Instead of \href{./variance.html\#Rule4}{\textbf{Rule 4}} that takes a number \(a\) and pulls out an \(a^{2}\), \protect\hyperlink{Rule10}{\textbf{Rule 10}} just pulls out a single factor of \(a\) (from either slot).

Just a couple more rules. We were talking about independence in conjunction with \protect\hyperlink{Rule8}{\textbf{Rule 8}} and \protect\hyperlink{Rule9}{\textbf{Rule 9}}. That leads directly to an interesting and super-important rule:

\begin{itemize}
\tightlist
\item
  \textbf{Rule 11}
\end{itemize}

If \(X_{1}\) and \(X_{2}\) are independent, then

\[
Cov\left(X_{1}, X_{2}\right) = 0
\]

Why is \protect\hyperlink{Rule11}{\textbf{Rule 11}} true, intuitively?

It's interesting to note that this rule only works one way. In other words, if you know that two variables are independent, then you can conclude their covariance is zero. However, if you know the covariance is zero, that doesn't necessarily mean that the two variables are independent. We'll see an example of this later in the chapter.

Finally, one rule to rule them all:

\begin{itemize}
\tightlist
\item
  \textbf{Rule 12}
\end{itemize}

For \emph{any} two variables \(X_{1}\) and \(X_{2}\):

\[
Var(aX_{1} + bX_{2}) = 
    a^2Var(X_{1}) + b^2Var(X_{2}) + 2abCov(X_{1}, X_{2})
\]

This brings practically everything we know together into one rule!

Proving \protect\hyperlink{Rule12}{\textbf{Rule 12}} will give us good practice with the type of manipulation we'll need to do in future chapters. So here goes. For the first few steps, you name what rule we're invoking. Then, you'll pick up the thread and follow it through the last few steps on your own.

\begin{align}
Var(aX_{1} + bX_{2}) &= Cov(aX_{1} + bX_{2}, aX_{1} + bX_{2}) \\
    &= Cov(aX_{1} + bX_{2}, aX_{1}) + Cov(aX_{1} + bX_{2}, bX_{2}) \\
    &=  Cov(aX_{1}, aX_{1}) + 
        Cov(bX_{2}, aX_{1}) + \\
    &   \qquad Cov(aX_{1}, bX_{2}) + 
        Cov(bX_{2}, bX_{2}) \\
    &= \quad ???
\end{align}

You'll need these rules to do calculations in future chapters. Rather than having to search for them in Chapter \ref{variance} and this chapter, we've gathered up all the rules in one convenient place in Appendix \ref{appendix-rules}.

\hypertarget{covariance-correlation}{%
\section{Correlation}\label{covariance-correlation}}

\hypertarget{covariance-visualizing}{%
\section{Visualizing covariance and correlation}\label{covariance-visualizing}}

\hypertarget{simple}{%
\chapter{Simple regression}\label{simple}}

\begin{center}\includegraphics{graphics/simple_regression} \end{center}

\hypertarget{multiple}{%
\chapter{Multiple regression}\label{multiple}}

\begin{center}\includegraphics{graphics/multiple_regression} \end{center}

\hypertarget{mediation}{%
\chapter{Mediation}\label{mediation}}

\hypertarget{path-analysis}{%
\chapter{Path analysis}\label{path-analysis}}

\hypertarget{latent-variables}{%
\chapter{Latent variables}\label{latent-variables}}

\hypertarget{confirmatory-factor-analysis}{%
\chapter{Confirmatory factor analysis}\label{confirmatory-factor-analysis}}

\hypertarget{structural-equation-models}{%
\chapter{Structural equation models}\label{structural-equation-models}}

\hypertarget{structural-causal-models}{%
\chapter{Structural causal models}\label{structural-causal-models}}

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{appendix-rules}{%
\chapter{Variance/covariance rules}\label{appendix-rules}}

\begin{itemize}
\tightlist
\item
  \href{./variance.html\#Rule1}{\textbf{Rule 1}}
\end{itemize}

If \(C\) is constant, then

\[
Var\left(C\right) = 0
\]

\begin{itemize}
\tightlist
\item
  \href{./variance.html\#Rule2}{\textbf{Rule 2}}
\end{itemize}

If \(X_{1}\) and \(X_{2}\) are independent, then

\[
Var\left(X_{1} + X_{2}\right) =
Var\left(X_{1}\right) + Var\left(X_{2}\right)
\]

Consequence of \href{./variance.html\#Rule1}{\textbf{Rule 1}} and \href{./variance.html\#Rule2}{\textbf{Rule 2}}:

\[
Var\left(X + C\right) = Var\left(X\right)
\]

\begin{itemize}
\tightlist
\item
  \href{./variance.html\#Rule3}{\textbf{Rule 3}}
\end{itemize}

If \(X_{1}\) and \(X_{2}\) are independent, then

\[
Var\left(X_{1} - X_{2}\right) =
Var\left(X_{1}\right) + Var\left(X_{2}\right)
\]

\begin{itemize}
\tightlist
\item
  \href{./variance.html\#Rule4}{\textbf{Rule 4}}
\end{itemize}

If \(a\) is any number,

\[
Var\left(aX\right) = a^2 Var\left(X\right)
\]

\begin{itemize}
\tightlist
\item
  \href{./covariance.html\#Rule5}{\textbf{Rule 5}}
\end{itemize}

\[
Cov(X, X) = Var(X)
\]

\begin{itemize}
\tightlist
\item
  \href{./covariance.html\#Rule6}{\textbf{Rule 6}}
\end{itemize}

\[
Cov\left(X_{1}, X_{2}\right) = Cov\left(X_{2}, X_{1}\right)
\]

\begin{itemize}
\tightlist
\item
  \href{./covariance.html\#Rule7}{\textbf{Rule 7}}
\end{itemize}

If \(C\) is constant, then

\[
Cov\left(X, C\right) = 0
\]

\begin{itemize}
\tightlist
\item
  \href{./covariance.html\#Rule8}{\textbf{Rule 8}}
\end{itemize}

\[
Cov\left(X_{1} + X_{2}, X_{3}\right) = Cov\left(X_{1}, X_{3}\right) + Cov\left(X_{2}, X_{3}\right)  
\]

\begin{itemize}
\tightlist
\item
  \href{./covariance.html\#Rule9}{\textbf{Rule 9}}
\end{itemize}

\[
Cov\left(X_{1} - X_{2}, X_{3}\right) = Cov\left(X_{1}, X_{3}\right) - Cov\left(X_{2}, X_{3}\right)  
\]

Consequence of \href{./covariance.html\#Rule6}{\textbf{Rule 6}}, \href{./covariance.html\#Rule8}{\textbf{Rule 8}}, and \href{./covariance.html\#Rule9}{\textbf{Rule 9}}:

\[
Cov\left(X_{1}, X_{2} \pm X_{3}\right) = Cov\left(X_{1}, X_{2}\right) \pm Cov\left(X_{1}, X_{3}\right)  
\]

\begin{itemize}
\tightlist
\item
  \href{./covariance.html\#Rule10}{\textbf{Rule 10}}
\end{itemize}

If \(a\) is any number,

\[
Cov\left(a X_{1}, X_{2}\right) = a Cov\left(X_{1}, X_{2}\right) =  Cov\left(X_{1}, a X_{2}\right)  
\]

\begin{itemize}
\tightlist
\item
  \href{./covariance.html\#Rule11}{\textbf{Rule 11}}
\end{itemize}

If \(X_{1}\) and \(X_{2}\) are independent, then

\[
Cov\left(X_{1}, X_{2}\right) = 0
\]

\begin{itemize}
\tightlist
\item
  \href{./covariance.html\#Rule12}{\textbf{Rule 12}}
\end{itemize}

For \emph{any} two variables \(X_{1}\) and \(X_{2}\):

\[
Var(aX_{1} + bX_{2}) = 
    a^2Var(X_{1}) + b^2Var(X_{2}) + 2abCov(X_{1}, X_{2})
\]

\hypertarget{appendix-lisrel}{%
\chapter{LISREL notation}\label{appendix-lisrel}}

  \bibliography{book.bib,packages.bib}

\end{document}
